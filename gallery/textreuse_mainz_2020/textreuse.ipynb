{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Text-Reuse\" data-toc-modified-id=\"Text-Reuse-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Text Reuse</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Blockseminar-Studiengang-&quot;Digitale-Methodik-in-den-Geistes--und-Kulturwissenschaften&quot;-(18.1.2020,-8.2.2020,-15.2.2020)\" data-toc-modified-id=\"Blockseminar-Studiengang-&quot;Digitale-Methodik-in-den-Geistes--und-Kulturwissenschaften&quot;-(18.1.2020,-8.2.2020,-15.2.2020)-1.0.0.1\"><span class=\"toc-item-num\">1.0.0.1&nbsp;&nbsp;</span>Blockseminar Studiengang \"Digitale Methodik in den Geistes- und Kulturwissenschaften\" (18.1.2020, 8.2.2020, 15.2.2020)</a></span></li></ul></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Reuse\n",
    "#### Blockseminar Studiengang \"Digitale Methodik in den Geistes- und Kulturwissenschaften\" (18.1.2020, 8.2.2020, 15.2.2020)\n",
    "\n",
    "- lxml\n",
    "\n",
    "Since for many of the subsequent steps, we need a language toolkit like [NLTK](http://www.nltk.org/) or [spaCy](https://spacy.io/) (to mention just the two best known ones) anyway, and since most of these also provide tokenization capabilities, we refrain from using a homebrew tokenizer like the one listed in the appendix.\n",
    "\n",
    "Actually, there are many Python Lemmatizers, but quite a couple of them are only for English. Some depend on Wordnet resources and may load wordnet data for other languages as well. (I have found an online [comparison](https://lars76.github.io/nlp/lemmatize-portuguese/) of Python Lemmatizers for Portuguese but I cannot tell how reliable it is.) But it seems none of the options is really up to the task, especially (that's my contention now) for historical language variants.\n",
    "\n",
    "Here is a list of toolkits and wordnets that I found:\n",
    "\n",
    "- [FreeLing](http://nlp.lsi.upc.edu/freeling/)\n",
    "- [spaCy](https://spacy.io/)\n",
    "- [NLTK](http://www.nltk.org/)\n",
    "- [Pattern](https://www.clips.uantwerpen.be/pattern)\n",
    "- [RDRPoSTagger](https://github.com/datquocnguyen/RDRPOSTagger)\n",
    "- [TreeTagger for Python](https://github.com/miotto/treetagger-python)\n",
    "- [TextBlob](https://textblob.readthedocs.io/en/dev/)\n",
    "- [StanfordNLP](https://stanfordnlp.github.io/stanfordnlp/)\n",
    "- [Polyglot](https://polyglot.readthedocs.io/en/latest/)\n",
    "\n",
    "\n",
    "- [WordNet](https://wordnet.princeton.edu/)\n",
    "- [Open Multilinugual Wordnet](http://compling.hss.ntu.edu.sg/omw/)\n",
    "- [MultiWordnet](http://multiwordnet.fbk.eu/english/home.php)\n",
    "- [OpenWordnet-PT](https://github.com/own-pt/openWordnet-PT) for Portuguese\n",
    "- [Multilingual Central Repository](http://adimen.si.ehu.es/web/MCR/)\n",
    "- [BabelNet](https://babelnet.org/)\n",
    "- [ConceptNet](http://conceptnet.io/)\n",
    "\n",
    "However, there is one toolkit -- [FreeLing](http://nlp.lsi.upc.edu/freeling/) \\[Padr贸/Stanilovsky 2012\\] -- that is often overlooked and I have used its dictionary of word forms for historical Spanish in the past to some satisfaction \\[also Sanchez-Marco/Boleda/Padr贸 2011\\]. We will use this one and, besides its dictionary resources, also use some of its more advanced methods. (For sense annotation, FreeLing relies on Wordnet as well, but not for Lemmatization.) For learning about its API and how to use it, you could start [here](https://talp-upc.gitbook.io/freeling-4-1-user-manual/installation/calling-freeling-library-from-languages-other-than-c++). In the appendix, you will find example code for how to use its Python 3 API.)\n",
    "\n",
    "<div class=\"alert alertbox alert-danger\">\n",
    "<p>Although I have tried for hours to build the python 3 interface for freeling on windows, I was not successful. So for the rest of this notebook, assume that it only works under linux!</p>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "1  Preprocessing\n",
    "In linguistic preprocessing, we add more preprocessing:\n",
    "\n",
    "Tokenisation\n",
    "(Normalisation?!)\n",
    "Lemmatisation\n",
    "2  Cosine similarity\n",
    "2.1  Filters\n",
    "Filter out things that might be irrelevant for characterizing a segment like stopwords or everything but tf/idf top words.\n",
    "\n",
    "(And then do cosine similarity again.)\n",
    "\n",
    "2.2  Boosters\n",
    "Add weight to overlap in marginal number, question marks, (long) homographs, (long) numbers, proper names.\n",
    "\n",
    "(And then do cosine similarity again.)\n",
    "\n",
    "3  Postponed\n",
    "3.1  Champollion/BSA\n",
    "since Champollion [Ma 2006] and the Microsoft Bilingual Sentence aligner [Moore 2002] are available in Perl implementations only (and we have enough alternatives), we postpone analyses with them for now.\n",
    "\n",
    "3.2  Bleualign\n",
    "Sennrich/Volk 2010 uses a machine translation (e.g. google or DeepL) of the source to the target language and then does intra-language alignment. We postpone this, too.\n",
    "\n",
    "3.3  Cognate alignment\n",
    "Darriba Bilbao/Pereira Lopes/Ildefonso 2005 align via Longest Sorted Sequence and recognize cognates from language resources.\n",
    "\n",
    "3.4  Embeddings\n",
    "try to align recognizing similarities in word-context/word-document vectors according to Bizzoni/Reboul 2016, Bouamor/Sajjad 2018, Guo/Shen/Xang et al. 2018.\n",
    "\n",
    "\n",
    "\n",
    "1.1  Knowledge-based\n",
    "To make use of knowledge graphs, we rely on the linguistic preprocessing) and add a bit more graph-oriented preprocessing.\n",
    "\n",
    "Then, for the knowledge-based mode of alignment, we use the following approaches:\n",
    "\n",
    "graph similarity according to (Franco-Salvador/Rosso/Montes-y-G贸mez 2016)\n",
    "1.1.0.1  Literature\n",
    "Franco-Salvador/Rosso/Montes-y-G贸mez 2016: A systematic study of knowledge graph analysis for cross-language plagiarism detection\n",
    "Mohamed/Oussalah 2018: A Hybrid Approach for Paraphrase Identification Based on Knowledge-enriched Semantic Heuristics\n",
    "Paul/Rettinger et al. 2016: Efficient Graph-based Document Similarity\n",
    "Speer/Chin/Havasi 2017: ConceptNet 5.5: An Open Multilingual Graph of General Knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import lxml\n",
    "from lxml import etree\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import locale\n",
    "locale.setlocale(locale.LC_ALL, '')  # Use '' for auto, or force e.g. to 'en_US.UTF-8'\n",
    "from functools import partial\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "from decimal import Decimal\n",
    "\n",
    "import ctypes\n",
    "import nltk.translate.gale_church\n",
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "import bleualign.gale_church   # from Rico Sennrich's Bleualign: https://github.com/rsennrich/Bleualign\n",
    "# import _align from gale-church   # from Li Ling Tan's https://github.com/alvations/gachalign\n",
    "\n",
    "import nltk\n",
    "from itertools import chain\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import tabulate\n",
    "from IPython.display import HTML, display\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from itertools import chain\n",
    "\n",
    "import tabulate\n",
    "from IPython.display import HTML, display\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "\n",
    "# -- Freeling\n",
    "\n",
    "aux_dir  = \"\\\\auxiliary_files\"\n",
    "nb_dir   = os.path.split(os.getcwd())[0] + \"\\\\\" + os.path.split(os.getcwd())[1] + aux_dir\n",
    "\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "\n",
    "print(sys.path)\n",
    "\n",
    "from auxiliary_files import pyfreeling\n",
    "\n",
    "## Check whether we know where to find FreeLing data files\n",
    "if \"FREELINGDIR\" not in os.environ :\n",
    "   if sys.platform == \"win32\" or sys.platform == \"win64\" : os.environ[\"FREELINGDIR\"] = \"C:\\\\Program Files\"\n",
    "   else : os.environ[\"FREELINGDIR\"] = \"/usr\"\n",
    "\n",
    "if not os.path.exists(os.environ[\"FREELINGDIR\"]+\"/share/freeling\") :\n",
    "   print(\"Folder\",os.environ[\"FREELINGDIR\"]+\"/share/freeling\",\n",
    "         \"not found.\\nPlease set FREELINGDIR environment variable to FreeLing installation directory\",\n",
    "         file=sys.stderr)\n",
    "   sys.exit(1)\n",
    "\n",
    "# Location of FreeLing configuration files.\n",
    "DATA = os.environ[\"FREELINGDIR\"]+\"/share/freeling/\"\n",
    "\n",
    "# Init locales\n",
    "pyfreeling.util_init_locale(\"default\")\n",
    "\n",
    "# -- graph-based\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
