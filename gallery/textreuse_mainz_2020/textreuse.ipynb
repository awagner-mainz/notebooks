{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Sprachübergreifende-Textalignierung\" data-toc-modified-id=\"Sprachübergreifende-Textalignierung-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Sprachübergreifende Textalignierung</a></div><div class=\"lev1 toc-item\"><a href=\"#Manuales-(sprachübergreifende-Alignierung)\" data-toc-modified-id=\"Manuales-(sprachübergreifende-Alignierung)-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Manuales (sprachübergreifende Alignierung)</a></div><div class=\"lev2 toc-item\"><a href=\"#LXML\" data-toc-modified-id=\"LXML-21\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>LXML</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprachübergreifende Textalignierung\n",
    "\n",
    "Blockseminar Studiengang \"Digitale Methodik in den Geistes- und Kulturwissenschaften\" (18.1.2020, 8.2.2020, 15.2.2020)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "\n",
    "import re\n",
    "import locale\n",
    "locale.setlocale(locale.LC_ALL, '')  # Use '' for auto, or force e.g. to 'en_US.UTF-8'\n",
    "from collections import OrderedDict\n",
    "from decimal import Decimal\n",
    "from functools import partial\n",
    "from itertools import chain\n",
    "import ctypes\n",
    "import numpy as np\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import lxml\n",
    "from lxml import etree\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "import tabulate\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "import nltk\n",
    "import nltk.translate.gale_church\n",
    "\n",
    "import bleualign.gale_church   # from Rico Sennrich's Bleualign: https://github.com/rsennrich/Bleualign\n",
    "# import _align from gale-church   # from Li Ling Tan's https://github.com/alvations/gachalign\n",
    "\n",
    "# Freeling\n",
    "aux_dir  = \"\\\\auxiliary_files\"\n",
    "nb_dir   = os.path.split(os.getcwd())[0] + \"\\\\\" + os.path.split(os.getcwd())[1] + aux_dir\n",
    "\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "\n",
    "from auxiliary_files import pyfreeling\n",
    "\n",
    "pyfreeling.util_init_locale(\"default\")\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T15:02:09.726254Z",
     "start_time": "2020-01-18T15:02:09.717089Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "in_dir = \"./data/constitutions/\"\n",
    "\n",
    "# we create a dictionary with our constitutions:\n",
    "sources = {}\n",
    "\n",
    "for file in sorted(os.listdir(in_dir)):\n",
    "    key = os.path.basename(file).split(os.extsep)[0]\n",
    "    with open(in_dir + '/' + file, encoding=\"utf-8\") as f:\n",
    "        sources[key] = f.read()\n",
    "\n",
    "# and a list of available constitutions for quick lookup:\n",
    "constitutions = list(sources.keys())\n",
    "\n",
    "print (\"{} files read:\".format(len(constitutions)))\n",
    "print (constitutions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T14:49:05.940842Z",
     "start_time": "2020-01-18T14:49:05.326798Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import tokenize\n",
    "# nltk.download('punkt')\n",
    "\n",
    "sentences = {}\n",
    "nos = {}\n",
    "for c in constitutions:\n",
    "    t = tokenize.sent_tokenize(sources[c])\n",
    "    nos[c] = len(t)\n",
    "    for i, s in enumerate(t):\n",
    "        sentences[c + '_' + str(i)] = s\n",
    "\n",
    "boundary = len(sentences) - nos['1948_-_BU_-_Burma_-_constitution_of_burma']\n",
    "print(\"Corpus has {} sentences.\".format(len(sentences)))\n",
    "print(\"1948_-_BU_-_Burma_-_constitution_of_burma has {}.\\n\".format(nos['1948_-_BU_-_Burma_-_constitution_of_burma']))\n",
    "\n",
    "print(\"Its first 3 sentences are:\\n{}\".format([sentences['1948_-_BU_-_Burma_-_constitution_of_burma_0'],\\\n",
    "                                              sentences['1948_-_BU_-_Burma_-_constitution_of_burma_1'],\\\n",
    "                                              sentences['1948_-_BU_-_Burma_-_constitution_of_burma_2']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T14:52:29.645985Z",
     "start_time": "2020-01-18T14:52:29.463923Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer='word', strip_accents='unicode', stop_words=[\"the\", \"of\", \"and\"])\n",
    "dfm = vectorizer.fit_transform(sentences.values())\n",
    "\n",
    "print(dfm.shape)\n",
    "print(type(dfm))\n",
    "print(dfm.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T14:54:09.837001Z",
     "start_time": "2020-01-18T14:54:09.774215Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "target = dfm[boundary:]\n",
    "sources = dfm[:boundary,]\n",
    "print(target.shape)\n",
    "print(sources.shape)\n",
    "\n",
    "simils = cosine_similarity(target, sources)\n",
    "print(simils.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T14:59:24.058415Z",
     "start_time": "2020-01-18T14:59:24.039714Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.argmax(simils, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manuales (sprachübergreifende Alignierung)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T02:03:19.302866Z",
     "start_time": "2020-02-15T02:03:19.234706Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "in_dir = \"./data/manual/\"\n",
    "\n",
    "# we create a dictionary with our manuales:\n",
    "sources = {}\n",
    "\n",
    "for file in sorted(os.listdir(in_dir)):\n",
    "    key = os.path.basename(file).split(os.extsep)[0]\n",
    "    with open(in_dir + '/' + file, encoding=\"utf-8\") as f:\n",
    "        sources[key] = f.read()\n",
    "\n",
    "# and a list of available constitutions for quick lookup:\n",
    "manuales = list(sources.keys())\n",
    "\n",
    "print (\"{} files read:\".format(len(manuales)))\n",
    "print (manuales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T02:03:01.777913Z",
     "start_time": "2020-02-15T02:02:57.287659Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import tokenize\n",
    "# nltk.download('punkt')\n",
    "\n",
    "sentences = {}\n",
    "nos = {}\n",
    "for c in manuales:\n",
    "    t = tokenize.sent_tokenize(sources[c])\n",
    "    nos[c] = len(t)\n",
    "    for i, s in enumerate(t):\n",
    "        sentences[c + '_' + str(i)] = s\n",
    "\n",
    "print(\"Corpus has {} sentences.\".format(len(sentences)))\n",
    "print(\"azp1552_ch17 {}.\\n\".format(nos['azp1552_ch17']))\n",
    "\n",
    "print(\"Its first 3 sentences are:\\n{}\".format([sentences['azp1552_ch17_2'],\\\n",
    "                                              sentences['azp1552_ch17_3'],\\\n",
    "                                              sentences['azp1552_ch17_4']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LXML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T02:12:16.471632Z",
     "start_time": "2020-02-15T02:12:15.314064Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['azp1552_ch17', 'azp1556_ch17', 'azp1573_ch17']\n",
      "text['azp1552_ch17'] is:\n",
      "<root><div type=\"chapter\"><p> <milestone type=\"lera-segment\"/>¶ Do ſeptimo mandamento. N ão furtaras. Capit. xvi j̈. <milestone type=\"lera-segment\"/> PEra fundamento † das preguntas de ſte mãdam ẽto mandamento di ʒemos. Ho pri meyro que ha hi furto m ẽtal, ⁊ fur to real. Ho m ẽtal he võtade vontade de co meter ho real. Eho real he ſeg ũdo segundo Paulo l. 1. ff. đ fur. .§. 1. In ſtit. de obliga...\n"
     ]
    }
   ],
   "source": [
    "import lxml\n",
    "from lxml import etree\n",
    "import glob\n",
    "import re\n",
    "import os\n",
    "\n",
    "in_dir = \"./data/manual/\"\n",
    "\n",
    "sources = glob.glob(in_dir + '*.xml')\n",
    "\n",
    "parsed = { os.path.basename(file).split(os.extsep)[0] :\n",
    "                 (etree.parse(file))\n",
    "                     for file in sorted(sources)\n",
    "         }\n",
    "\n",
    "manuales = list(parsed.keys())\n",
    "print (manuales)\n",
    "\n",
    "nsmap = {\"tei\": \"http://www.tei-c.org/ns/1.0\"}\n",
    "\n",
    "def flatten(element):\n",
    "    t = \"\"\n",
    "    # Dagger milestones\n",
    "    if element.get(\"rendition\")==\"#dagger\":\n",
    "        t += \"†\"\n",
    "        if element.tail:\n",
    "            t += str.replace(element.tail, \"\\n\", \" \")\n",
    "    # asterisk milestones (additions in the 1556 ed.) - create temporary marker\n",
    "    elif element.get(\"rendition\")==\"#asterisk\":\n",
    "        t += \"*\"\n",
    "        if element.tail:\n",
    "            t += str.replace(element.tail, \"\\n\", \" \")\n",
    "    # Unanchored milestones - create temporary marker\n",
    "    elif element.get(\"rendition\")==\"#unanchored\":\n",
    "        t += \"‡\"\n",
    "        if element.tail:\n",
    "            t += str.replace(element.tail, \"\\n\", \" \")\n",
    "    else:\n",
    "        for c in element.iter(\"expan\"):\n",
    "            flatten(c)\n",
    "            if element.tail:\n",
    "                t += str.replace(element.tail, \"\\n\", \" \")\n",
    "        for c in element.iter(\"corr\"):\n",
    "            flatten(c)\n",
    "            if element.tail:\n",
    "                t += str.replace(element.tail, \"\\n\", \" \")\n",
    "        if element.text:\n",
    "            t += str.replace(element.text, \"\\n\", \" \")\n",
    "        if element.getchildren():\n",
    "            t += \" \".join((flatten(child)) for child in element.getchildren())\n",
    "        if element.tail:\n",
    "            t += str.replace(element.tail, \"\\n\", \" \")\n",
    "\n",
    "    return t\n",
    "\n",
    "xp_divs = etree.XPath(\"(//tei:body/tei:div[@type = 'chapter'][not(@n = '0')])\", namespaces = nsmap)\n",
    "\n",
    "divs = {}\n",
    "text = {}\n",
    "\n",
    "for ed in manuales:\n",
    "    t1 = \"\"\n",
    "    divs[ed] = xp_divs(parsed[ed])\n",
    "    t1  = \"\".join(\"++div--\" + re.sub('\\s+', ' ', '<p>' + flatten(div)) for div in divs[ed])\n",
    "    t2  = re.sub(r'¶', '++break--¶',                       t1)       # where pilcrow signs are\n",
    "    t3  = re.sub(r'([:\\.\\?\\]])\\s+([A-Z])(?!([CIJLVX]+|.)?\\.)(?![^†‡*]{0,80}[:\\.\\?\\]][^a-z]*[A-Z])(?=.{0,80}[†‡*])',\n",
    "                     r'\\1 ++break-- \\2',                   t2)       # sentences beginning\n",
    "                                                                     # with punctuation, whitespace, and a\n",
    "                                                                     # capital letter (not immediately followed by\n",
    "                                                                     # an abbreviation period)\n",
    "                                                                     # and a milestone follows within 80 characters\n",
    "                                                                     # (that do not contain a punctuation character)\n",
    "    t4  = re.sub(r'\\b([A-Z]{2}\\s*[a-z])', r'++break-- \\1', t3)       # two capital letters\n",
    "    t5  = t4[::-1]                                                   # reverse the string\n",
    "    t6  = re.sub(r'([†‡*])(?!.{0,100}--kaerb)', r'\\1--kaerb++', t5)  # daggers without sentence boundaries, i.e. not covered above\n",
    "    t7  = t6[::-1]                                                   # reverse the string\n",
    "    t8  = re.sub(r'‡', '',                                 t7)       # Eliminate temporary markers: unanchored milestones\n",
    "\n",
    "    # Concat everything and do a final removal of redundant breaks.\n",
    "    t9 = re.sub(r'\\+\\+break--\\s*\\+\\+break--', '++break--', \" \".join(t8.strip().split()))\n",
    "    \n",
    "    t10 = re.sub(r'\\+\\+break--', r'<milestone type=\"lera-segment\"/>', t9)\n",
    "    t11 = re.sub(r'\\+\\+div--', r'</div><div type=\"chapter\">', t10)\n",
    "    text[ed] = '<root>' + re.sub(r'&', '&amp;', t11)[6:] + '</div></root>'\n",
    "\n",
    "\n",
    "print(\"text['azp1552_ch17'] is:\\n{}...\".format(text['azp1552_ch17'][:400]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T02:15:55.741252Z",
     "start_time": "2020-02-15T02:15:55.720282Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus has 3 sentences.\n",
      "azp1552_ch17 has 387.\n",
      "\n",
      "Its first 5 sentences are:\n",
      "['<root><div type=\"chapter\"><p>', '¶ Do ſeptimo mandamento. N ão furtaras. Capit. xvi j̈.', 'PEra fundamento † das preguntas de ſte mãdam ẽto mandamento di ʒemos. Ho pri meyro que ha hi furto m ẽtal, ⁊ fur to real. Ho m ẽtal he võtade vontade de co meter ho real. Eho real he ſeg ũdo segundo Paulo l. 1. ff. đ fur. .§. 1. In ſtit. de obligat. qu æ ex de li. na ſc. contrata ç ã, ou tratam ẽto engano ſa do alheo cõtra contra võtade vontade do ſe ñor, pera auer a ꝓpriedade, ou po ſ ſi ſ ſam, ou ho v ſo della. Di ſ ſemos (c õtrata ç ã) porq̃ porque ſem ella n ão ha furto real in. d. author=\"bragagnolo\" timestamp=\"20190408T191343+0200\" comment=\"check\" : ainda q̃ que ho ha metal. Di ſ ſemos (do alheo) por q̃ ho tratam ẽto do ſeu, em q̃nto quanto ho he, ou cõ com re ʒ ão cree q̃ que he ſeu, n ão he furto. l. Inter o ẽs .§. Recte. ff. de furt. Acrec ẽtamos ( cõtra contra a võtade vontade do ſe ñor) porq̃ porque ſendo cõ com ſeu c õ ſ ẽ tim ẽto, n ão he furto in d. §. Re cte.. Di ſ ſemos (engano ſo) porq̃ porque ſe ſe fa ʒ por ʒ õbar, ou polo fa ʒer mais e ſperto: t ã pouco he furto. Di ſ ſemos ( ꝑa para auer a ꝓpriedade ou po ſ ſi ſ ſam) porq̃ porque aba ſta q̃rer querer auer alg ũa cou ſa \\uebd1 ſtas ꝑa para q̃ que ſeja furto, ſeg ũdo segundo todos.', '† Ho. ij. q̃ que por e ſte mãdam ẽto mandamento, como ẽ em outra ꝑte di ſ ſemos in additio. rep. e Q ñ de c õ ſecr. d. 1. n. 231., nã não ſom ẽte ſe defende o q̃ que ſecretam ẽte ſe toma ao ꝓ ximo cõtra contra ſuav õtade ( q̃ que ꝓpriam ẽte propriamente ſe chama fur to) mas ainda tudo q̃nto quanto ſe toma mal, ⁊mal ſe tẽ tem: ⁊ todo ho d ãno q̃ que inju ſtam ẽte ſe daa: ou porfor ça ou por leys inju ſtas, ou por outras v ſurpa ç ões il licitas c. penale. 14. q. 5. ⁊ tãb ẽ tambem toda võtade vontade deliberada \\uebd1 de tomar, reter, d ãnar, ⁊ v ſurpar illicitam ẽte cõtra contra a v õta \\uebd1 de ſeu dono, porq̃ porque como acima in c. li. n. 8., ⁊ em outra ꝑte in dicta ad diti. n. 233. di ſ ſemos, os pecados da võtade vontade, palaura, ⁊ obra de hũa huma me ſma q̃lidade ſ ã: ainda q̃ que os da soo v õ tade n ão obrig ã a re ſtitui ç ã, como os da obra ⁊ palaura.', '† Ho. iij. que apouquidade ⁊ indelibera ç ã e ſcu ſ ã de mortal, a ſ ſi ne ſta, como em toda outra mate ria, ſegundo acima ho di ſ ſemos in. d. c. 11. n. 4. : polo qual quẽ quem furta hũa huma ma ça ã, ainda que ſeja com animo de furtar, n ão pecca mais de venialmente, ſe n ão te ue enren ç ã de furtar cou ſa notauel, nẽ nem de dar d ã no notauel, ſe podera: doutra maneira ſi, ſeg ũdo segundo S. Tho. 2. S æc. q. 66. art. 6. Ant. 2. ꝑ. t. 1. c. 15. §. 1. et Syl. in ro ſa aurea. ca ſu. 38. porque ni ſto n ão tam ſomente ſe tem re ſpeyto ao que ſe toma, mas aa enten ç ão ⁊ v õ tade do que furta: ſegundo S. Hieronymo in c. fin. 14. q. 4. , ao menos quanto ao foro da con ſciencia, como em outra parte ho di ſ ſemos in repet. d. c. fin. . E ſcu ſao porem ainda de venial a ignorancia prouauel, de n ão ſaber q̃ que a cou ſa era alhea, ⁊ a ſua grande nece ſ ſidade, a juy ʒo de bõ bom var ão c. Si quis ꝓ pter nece ſ ſitatem de furt, vbi Pan. &amp; alii : ⁊ tambẽ tambem quãdo quando cree cõ com cau ſa prouauel, que ho ſe ñor da cou ſa ho auera por b ẽ, l. inter o ẽs .§. Recte. ff de furt.']\n"
     ]
    }
   ],
   "source": [
    "sentences = {}\n",
    "nos = {}\n",
    "for ed in manuales:\n",
    "    sentences[ed] = {}\n",
    "    segments = text[ed].split('<milestone type=\"lera-segment\"/>')\n",
    "    nos[ed] = len(segments)\n",
    "    for i, s in enumerate(segments):\n",
    "        sentences[ed][ed + '_' + str(i)] = s.strip()\n",
    "\n",
    "print(\"Corpus has {} sentences.\".format(len(sentences)))\n",
    "print(\"azp1552_ch17 has {}.\\n\".format(nos['azp1552_ch17']))\n",
    "\n",
    "print(\"Its first 5 sentences are:\\n{}\".format([sentences['azp1552_ch17']['azp1552_ch17_0'],\\\n",
    "                                               sentences['azp1552_ch17']['azp1552_ch17_1'],\\\n",
    "                                               sentences['azp1552_ch17']['azp1552_ch17_2'],\\\n",
    "                                               sentences['azp1552_ch17']['azp1552_ch17_3'],\\\n",
    "                                               sentences['azp1552_ch17']['azp1552_ch17_4']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save sentences as plaintext files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T02:16:06.609486Z",
     "start_time": "2020-02-15T02:16:06.554091Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "for ed in manuales:\n",
    "    with open('./data/manual/' + ed + '_seg.csv', 'w', encoding='utf-8') as csv_file:\n",
    "        writer = csv.writer(csv_file, lineterminator=\"\\n\")\n",
    "        for key, value in sentences[ed].items():\n",
    "           writer.writerow([key, value])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {},
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "threshold": "2",
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false,
   "widenNotebook": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
