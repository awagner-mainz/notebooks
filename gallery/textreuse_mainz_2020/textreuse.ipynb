{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Sprachübergreifende-Textalignierung\" data-toc-modified-id=\"Sprachübergreifende-Textalignierung-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Sprachübergreifende Textalignierung</a></div><div class=\"lev1 toc-item\"><a href=\"#Manuales-(sprachübergreifende-Alignierung)\" data-toc-modified-id=\"Manuales-(sprachübergreifende-Alignierung)-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Manuales (sprachübergreifende Alignierung)</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprachübergreifende Textalignierung\n",
    "\n",
    "Blockseminar Studiengang \"Digitale Methodik in den Geistes- und Kulturwissenschaften\" (18.1.2020, 8.2.2020, 15.2.2020)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "\n",
    "import re\n",
    "import locale\n",
    "locale.setlocale(locale.LC_ALL, '')  # Use '' for auto, or force e.g. to 'en_US.UTF-8'\n",
    "from collections import OrderedDict\n",
    "from decimal import Decimal\n",
    "from functools import partial\n",
    "from itertools import chain\n",
    "import ctypes\n",
    "import numpy as np\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import lxml\n",
    "from lxml import etree\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "import tabulate\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "import nltk\n",
    "import nltk.translate.gale_church\n",
    "\n",
    "import bleualign.gale_church   # from Rico Sennrich's Bleualign: https://github.com/rsennrich/Bleualign\n",
    "# import _align from gale-church   # from Li Ling Tan's https://github.com/alvations/gachalign\n",
    "\n",
    "# Freeling\n",
    "aux_dir  = \"\\\\auxiliary_files\"\n",
    "nb_dir   = os.path.split(os.getcwd())[0] + \"\\\\\" + os.path.split(os.getcwd())[1] + aux_dir\n",
    "\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "\n",
    "from auxiliary_files import pyfreeling\n",
    "\n",
    "pyfreeling.util_init_locale(\"default\")\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T15:02:09.726254Z",
     "start_time": "2020-01-18T15:02:09.717089Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 files read:\n",
      "['1867_-_GB-NA_-_Great_Britain_-_british_north_american_act', '1909_-_GB-SA_-_Great_Britain_-_british_south_africa_act', '1935_-_GB-BU_-_Great_Britain_-_british_government_of_burma_act', '1937_-_IE_-_Ireland_-_constitution_of_ireland', '1948_-_BU_-_Burma_-_constitution_of_burma']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "in_dir = \"./data/constitutions/\"\n",
    "\n",
    "# we create a dictionary with our constitutions:\n",
    "sources = {}\n",
    "\n",
    "for file in sorted(os.listdir(in_dir)):\n",
    "    key = os.path.basename(file).split(os.extsep)[0]\n",
    "    with open(in_dir + '/' + file, encoding=\"utf-8\") as f:\n",
    "        sources[key] = f.read()\n",
    "\n",
    "# and a list of available constitutions for quick lookup:\n",
    "constitutions = list(sources.keys())\n",
    "\n",
    "print (\"{} files read:\".format(len(constitutions)))\n",
    "print (constitutions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T14:49:05.940842Z",
     "start_time": "2020-01-18T14:49:05.326798Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus has 3240 sentences.\n",
      "1948_-_BU_-_Burma_-_constitution_of_burma has 963.\n",
      "\n",
      "Its first 3 sentences are:\n",
      "['The Constitution of the Union of Burma, 24 September 1947, Effective 4 January 1948\\n\\nPREAMBLE\\nWE, THE PEOPLE OF BURMA including the Frontier Areas and the Karenni States, Determined to establish in strength and unity a\\nSOVEREIGN INDEPENDENT STATE, To maintain social order on the basis of the eternal principles of JUSTICE, LIBERTY AND EQUALITY\\nand To guarantee and secure to all citizens JUSTICE social, economic and political; LIBERTY of thought, expression, belief,\\nfaith, worship, vocation, association and action; EQUALITY of status, of opportunity and before the law, IN OUR CONSTITUENT\\nASSEMBLY this Tenth day of Thadingyut waxing, 1309 B.E.', '(Twentyfourth day of September, 1947 A.D.), DO HEREBY ADOPT,\\nENACT AND GIVE TO OURSELVES THIS CONSTITUTION.', 'CHAPTER I.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import tokenize\n",
    "# nltk.download('punkt')\n",
    "\n",
    "sentences = {}\n",
    "nos = {}\n",
    "for c in constitutions:\n",
    "    t = tokenize.sent_tokenize(sources[c])\n",
    "    nos[c] = len(t)\n",
    "    for i, s in enumerate(t):\n",
    "        sentences[c + '_' + str(i)] = s\n",
    "\n",
    "boundary = len(sentences) - nos['1948_-_BU_-_Burma_-_constitution_of_burma']\n",
    "print(\"Corpus has {} sentences.\".format(len(sentences)))\n",
    "print(\"1948_-_BU_-_Burma_-_constitution_of_burma has {}.\\n\".format(nos['1948_-_BU_-_Burma_-_constitution_of_burma']))\n",
    "\n",
    "print(\"Its first 3 sentences are:\\n{}\".format([sentences['1948_-_BU_-_Burma_-_constitution_of_burma_0'],\\\n",
    "                                              sentences['1948_-_BU_-_Burma_-_constitution_of_burma_1'],\\\n",
    "                                              sentences['1948_-_BU_-_Burma_-_constitution_of_burma_2']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T14:52:29.645985Z",
     "start_time": "2020-01-18T14:52:29.463923Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3240, 4685)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer='word', strip_accents='unicode', stop_words=[\"the\", \"of\", \"and\"])\n",
    "dfm = vectorizer.fit_transform(sentences.values())\n",
    "\n",
    "print(dfm.shape)\n",
    "print(type(dfm))\n",
    "print(dfm.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T14:54:09.837001Z",
     "start_time": "2020-01-18T14:54:09.774215Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(963, 4685)\n",
      "(2277, 4685)\n",
      "(963, 2277)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "target = dfm[boundary:]\n",
    "sources = dfm[:boundary,]\n",
    "print(target.shape)\n",
    "print(sources.shape)\n",
    "\n",
    "simils = cosine_similarity(target, sources)\n",
    "print(simils.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T14:59:24.058415Z",
     "start_time": "2020-01-18T14:59:24.039714Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1703, 1898,   89,  589,    0, 1741,    0, 1334,    0,  499,    0,\n",
       "       1905,    0, 1757,    0, 1765,    0, 1765,    0, 2099,    6, 2199,\n",
       "        589,    0,  536, 1911,  992,  927,  994, 1475,  997,  256, 2199,\n",
       "        999, 1313,  529, 1680, 1005, 1749, 2199, 1007, 2206, 1009, 2213,\n",
       "       2216, 2218, 2198, 1653, 1012, 2161, 1014,  948,    0, 1571, 1011,\n",
       "       1016, 2241,    0, 2155,    0, 2200, 1018, 2239, 2240, 2243, 2272,\n",
       "       2199, 1020, 2014, 2199, 1022, 2233, 2234, 1888, 2215, 2272, 1297,\n",
       "       1024, 1426, 1349, 1026, 2272, 1653, 2102, 1198, 1043, 1200, 2102,\n",
       "       1206, 1230, 1209, 2274,   20, 1659, 1215, 1902, 1690,  615, 1947,\n",
       "       1217, 1916, 1349,   38, 2247, 1220, 2247, 2248, 1226, 2234,  797,\n",
       "       1232, 2230, 1236, 2249, 1240, 1995, 1243, 2258, 2224, 1247, 2130,\n",
       "       1251, 1577, 1254,  589, 2201, 1258,    7, 1262, 2200, 1376,  645,\n",
       "        531,  648,  789,  789,  570, 1283, 1919, 1286, 1920, 1931, 1294,\n",
       "       1932, 1933, 1934,  659, 1923, 1833, 1303, 1930, 1305, 1935, 1309,\n",
       "       1936,  122, 1938, 1325, 1948, 1327, 1939, 1940, 1941, 1942, 1943,\n",
       "       1944, 1945, 1331, 1947, 1948, 1333, 1949, 1949, 1951, 1335,  990,\n",
       "       1338, 1955, 1956, 1342, 1109, 1347,  927, 1351, 1961, 1962, 1355,\n",
       "       1964, 1965, 1130, 1357, 1966, 1120, 1362, 1969, 1970, 1177, 1971,\n",
       "       1971, 1971, 1974, 1976,  204, 1159,  542, 1369,  548, 1374,  654,\n",
       "       1176, 1177, 1178, 1296, 1378, 1201, 1202, 1385, 1181, 2000, 1183,\n",
       "       1349, 1390, 2001, 1398, 2004, 1404, 1187,  166, 2003, 1190, 1191,\n",
       "       1410, 1193, 1195, 1196, 1416, 1199, 1422, 2005, 2006, 1921, 2007,\n",
       "       1922, 1424, 2021, 1428, 2161, 1431, 1173, 1433,  686, 1211, 1214,\n",
       "       1437, 1218, 1443, 1221, 1222, 1100, 1445, 2009,    0,  598, 2010,\n",
       "       2011, 2012, 1449, 2016, 2017, 2018, 1451,  544, 1454, 2022, 2023,\n",
       "       2024, 1157, 1457, 1163, 1460, 1239, 2035, 1462, 2035, 1223, 1466,\n",
       "        763, 1471, 2205, 1985, 1474, 1519,  206,  234, 1982, 1520, 1476,\n",
       "       1075, 1478, 1653, 1987, 1655, 1656, 1480, 1591, 1482, 1594, 1641,\n",
       "       1485, 1990, 1991, 1224, 2041, 1496, 1360, 1500, 2044, 1503,  698,\n",
       "       1509, 1000,  692, 2045, 1513, 2046, 1517, 2047, 2048, 1525, 2049,\n",
       "       2050, 1530, 2051, 2052, 2053, 2054, 2055, 2056, 1535, 2057, 1543,\n",
       "        702, 1246, 1546, 1259, 1265, 1265,  717, 1261, 2066, 1550, 2066,\n",
       "       2067, 1271, 1556, 2092, 2071, 2070, 1559, 2074,  259,    7, 1564,\n",
       "       1391, 1568, 2104, 1572, 1118, 1576, 2116, 2117, 2118, 1579, 2119,\n",
       "       1582, 2120,  866, 2121, 2122, 1590, 1152, 1153, 1154, 1285,  872,\n",
       "       1653, 1596, 2100, 2101, 1598, 2109, 1601, 2107, 1163, 1159, 1605,\n",
       "       1140, 1141, 1607, 2137, 2138, 2139, 2140, 2141, 1159, 2153, 1611,\n",
       "       1370, 2155, 1371, 1616, 1371, 1622, 2156,  903, 2160,  269,    7,\n",
       "       1630, 2161,  914, 2163, 1436, 1642,  536, 2208, 1644, 2167, 2168,\n",
       "       2169, 1646, 2171, 1648, 2172, 1652, 2173, 2175, 2176, 2177, 1660,\n",
       "       1798, 2178, 1669, 2179, 1675, 1440, 1440, 1441, 1677,  656, 2181,\n",
       "       1940, 1941, 1942, 1591, 2168, 1944, 2055, 1943, 2161, 1945, 1679,\n",
       "       1444, 1683, 1446, 1689, 2198, 1447,  947, 1447, 1695, 1981, 1697,\n",
       "       2187, 2188, 1704, 2208, 2184, 1711, 2269, 1713, 1975,  320, 1614,\n",
       "        589, 1715,  531, 2201, 2026, 1933, 1721,  769, 1723,  777, 2083,\n",
       "       1726, 2081, 2084, 2085, 2086, 2088, 2086, 1730, 2077, 1737,  733,\n",
       "       2105,    0,  531, 1089,    0, 2099, 1349,    0, 2142, 1902, 1634,\n",
       "          0, 1647,    0, 2107, 1339,    0, 2230, 1100,  589, 2142,    0,\n",
       "        531, 1782, 2140,  600,  497, 1634,    0,  769,    0,  777, 2083,\n",
       "          0, 2081, 2084, 2085, 2086, 2088, 2086,    0, 2077,    0,  733,\n",
       "       2105,  531, 1089,    0, 2099,  761,    0, 2142, 1902,  742,    0,\n",
       "       1647,    0, 2107, 1339,    0, 1976,    0, 2230, 1157,  589,    0,\n",
       "        129,    0, 1765, 1947, 1772,  531, 2140,  769,  777, 2083, 2081,\n",
       "       2084, 2085, 2086, 2088, 2086, 2077,  733, 2105,    0,  129,    0,\n",
       "        531, 1089, 2099, 1349, 2142, 1634, 1634, 1647, 2107, 1656, 1976,\n",
       "       2142, 1223,  589,    0, 2256, 1947,    0,  531, 2140,    0,  769,\n",
       "          0,  777, 2083,    0, 2081, 2084, 2085, 2086, 2088, 2086,    0,\n",
       "       2077,    0,  733, 2105,    0,  531, 1089,    0, 2099, 1349,    0,\n",
       "       2142, 1634, 1567,    0, 1647,    0, 2107, 1339,    0, 2230, 1614,\n",
       "          0, 1947,    0,  531,   27, 1155, 1718, 1933,    0, 1011,  204,\n",
       "          0, 1649,    0, 1256,   89, 2220,    0,  254,    0, 2070,    0,\n",
       "       2230, 2093, 2182,    0,  531,    0, 2053,    0, 1931,  356,  959,\n",
       "          0, 2259,    0, 2261, 2262,    0, 2042,  614, 2072, 1043, 1246,\n",
       "          0, 2096, 1650, 2124,    0, 2126,    0, 2124,    0, 2129, 1406,\n",
       "       2130,    0, 2131,    0, 2132, 2132, 1658,  951,    0, 1577,  722,\n",
       "        631, 2091, 2085,  574,    0, 1216,    0,  782, 2248,    0, 2272,\n",
       "          0, 1066, 1358,    0, 1075,    0, 1588,   42, 1859,    0, 1731,\n",
       "       1902, 1332,   89,  951,    0, 1098, 1617, 2179,    0, 1632,    7,\n",
       "       1634,    0, 2274, 1698,    0, 1618,    0, 2274,  855,    0, 1561,\n",
       "          0, 1724, 1725,    0,  500, 1919,  931, 2263,    0,  531,  706,\n",
       "       1045, 2216, 1899, 2100,    0,  207, 1321, 1577,  496,  357,  207,\n",
       "        330,  230,    0,  992,  994,  997, 2190,  357, 1005, 1007,    0,\n",
       "        216,    0,    7,  751,  878,  215, 2208, 2205, 2272,  215,  994,\n",
       "          0,  209,  336,  176, 2266, 2266, 1897,  289, 2266, 1218,  992,\n",
       "        994,  223, 1966,  529, 1450,    0,  220,  219,    7,  870, 1066,\n",
       "       1359,   18,    7, 1349,  657,  994,  232,  250,  529, 1005, 1007,\n",
       "       1009, 1012, 1014, 1854, 1018, 1020, 1022, 1024, 1026,  223, 1200,\n",
       "       1206, 1209, 1215,  222, 1220, 2032,  498, 1236,  874, 1243, 1450,\n",
       "       1813,  251,    6,  155,  662,    0, 2099, 1581, 2230,  589,    0,\n",
       "        946, 2223, 1004, 1577,  765,  463, 2185, 1203,    6,   20, 1234,\n",
       "       2266,  992,  994,  997,    0,  537,  411, 2187, 1450,  242, 1813,\n",
       "          0,  246,  244,  217,  765,    0,  253, 2198, 2198,  792,    0,\n",
       "        464,  243,  633, 1680,    0, 2050,  244,    0, 1159,  251,  252,\n",
       "       1816,  589,  662,    0,  318,    6,   20, 2266, 2266,  204,    0,\n",
       "       1203,    6,   20, 1234,    0, 1436,    0,    0,    0,  633,    0,\n",
       "       2266,    0, 2266,    0, 2266,    0,  340,  992, 1532,  997,  999,\n",
       "        875,    0, 1504,  253, 1510,  464,  257,    0, 2124,  633,  896,\n",
       "       1577,    0,    0,    0,    0, 1429])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.argmax(simils, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manuales (sprachübergreifende Alignierung)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T09:57:35.350483Z",
     "start_time": "2020-02-08T09:57:35.316382Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 files read:\n",
      "['azp1552_ch17', 'azp1556_ch17', 'azp1573_ch17']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "in_dir = \"./data/manual/\"\n",
    "\n",
    "# we create a dictionary with our manuales:\n",
    "sources = {}\n",
    "\n",
    "for file in sorted(os.listdir(in_dir)):\n",
    "    key = os.path.basename(file).split(os.extsep)[0]\n",
    "    with open(in_dir + '/' + file, encoding=\"utf-8\") as f:\n",
    "        sources[key] = f.read()\n",
    "\n",
    "# and a list of available constitutions for quick lookup:\n",
    "manuales = list(sources.keys())\n",
    "\n",
    "print (\"{} files read:\".format(len(manuales)))\n",
    "print (manuales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T10:05:06.674849Z",
     "start_time": "2020-02-08T10:05:04.694856Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus has 23208 sentences.\n",
      "azp1552_ch17 5477.\n",
      "\n",
      "Its first 3 sentences are:\n",
      "['S. Franci<g ref=\"#char017f\">ſ</g>co <lb xml:id=\"Azp1552-00-0007-lb-0008\"/>da\\n                    prouincia da piedade: E vi<g ref=\"#char017f\">ſ</g>to <g ref=\"#char0026\">&amp;</g> em <choice xml:id=\"d2e2063\"><abbr>alg<g ref=\"#charu0303\">ũ</g>s</abbr><expan resp=\"#auto\">alguns</expan></choice> <lb xml:id=\"Azp1552-00-0007-lb-0009\"/>pa<g ref=\"#char017f\">ſ</g><g ref=\"#char017f\">ſ</g>os declarado polo muy famo<g ref=\"#char017f\">ſ</g>o Doutor <lb xml:id=\"Azp1552-00-0007-lb-0010\"/>Martim de Azpilcueta Nauarro, cathredati<lb xml:id=\"Azp1552-00-0007-lb-0011\" break=\"no\" rendition=\"#hyphen\"/>co iubilado de\\n                    Prima em Canones na vniuer<lb break=\"no\" rendition=\"#noHyphen\" xml:id=\"Azp1552-00-0007-lb-0012\"/><g ref=\"#char017f\">ſ</g>idade de Coimbra.', 'E de<g ref=\"#char017f\">ſ</g>pois <choice xml:id=\"d2e2091\"><abbr>c<g ref=\"#charo0303\">õ</g></abbr><expan resp=\"#auto\">com</expan></choice> <g ref=\"#char017f\">ſ</g>ummo cui<lb xml:id=\"Azp1552-00-0007-lb-0013\" break=\"no\" rendition=\"#hyphen\"/>dado,\\n                    <choice xml:id=\"d2e2101\"><abbr>dilig<g ref=\"#chare0303\">ẽ</g>cia</abbr><expan resp=\"#auto\">diligencia</expan></choice> <g ref=\"#char0026\">&amp;</g> e<g ref=\"#char017f\">ſ</g>tudo, t<g ref=\"#chara0303\">ã</g> reformado <g ref=\"#char0026\">&amp;</g> acre<lb break=\"no\" rendition=\"#noHyphen\" xml:id=\"Azp1552-00-0007-lb-0014\"/>c<g ref=\"#chare0303\">ẽ</g>tado polo me<g ref=\"#char017f\">ſ</g>mo\\n                    Author <g ref=\"#char0026\">&amp;</g> o dito Doutor <lb xml:id=\"Azp1552-00-0007-lb-0015\"/>em materias,\\n                    <g ref=\"#char017f\">ſ</g>enten<g ref=\"#charc0327\">ç</g>as, allega<g ref=\"#charc0327\">ç</g><g ref=\"#charo0303\">õ</g>es <g ref=\"#char0026\">&amp;</g> e<g ref=\"#char017f\">ſ</g>tilo, <g ref=\"#chare68b\">ꝗ̃</g> <lb xml:id=\"Azp1552-00-0007-lb-0016\"/>pode\\n                    parecer outro, com Reportorio copio<g ref=\"#char017f\">ſ</g>o <lb xml:id=\"Azp1552-00-0007-lb-0017\"/>no\\n                    cabo.</titlePart>\\n                <byline> <lb xml:id=\"Azp1552-00-0007-lb-0018\"/>Anno de M. D. LII.', '<lb xml:id=\"Azp1552-00-0007-lb-0019\"/>Venden<g ref=\"#char017f\">ſ</g>e em Coimbra a cento <lb xml:id=\"Azp1552-00-0007-lb-0020\"/><g ref=\"#char0026\">&amp;</g> <g ref=\"#char017f\">ſ</g>e<g ref=\"#char017f\">ſ</g><g ref=\"#char017f\">ſ</g>enta reaes, em papel.</byline>\\n                <imprimatur> <lb xml:id=\"Azp1552-00-0007-lb-0021\"/>Com Priuilegio.</imprimatur>\\n            </titlePage>\\n<div type=\"part\" xml:id=\"Azp1552-00-0008-d1-03e8\" xml:lang=\"es\">\\n<p rend=\"hx\" xml:id=\"Azp1552-00-0008-pa-03ea\" part=\"N\"><pb n=\"[ii]\" facs=\"facs:Azp1552-0008\" xml:id=\"Azp1552-00-0008-pb-03e9\"/>\\n<lb xml:id=\"Azp1552-00-0008-lb-0001\"/>Al muy alto, y muy\\n<lb xml:id=\"Azp1552-00-0008-lb-0002\"/>excellente <g ref=\"#char017f\">ſ</g>e<g ref=\"#charn0303\">ñ</g>or, el Cardenal Infante dom\\n<lb xml:id=\"Azp1552-00-0008-lb-0003\"/>Henrrique, el doctor Martin de Azpilcueta Nauar<lb xml:id=\"Azp1552-00-0008-lb-0004\" break=\"no\" rendition=\"#hyphen\"/>ro, per<g ref=\"#char017f\">ſ</g>euerancia en <g ref=\"#char017f\">ſ</g>us heroicas virtudes, y corona\\n<lb xml:id=\"Azp1552-00-0008-lb-0005\"/>muy <g ref=\"#char017f\">ſ</g>ublime dellas.</p>\\n<p xml:id=\"Azp1552-00-0008-pa-03eb\" part=\"N\"> <lb xml:id=\"Azp1552-00-0008-lb-0006\"/><hi rendition=\"#initCaps\">P</hi>OR muchos re<g ref=\"#char017f\">ſ</g>pectos, muy alto, y muy excel<lb break=\"no\" rendition=\"#noHyphen\" xml:id=\"Azp1552-00-0008-lb-0007\"/>lente <g ref=\"#char017f\">ſ</g>e<g ref=\"#charn0303\">ñ</g>or, o<g ref=\"#char017f\">ſ</g>o\\n                    dedicar a. V. A. el fruto de los <lb xml:id=\"Azp1552-00-0008-lb-0008\"/>trabajos\\n                    pue<g ref=\"#char017f\">ſ</g>tos por mi parte, en la reforma<lb xml:id=\"Azp1552-00-0008-lb-0009\" break=\"no\" rendition=\"#hyphen\"/>ci<g ref=\"#charo0303\">õ</g> de<g ref=\"#char017f\">ſ</g>te Manual de c<g ref=\"#charo0303\">õ</g>fe<g ref=\"#char017f\">ſ</g><g ref=\"#char017f\">ſ</g>ores.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import tokenize\n",
    "# nltk.download('punkt')\n",
    "\n",
    "sentences = {}\n",
    "nos = {}\n",
    "for c in manuales:\n",
    "    t = tokenize.sent_tokenize(sources[c])\n",
    "    nos[c] = len(t)\n",
    "    for i, s in enumerate(t):\n",
    "        sentences[c + '_' + str(i)] = s\n",
    "\n",
    "print(\"Corpus has {} sentences.\".format(len(sentences)))\n",
    "print(\"azp1552_ch17 {}.\\n\".format(nos['azp1552_ch17']))\n",
    "\n",
    "print(\"Its first 3 sentences are:\\n{}\".format([sentences['azp1552_ch17_2'],\\\n",
    "                                              sentences['azp1552_ch17_3'],\\\n",
    "                                              sentences['azp1552_ch17_4']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T10:39:20.605485Z",
     "start_time": "2020-02-08T10:39:20.039498Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t[azp1552_ch17] is '++div_17--   ¶ Do  ſeptimo mandamento. N ão furtaras.  Capit. xvi j̈.    PEra fundamento   das preguntas  de ſte  mãdam ẽto mandamento di ʒemos. Ho pri meyro que ha hi furto m ẽtal,  ⁊ fur to real. Ho m ẽtal he  võtade vontade de co meter ho real. Eho real he  ſeg ũdo segundo  Paulo  l. 1. ff.  đ fur.  .§. 1. In ſtit.  de obligat.  qu æ ex de  li. na ſc. contrata ç ã, ou tratam ẽto  engano ſa '.\n"
     ]
    }
   ],
   "source": [
    "import lxml\n",
    "from lxml import etree\n",
    "# import re\n",
    "\n",
    "parsed = { os.path.basename(file).split(os.extsep)[0] :\n",
    "                 (etree.parse(in_dir + \"/\" + file))\n",
    "                     for file in sorted(os.listdir(in_dir))\n",
    "         }\n",
    "\n",
    "nsmap = {\"tei\": \"http://www.tei-c.org/ns/1.0\"}\n",
    "\n",
    "def flatten(element: lxml.etree._Element):\n",
    "    t = \"\"\n",
    "    if element.text:\n",
    "        t += str.replace(element.text, \"\\n\", \" \")\n",
    "    if element.getchildren():\n",
    "        t += \" \".join((flatten(child)) for child in element.getchildren())\n",
    "    if element.tail:\n",
    "        t += str.replace(element.tail, \"\\n\", \" \")\n",
    "    return t\n",
    "\n",
    "xp_divs = etree.XPath(\"(//tei:body/tei:div[@type = 'chapter'][not(@n = '0')])\", namespaces = nsmap)\n",
    "\n",
    "divs = {}\n",
    "t = {}\n",
    "\n",
    "for c in manuales:\n",
    "    divs[c] = xp_divs(parsed[c])\n",
    "\n",
    "    t[c] = \"\".join(\"++div_\" + str(div.get(\"n\")) + \"--\" + flatten(div) for div in divs[c])\n",
    "\n",
    "print(\"t[azp1552_ch17] is '{}'.\".format(t['azp1552_ch17'][:400]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T10:41:09.762438Z",
     "start_time": "2020-02-08T10:41:09.192821Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus has 19522 sentences.\n",
      "azp1552_ch17 5054.\n",
      "\n",
      "Its first 10 sentences are:\n",
      "['++div_17--   ¶ Do  ſeptimo mandamento.', 'N ão furtaras.', 'Capit.', 'xvi j̈.', 'PEra fundamento   das preguntas  de ſte  mãdam ẽto mandamento di ʒemos.', 'Ho pri meyro que ha hi furto m ẽtal,  ⁊ fur to real.', 'Ho m ẽtal he  võtade vontade de co meter ho real.', 'Eho real he  ſeg ũdo segundo  Paulo  l. 1. ff.', 'đ fur.', '.§.']\n"
     ]
    }
   ],
   "source": [
    "sentences = {}\n",
    "nos = {}\n",
    "for c in manuales:\n",
    "    segments = tokenize.sent_tokenize(t[c])\n",
    "    nos[c] = len(segments)\n",
    "    for i, s in enumerate(segments):\n",
    "        sentences[c + '_' + str(i)] = s\n",
    "\n",
    "print(\"Corpus has {} sentences.\".format(len(sentences)))\n",
    "print(\"azp1552_ch17 {}.\\n\".format(nos['azp1552_ch17']))\n",
    "\n",
    "print(\"Its first 10 sentences are:\\n{}\".format([sentences['azp1552_ch17_0'],\\\n",
    "                                              sentences['azp1552_ch17_1'],\\\n",
    "                                              sentences['azp1552_ch17_2'],\\\n",
    "                                              sentences['azp1552_ch17_3'],\\\n",
    "                                              sentences['azp1552_ch17_4'],\\\n",
    "                                              sentences['azp1552_ch17_5'],\\\n",
    "                                              sentences['azp1552_ch17_6'],\\\n",
    "                                              sentences['azp1552_ch17_7'],\\\n",
    "                                              sentences['azp1552_ch17_8'],\\\n",
    "                                              sentences['azp1552_ch17_9']]))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {},
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "threshold": "2",
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false,
   "widenNotebook": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
