{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Preparations\" data-toc-modified-id=\"Preparations-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Preparations</a></div><div class=\"lev2 toc-item\"><a href=\"#Get-fulltext-\" data-toc-modified-id=\"Get-fulltext--11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Get fulltext </a></div><div class=\"lev2 toc-item\"><a href=\"#Segment-source-text\" data-toc-modified-id=\"Segment-source-text-12\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Segment source text</a></div><div class=\"lev2 toc-item\"><a href=\"#Read-segments-into-a-variable-\" data-toc-modified-id=\"Read-segments-into-a-variable--13\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Read segments into a variable </a></div><div class=\"lev2 toc-item\"><a href=\"#Tokenising-\" data-toc-modified-id=\"Tokenising--14\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Tokenising </a></div><div class=\"lev2 toc-item\"><a href=\"#Stemming-/-Lemmatising-\" data-toc-modified-id=\"Stemming-/-Lemmatising--15\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Stemming / Lemmatising </a></div><div class=\"lev2 toc-item\"><a href=\"#Eliminate-Stopwords-\" data-toc-modified-id=\"Eliminate-Stopwords--16\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Eliminate Stopwords </a></div><div class=\"lev1 toc-item\"><a href=\"#Characterise-passages:-TF/IDF\" data-toc-modified-id=\"Characterise-passages:-TF/IDF-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Characterise passages: TF/IDF</a></div><div class=\"lev1 toc-item\"><a href=\"#Find-similar-passages:-Clustering\" data-toc-modified-id=\"Find-similar-passages:-Clustering-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Find similar passages: Clustering</a></div><div class=\"lev1 toc-item\"><a href=\"#Topic-Modelling\" data-toc-modified-id=\"Topic-Modelling-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Topic Modelling</a></div><div class=\"lev1 toc-item\"><a href=\"#Manual-Annotation\" data-toc-modified-id=\"Manual-Annotation-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Manual Annotation</a></div><div class=\"lev1 toc-item\"><a href=\"#Cope-with-different-languages\" data-toc-modified-id=\"Cope-with-different-languages-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Cope with different languages</a></div><div class=\"lev1 toc-item\"><a href=\"#Further-information\" data-toc-modified-id=\"Further-information-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Further information</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text Processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an introduction to some algorithms used in text analysis. While I cannot define **what questions** a scholar can ask, I can and do describe here **the kind of information** about text that some popular methods deliver. From this, you need to draw on your own research interests and creativity..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will describe methods of finding words that are characteristic for a certain passage (\"tf/tdf\"), constructing fingerprints for passages that go beyond the most significant words (\"word vectors\"), group passages according to their similarity (\"clustering\"), and forming an idea about different contexts being treated in a passage (\"topic modelling\"). Of course, an important resource in text analysis is the hermeneutic interpretation of the scholar herself, so I will present a method of adding manual annotations to the text, and finally I will also say something about possible approaches to working across languages.\n",
    "This page will *not* cover stylistic analyses (\"stylometry\") and typical neighborship relations between words (\"collocation\", \"word2vec\"). Maybe these can be discussed at another occasion and on another page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For many of the steps discussed on this page there are ready-made tools and libraries, often with easy interfaces. But first, it is important to understand what these tools are actually doing and how their results are affected by the selection of parameters (that one can or cannot modify). And second, most of these tools expect the input to be in some particular format, say, a series of plaintext files in their own directory. So, by understanding the process, you should be better prepared to provide your text to the tools in the most productive way. Finally, it is important to be aware of what information has been **lost** at which point in the process. If the research requires so, one can then either look for a different tool or approach to this step (e.g. using an additional dimension in the list of words to keep both original and regularized word forms, or to remember the position of the current token in the original text), or one can compensate for the data loss (e.g. offering a lemmatised search to find occurrences after the analysis returns only normalised word forms)..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As indicated above, before doing maths, language processing tools normally expect their input to be in a certain format. First of all, you have to have an input in the first place: Therefore, a scholar wishing to experiment with such methods should avail herself of the text that should be studied, as a full transcription. This can be done by transcribing it herself, using transcriptions that are available from elsewhere, or even from OCR. (Although in the latter case, the results depend of course on the quality of the OCR output.) Second, many tools get tripped up when formatting or bibliographical metainformation is included in their input. And since the approaches presented here are not concerned with a digital edition or any other form of true representation of the source, *markup* (e.g. for bold font, heading or note elements) should be *suppressed*. (Other tools accept marked up text and strip the formatting internally.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For another detail regarding these plain text files, we have to make a short excursus, because even with plain text, there are some important aspects to consider: As you surely know, computers understand number only and as you probably also know, the first standards to encode alphanumeric characters, like ASCII, in numbers were designed for teleprinters and the reduced character set of the english language. When more extraordinary characters, like *Umlauts* or *accents* were to be encoded, one had to rely on extra rules, of which - unfortunately - there have been quite a lot. These are called \"encodings\" and one of the more important set of such rules are the windows encodings (e.g. CP-1252), another one is called Latin-9/ISO 8859-15 (it differs from the older Latin-1 encoding among others by including the Euro sign). Maybe you have seen web pages with garbled *Umlauts* or other special characters, then that was probably because your browser interpreted the numbers according to an encoding different from the one that the webpage author used. Anyway, the point here is that there is another standard encompassing virtually all the special signs from all languages and for a few years now, it is also supported quite well by operating systems, programming languages and linguistic tools. This standard is called \"Unicode\" and the encoding you want to use is called *utf-8*. So when you export or import your texts, try to make sure that this is what is used. ([Here](https://unicode-table.com/) is a webpage with the complete unicode table - it is loaded incrementally, so make sure to scroll down. But on the other hand, it is so extensive that you don't want to scroll through all the table...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, you should consider whether or not you can replace *abbreviations* with their expanded versions. While at some points (e.g. when lemmatising), you can associate expansions to abbreviations, the whole processing is easier when words in the text are indeed words, and periods are rather sentence punctuation than abbreviation signs. Of course, this also depends on the effort you can spend on the text...\n",
    "\n",
    "This section describes how the plaintext can further be prepared for analyses: E.g. if you want to process the *distribution* of words in the text, the processing method has to have some notion of different places in the text -- normally you want to manage words not according to their absolute position in the whole work (say, the 6.349th word and the 3.100th), but according to their occurrence in a particular section (say, in the third chapter, without caring too much whether it is in the 13th or in the 643th position in this chapter). So, you partition the text into meaningful segments which you can then label, compare etc.\n",
    "\n",
    "Other preparatory work includes suppressing stopwords (like \"the\", \"is\", \"of\" in english) or making the tools manage different forms of the same word or different historical writings identically. Here is what falls under this category:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  1. [Get fulltext](#GetFulltext)\n",
    "  2. [Segment source text](#SegmentSourceText) \n",
    "  3. [Read segments into Variable/List](#ReadSegmentsIntoVariable)\n",
    "  4. [Tokenising](#Tokenising)\n",
    "  5. [Stemming/Lemmatising](#StemmingLemmatising)\n",
    "  6. [Eliminate stopwords](#EliminateStopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get fulltext <a name=\"GetFulltext\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the examples given on this page, I have loaded a plaintext export of Francisco de Vitoria's \"Relectiones\" from the School of Salamanca's project, available as one single file at this URL: [http://api.salamanca.school/txt/works.W0013.orig]. I have saved this to the file **TextProcessing_2017/W0013.orig.txt**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-28T08:46:34.342432Z",
     "start_time": "2017-06-28T10:46:34.294308+02:00"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['                  REVERENDI  PATRIS F. FRANCISCI DE VIctoria, ordinis Prædicatorũ, sacræ Theologiæ   in Salmanticensi Academia quondam  primarij Professoris, Relectiones  Theologicæ XII. in duos  Tomos diuisæ:  Quarum seriem uersa pagella iudicabit.   SVMMARIIS suis ubique locis adiectis, una cum  INDICE omnium copiosißimo.  TOMVS PRIMVS.           Lugduni, apud Iacobum Boyerium,  M. D. LVII.   Cum priuilegio Regis ad decennium.     \\n',\n",
       " '  \\n',\n",
       " '        PRIMVS TOMVS, \\n',\n",
       " '  \\n',\n",
       " '   De\\n',\n",
       " '  \\n',\n",
       " '  -\\xa0Potestate ecclesiæ, prior & posterior. \\n',\n",
       " '  -\\xa0   Potestate ciuili.\\n',\n",
       " '  -\\xa0   Potestate concilij.\\n',\n",
       " '  -\\xa0   Indis prior.\\n']"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigsourcefile = 'TextProcessing_2017/W0013.orig.txt'         # This is the path to our file\n",
    "input = open(bigsourcefile, encoding='utf-8').readlines()   # We use a variable 'input' for\n",
    "                                                            # keeping its contents.\n",
    "\n",
    "input[:10]                                     # Just for information,\n",
    "                                               # let's see the first 10 lines of the file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment source text<a name=\"SegmentSourceText\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, as mentioned above, we want to associate information with only passages of the text, not the text as a whole. Therefore, the text has to be segmented. The one single file is being split into meaningful smaller chunks. What exactly constitutes a meaningful chunk -- a chapter, an article, a paragraph etc. -- cannot be known independently of the text in question and of the research questions. Therefore, it is suggested that the scholar either splits the text manually or inserts some symbols that otherwise do not appear in the text. Then, processing tools can identify these and split the file accordingly. For keeping things neat and orderly, the resulting files should be saved in a directory of their own..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I am splitting the file arbitrarily every 80 lines... Note though, that this leads to a rather unusual condition: all segments are of (roughly) the same length. When counting words and assessing their relative \"importance\", if a word occurs twice in a very short passage, this is more telling about the passage than if the passage was very, very long. Later, we will see ways to compensate for the normal variance in passage length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-28T08:46:34.944938Z",
     "start_time": "2017-06-28T10:46:34.876641+02:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 files written.\n"
     ]
    }
   ],
   "source": [
    "splitLen = 80                 # 80 lines per file\n",
    "outputBase = 'TextProcessing_2017/segment'  # source/segment.1.txt, source/segment.2.txt, etc.\n",
    "\n",
    "count = 0                     # initialise some variables. \n",
    "at    = 0\n",
    "dest  = None                  # this later takes our destination files\n",
    "\n",
    "for line in input:\n",
    "    if count % splitLen == 0:\n",
    "        if dest: dest.close()\n",
    "        dest = open(outputBase + '.' + str(at) + '.txt', encoding='utf-8', mode='w')   # 'w' is for writing: here we open the file the current segment is being written to\n",
    "        at += 1\n",
    "    dest.write(line.strip())\n",
    "    count += 1\n",
    "\n",
    "print(str(at - 1) + ' files written.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read segments into a variable <a name=\"ReadSegmentsIntoVariable\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the segments, we rebuild our corpus, iterating through them and reading them into another variable (which now stores, technically speaking, a list of strings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-28T08:46:35.294704Z",
     "start_time": "2017-06-28T10:46:35.269419+02:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import glob\n",
    "import errno\n",
    "\n",
    "path     = 'TextProcessing_2017'\n",
    "filename = 'segment.'\n",
    "suffix   = '.txt'\n",
    "corpus   = []\n",
    "\n",
    "for i in range(0, at - 1):\n",
    "    try:\n",
    "        with open(path + '/' + filename + str(i) + suffix, encoding='utf-8') as f:\n",
    "            corpus.append(f.read())\n",
    "            f.close()\n",
    "    except IOError as exc:\n",
    "        if exc.errno != errno.EISDIR: # Do not fail if a directory is found, just ignore it.\n",
    "            raise # Propagate other kinds of IOError."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should have 45 strings in the variable *corpus* to play around with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-28T08:46:35.482574Z",
     "start_time": "2017-06-28T10:46:35.468015+02:00"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a quick impression, let's see the opening 500 characters of an arbitrary one of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-28T08:46:35.750869Z",
     "start_time": "2017-06-28T10:46:35.735945+02:00"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'claues uerò in foro interiori, Iohan. 20.{Iohan. 20.}Primatum autem, & plenitudinem potestatis uidetur Petrus accepisse Iohã.  21.{Iohan. 21.}Pasce oues meas. & quod Armachanus ait,  si non simul acceperunt totam potestatem, fore, ut non esset unum sacramentum ordinis,  non necesse est. Quãquam enim Christus potestatem clauium non simul totam nec uno loco dedit: non ideò tamen consequitur, ut etiã   pontifices possint eam authoritatem diuidere, sed totam simul tribuunt, & uno ordinis sacramento:'"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[5][:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenising <a name=\"Tokenising\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Tokenising\" means splitting the long lines of the input into single words. Since we are dealing with plain latin, we can use the default split method which relies on spaces to identify word boundaries. (In languages like Japanese or scripts like Arabic, this is more difficult.) **Note that we do not compensate for words that are hyphenated/split across lines here!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-28T08:46:36.415958Z",
     "start_time": "2017-06-28T10:46:36.143767+02:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "tokenised = []\n",
    "for segment in corpus:\n",
    "    tokenised.append(list(filter(None, (word.lower() for word in re.split('\\W+', segment)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our examples, let's have a look at (the first 50 words of) an arbitrary one of those segments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-28T08:46:36.447070Z",
     "start_time": "2017-06-28T10:46:36.424868+02:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['claues', 'uerò', 'in', 'foro', 'interiori', 'iohan', '20', 'iohan', '20', 'primatum', 'autem', 'plenitudinem', 'potestatis', 'uidetur', 'petrus', 'accepisse', 'iohã', '21', 'iohan', '21', 'pasce', 'oues', 'meas', 'quod', 'armachanus', 'ait', 'si', 'non', 'simul', 'acceperunt', 'totam', 'potestatem', 'fore', 'ut', 'non', 'esset', 'unum', 'sacramentum', 'ordinis', 'non', 'necesse', 'est', 'quãquam', 'enim', 'christus', 'potestatem', 'clauium', 'non', 'simul', 'totam']\n"
     ]
    }
   ],
   "source": [
    "print(tokenised[5][:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already, we can have a first go at finding the most frequent words for a segment. (For this we use a simple library of functions that we import by the name of 'collections'.):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-28T08:46:36.624307Z",
     "start_time": "2017-06-28T10:46:36.596527+02:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('in', 105), ('non', 103), ('est', 101), ('ad', 77), ('quòd', 59), ('nec', 56), ('sed', 50), ('potestas', 50), ('papa', 47), ('ut', 45)]\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "counter = collections.Counter(tokenised[5])\n",
    "print(counter.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps now is a good opportunity for a small excursus. What we have printed in the last code is a series of pairs: Words and their number of occurrences, sorted by the latter. Yet the display looks a bit ugly. With another library called \"pandas\" (for python data analysis), we can make this more intuitive. (Of course, your system must have this library installed in the first place so that we can import it in our code.):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-28T08:46:36.895971Z",
     "start_time": "2017-06-28T10:46:36.846148+02:00"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>non</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>est</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>ad</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>quòd</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>nec</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>sed</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>potestas</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>papa</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>ut</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        lemma  count\n",
       "2          in    105\n",
       "23        non    103\n",
       "35        est    101\n",
       "191        ad     77\n",
       "137      quòd     59\n",
       "40        nec     56\n",
       "53        sed     50\n",
       "150  potestas     50\n",
       "162      papa     47\n",
       "29         ut     45"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.DataFrame.from_dict(counter, orient='index').reset_index()\n",
    "df2 = df1.rename(columns={'index':'lemma',0:'count'})\n",
    "\n",
    "df2.sort_values('count',0,False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks better now, doesn't it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming / Lemmatising <a name=\"StemmingLemmatising\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, since we prefer to count different word forms as one and the same \"lemma\", we need to do a step called \"lemmatisation\". In languages like English, that are not strongly inflected, one can get away with \"stemming\", i.e. just eliminating the ending of words: \"wish\", \"wished\", \"wishing\", \"wishes\" all can count as instances of \"wish\\*\". With Latin this is not so easy: we want to count occurrences of \"legum\", \"leges\", \"lex\" as one and the same word, but if we truncate after \"le\", we get too many hits that have nothing to do with lex at all. There are a couple of \"lemmatising\" tools available, we do our own with a dictionary approach..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we have to have a dictionary which associates all known word forms to their lemma. This also helps us with historical orthography. Suppose from some other context, we have a file \"wordforms-lat.txt\" at our disposal in the TextProcessing_2017 directory. Its contents looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-28T08:46:37.900688Z",
     "start_time": "2017-06-28T10:46:37.878020+02:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿a > a\n",
      "à > a\n",
      "ab > a\n",
      "abbas > abbas\n",
      "abbate > abbas\n",
      "abbatem > \n"
     ]
    }
   ],
   "source": [
    "wordfile_path = 'TextProcessing_2017/wordforms-lat.txt'\n",
    "wordfile = open(wordfile_path, encoding='utf-8')\n",
    "\n",
    "print (wordfile.read()[:59])\n",
    "wordfile.close;                # (The semicolon suppresses the returned object in cell output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can again build a dictionary of key-value pairs associating all the lemmata (\"values\") with their wordforms (\"keys\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-28T08:46:38.242487Z",
     "start_time": "2017-06-28T10:46:38.214406+02:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1682 wordforms registered.\n"
     ]
    }
   ],
   "source": [
    "lemma    = {}    # we build a so-called dictionary for the lookups\n",
    "tempdict = []\n",
    "\n",
    "wordfile = open(wordfile_path, encoding='utf-8')\n",
    "\n",
    "for line in wordfile.readlines():\n",
    "    tempdict.append(tuple(line.split('>')))\n",
    "\n",
    "lemma = {k.strip(): v.strip() for k, v in tempdict}\n",
    "wordfile.close;\n",
    "print(str(len(lemma)) + ' wordforms registered.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, a quick test: Let's see with which basic word the wordform \"ciuicior\" is associated, or, in other words, what *value* our lemma variable returns when we query for the *key* \"ciuicior\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-28T08:46:38.565243Z",
     "start_time": "2017-06-28T10:46:38.551229+02:00"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'civicus'"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma['ciuicior']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use this dictionary to build a new list of words, where only lemmatised forms occur:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-28T08:46:41.781762Z",
     "start_time": "2017-06-28T10:46:41.688299+02:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['clavis', 'uerò', 'in', 'foro', 'interiori', 'iohan', '20', 'iohan', '20', 'primatum', 'autem', 'plenitudinem', 'potestas', 'uidetur', 'petrus', 'accepisse', 'iohã', '21', 'iohan', '21', 'pasce', 'oues', 'meas', 'qui', 'armachanus', 'aio', 'si', 'nolo', 'simul', 'acceperunt', 'totam', 'potestas', 'fore', 'ut', 'nolo', 'sum', 'unum', 'sacramentum', 'ordo', 'nolo', 'necesse', 'sum', 'quãquam', 'enim', 'christus', 'potestas', 'clavis', 'nolo', 'simul', 'totam']\n"
     ]
    }
   ],
   "source": [
    "lemmatised = [[lemma[word] if word in lemma else word for word in segment] \\\n",
    "              for segment in tokenised]\n",
    "\n",
    "print(lemmatised[5][:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the original text is lost now from the data that we are currently working with (unless we add another dimension to our lemmatised variable which can keep the original word form). But let us see if something in the 10 most frequent words has changed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-28T08:47:51.614805Z",
     "start_time": "2017-06-28T10:47:51.559834+02:00"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>sum</td>\n",
       "      <td>223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>potestas</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>nolo</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>ad</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>quòd</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>nec</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>papa</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>sed</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>habeo</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        lemma  count\n",
       "29        sum    223\n",
       "10   potestas    120\n",
       "2          in    105\n",
       "23       nolo    103\n",
       "178        ad     77\n",
       "129      quòd     59\n",
       "37        nec     56\n",
       "150      papa     52\n",
       "50        sed     50\n",
       "103     habeo     49"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter2 = collections.Counter(lemmatised[5])\n",
    "df1 = pd.DataFrame.from_dict(counter2, orient='index').reset_index()\n",
    "df2 = df1.rename(columns={'index':'lemma',0:'count'})\n",
    "\n",
    "df2.sort_values('count',0,False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, things have changed: \"esse/sum\" has moved to the most frequent place, \"non\" is now counted among the \"nolo\" (I am not sure this makes sense, but such is the dictionary of wordforms we have used) and \"potestas\" has now made it from the eighth to the second place!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminate Stopwords <a name=\"EliminateStopwords\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably \"sum/esse\", \"non/nolo\", \"in\", \"ad\" and the like are not really very informative words. They are what one calls *stopwords*, and we have another list of such words that we would rather want to ignore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-28T08:50:55.291310Z",
     "start_time": "2017-06-28T10:50:55.260085+02:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297 stopwords, e.g.: ['ab', 'ac', 'ad', 'adhic', 'adhuc', 'ae', 'ait', 'ali', 'alii', 'aliis', 'alio', 'aliqua', 'aliqui', 'aliquid', 'aliquis', 'aliquo', 'am', 'an', 'ante', 'apud', 'ar', 'at', 'atque', 'au', 'aut', 'autem', 'bus', 'c', 'ca', 'ceptum']\n"
     ]
    }
   ],
   "source": [
    "stopwords_path = 'TextProcessing_2017/stopwords-lat.txt'\n",
    "stopwords = open(stopwords_path, encoding='utf-8').read().splitlines()\n",
    "\n",
    "print(str(len(stopwords)) + ' stopwords, e.g.: ' + str(stopwords[24:54]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try and suppress the stopwords in the segments..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-28T08:50:59.498382Z",
     "start_time": "2017-06-28T10:50:57.820781+02:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['clavis', 'uerò', 'foro', 'interiori', 'iohan', 'iohan', 'primatum', 'plenitudinem', 'potestas', 'uidetur', 'petrus', 'accepisse', 'iohã', 'iohan', 'pasce', 'oues', 'meas', 'armachanus', 'aio', 'simul']\n"
     ]
    }
   ],
   "source": [
    "stopped = [[item for item in lemmatised_segment if item not in stopwords] \\\n",
    "           for lemmatised_segment in lemmatised]\n",
    "print(stopped[5][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we can already create a first \"profile\" of our first 4 segments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-28T08:51:26.879413Z",
     "start_time": "2017-06-28T10:51:26.750192+02:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " Most frequent lemmata in the first text segment\n",
      "          lemma  count\n",
      "42     ecclesia     46\n",
      "183        queo     14\n",
      "270        homo      8\n",
      "715  haereticus      8\n",
      "823       video      7\n",
      "706    synagoga      7\n",
      "281         vir      7\n",
      "342         res      7\n",
      "3      victoria      6\n",
      "831        dico      6\n",
      " \n",
      " \n",
      " Most frequent lemmata in the second text segment\n",
      "           lemma  count\n",
      "50      potestas     52\n",
      "16      ecclesia     24\n",
      "51       civilis     14\n",
      "66   spiritualis     10\n",
      "248        matth      9\n",
      "107         deus      8\n",
      "57          queo      6\n",
      "81        regnum      6\n",
      "167          rom      6\n",
      "269          luc      6\n",
      " \n",
      " \n",
      " Most frequent lemmata in the third text segment\n",
      "           lemma  count\n",
      "53      potestas     50\n",
      "413     peccatum     50\n",
      "420       clavis     28\n",
      "305  spiritualis     23\n",
      "581     remissio     23\n",
      "419     peccator     22\n",
      "67          deus     22\n",
      "144         dico     21\n",
      "445        dolor     15\n",
      "263         semi     14\n",
      " \n",
      " \n",
      " Most frequent lemmata in the fourth text segment\n",
      "              lemma  count\n",
      "219        potestas     74\n",
      "222     spiritualis     32\n",
      "252             ius     28\n",
      "10             deus     22\n",
      "59             queo     18\n",
      "273            lego     16\n",
      "254  ecclesiasticus     13\n",
      "1              dico     13\n",
      "265            tota     12\n",
      "122           solum     12\n"
     ]
    }
   ],
   "source": [
    "counter3 = collections.Counter(stopped[0])\n",
    "counter4 = collections.Counter(stopped[1])\n",
    "counter5 = collections.Counter(stopped[2])\n",
    "counter6 = collections.Counter(stopped[3])\n",
    "\n",
    "df0_1 = pd.DataFrame.from_dict(counter3, orient='index').reset_index()\n",
    "df0_2 = df0_1.rename(columns={'index':'lemma',0:'count'})\n",
    "\n",
    "df1_1 = pd.DataFrame.from_dict(counter4, orient='index').reset_index()\n",
    "df1_2 = df1_1.rename(columns={'index':'lemma',0:'count'})\n",
    "\n",
    "df2_1 = pd.DataFrame.from_dict(counter5, orient='index').reset_index()\n",
    "df2_2 = df2_1.rename(columns={'index':'lemma',0:'count'})\n",
    "\n",
    "df3_1 = pd.DataFrame.from_dict(counter6, orient='index').reset_index()\n",
    "df3_2 = df3_1.rename(columns={'index':'lemma',0:'count'})\n",
    "\n",
    "print(' ')\n",
    "print(' Most frequent lemmata in the first text segment')\n",
    "print(df0_2.sort_values(by='count',axis=0,ascending=False)[:10])\n",
    "print(' ')\n",
    "print(' ')\n",
    "print(' Most frequent lemmata in the second text segment')\n",
    "print(df1_2.sort_values(by='count',axis=0,ascending=False)[:10])\n",
    "print(' ')\n",
    "print(' ')\n",
    "print(' Most frequent lemmata in the third text segment')\n",
    "print(df2_2.sort_values(by='count',axis=0,ascending=False)[:10])\n",
    "print(' ')\n",
    "print(' ')\n",
    "print(' Most frequent lemmata in the fourth text segment')\n",
    "print(df3_2.sort_values(by='count',axis=0,ascending=False)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far our initial analyses, then. There are several ways in which we can continue now. We could for example use either our lemma list or our stopwords list to filter out certain words, like all non-substantives...\n",
    "\n",
    "However, we can already observe that meaningful words like \"potestas\" are maybe not so helpful in characterising individual passages after all, since they occur all over the place. Also, we would like to give some weight to the fact that a passage may consist of all stopwords and perhaps one or two substantial words, whereas another might be full of substantial words and few stopwords (think e.g. of an abstract or an opening chapter describing the rest of the work). Or, in case we have text segments of varying length (which is probably rather the norm than the exception), we would like our figures to reflect the fact that a tenfold occurrence in a very short passage may be more significant than a tenfold occurrence in a very, very, very long passage.\n",
    "\n",
    "These phenomena are treated with more mathematical tools, so let's say that our preparatory work is done ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Characterise passages: TF/IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As described, we are now going to delve a wee bit deeper into mathematics in order to get more precise characterizations of our text segments. The approach we are going to use is called \"TF/IDF\" and is a simple, yet powerful method that is very popular in data mining and search engine discussions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  1. [Build vocabulary](#BuildVocabulary)\n",
    "  2. [Vector space model of the text](#VectorSpaceModel)\n",
    "  3. [Calculate Terms' Text Frequencies (TF)](#CalculateTF) \n",
    "  4. [Normalise TF](#NormaliseTF)\n",
    "  5. [Calculate Inverse Document Frequencies (IDF)](#CalculateIDF)\n",
    "  6. [Calculate TF/IDF](#CalculateTFIDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since maths works with numbers, let's first of all build a list of all the words (in their basic form) that occur anywhere in the text, and give each one of those words an ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find similar passages: Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - Find good measure (word vectors, authorities cited, style, ...)\n",
    "  - Find starting centroids\n",
    "  - Find good K value\n",
    "  - K-Means clustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cope with different languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This is the [API documentation](https://github.com/commonsense/conceptnet5/wiki/API) of [conceptnet.io](http://conceptnet.io). Which we can use to lookup synonyms, related terms and translations. Like with such a URI:\n",
    "\n",
    "```\n",
    "http://api.conceptnet.io/related/c/la/rex?filter=/c/es\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - http://jonathansoma.com/lede/foundations/classes/text%20processing/tf-idf/\n",
    "  - http://blog.christianperone.com/2011/09/machine-learning-text-feature-extraction-tf-idf-part-i/\n",
    "  - http://blog.christianperone.com/2011/10/machine-learning-text-feature-extraction-tf-idf-part-ii/\n",
    "  - https://stanford.edu/~rjweiss/public_html/IRiSS2013/text2/notebooks/tfidf.html\n",
    "  - http://takwatanabe.me/data_science/pyspark/cs110_lab3b.html\n",
    "  - https://github.com/mccurdyc/tf-idf/blob/master/README.md\n",
    "  - https://people.duke.edu/~ccc14/sta-663/TextProcessingExtras.html\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "245px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": "2",
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
