{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Text-Processing\" data-toc-modified-id=\"Text-Processing-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Text Processing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Introduction</a></span></li></ul></li><li><span><a href=\"#Preparations\" data-toc-modified-id=\"Preparations-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Preparations</a></span><ul class=\"toc-item\"><li><span><a href=\"#TF/IDF-\" data-toc-modified-id=\"TF/IDF--2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>TF/IDF <a name=\"tfidf\"></a></a></span></li></ul></li><li><span><a href=\"#Vector-Space-Model-of-the-text-\" data-toc-modified-id=\"Vector-Space-Model-of-the-text--3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Vector Space Model of the text <a name=\"#VectorSpaceModel\"></a></a></span><ul class=\"toc-item\"><li><span><a href=\"#Another-method-to-generate-the-dimensions:-n-grams-\" data-toc-modified-id=\"Another-method-to-generate-the-dimensions:-n-grams--3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Another method to generate the dimensions: n-grams <a name=\"N-Grams\"></a></a></span></li><li><span><a href=\"#Extending-the-dimensions-\" data-toc-modified-id=\"Extending-the-dimensions--3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Extending the dimensions <a name=\"AddDimensions\"></a></a></span></li><li><span><a href=\"#Word-Clouds-\" data-toc-modified-id=\"Word-Clouds--3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Word Clouds <a name=\"WordClouds\"></a></a></span></li><li><span><a href=\"#Similarity-\" data-toc-modified-id=\"Similarity--3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Similarity <a name=\"DocumentSimilarity\"></a></a></span></li><li><span><a href=\"#Clustering-\" data-toc-modified-id=\"Clustering--3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Clustering <a name=\"DocumentClustering\"></a></a></span></li></ul></li><li><span><a href=\"#Working-with-several-languages\" data-toc-modified-id=\"Working-with-several-languages-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Working with several languages</a></span><ul class=\"toc-item\"><li><span><a href=\"#Translations?\" data-toc-modified-id=\"Translations?-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Translations?</a></span></li></ul></li><li><span><a href=\"#Graph-based-NLP\" data-toc-modified-id=\"Graph-based-NLP-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Graph-based NLP</a></span></li><li><span><a href=\"#Topic-Modelling\" data-toc-modified-id=\"Topic-Modelling-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Topic Modelling</a></span></li><li><span><a href=\"#Manual-Annotation\" data-toc-modified-id=\"Manual-Annotation-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Manual Annotation</a></span></li><li><span><a href=\"#Further-information\" data-toc-modified-id=\"Further-information-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Further information</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file is the continuation of preceding work. Previously, I have worked my way through a couple of text-analysing approaches - such as tf/idf frequencies, n-grams and the like - in the context of a project concerned with Juan de Solórzano Pereira's *Politica Indiana*. This can be seen [here](TextProcessing_Solorzano.ipynb).\n",
    "\n",
    "In the former context, I got somewhat stuck when I was trying to automatically align corresponding passages of two editions of the same work ... where the one edition would be a **translation** of the other and thus we would have two different languages.\n",
    "\n",
    "The present file takes this up, tries to refine an approach taken there and to find alternative ways of analysing a text across several languages. This time, the work concerned is Martín de Azpilcueta's *Manual de confesores*, a work of the 16th century that has seen very many editions and translations, quite a few of them even by the work's original author and it is the subject of the research project [\"Martín de Azpilcueta’s Manual for Confessors and the Phenomenon of Epitomisation\"](http://www.rg.mpg.de/research/martin-de-azpilcuetas-manual-for-confessors) by Manuela Bragagnolo. \n",
    "\n",
    "(There are a few DH-ey things about the project that are not directly of concern here, like a synoptic display of several editions or the presentation of the divergence of many actual translations of a given term. Such aspects are being treated with other software, like [HyperMachiavel](http://hyperprince.ens-lyon.fr/hypermachiavel) or [Lera](http://lera.uzi.uni-halle.de/).)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous case, the programming language used in the following examples is called \"python\" and the tool used to get prose discussion and code samples together is called [\"jupyter\"](http://jupyter.org/). (A common way of installing both the language and the jupyter software, especially in windows, is by installing a python \"distribution\" like [Anaconda](https://www.anaconda.com/what-is-anaconda/).) In jupyter, you have a \"notebook\" that you can populate with text (if you want to use it, jupyter understands [markdown](http://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Working%20With%20Markdown%20Cells.html) code formatting), or code and a program that pipes a nice rendering of the notebook to a web browser as you are reading right now. In many places in such a notebook, the output that the code samples produce is printed right below the code itself. Sometimes this can be quite a lot of output and depending on your viewing environment you might have to scroll quite some way to get to the continuation of the discussion.\n",
    "\n",
    "You can save your notebook online (the current one is [here at github](https://github.com/awagner-mainz/notebooks/blob/master/gallery/TextProcessing_Azpilcueta.ipynb)) and there is an online service, nbviewer, able to render any notebook that it can access online. So chances are you are reading this present notebook at the web address [https://nbviewer.jupyter.org/github/awagner-mainz/notebooks/blob/master/gallery/TextProcessing_Azpilcueta.ipynb](https://nbviewer.jupyter.org/github/awagner-mainz/notebooks/blob/master/gallery/TextProcessing_Azpilcueta.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A final word about the elements of this notebook:\n",
    "\n",
    "<div class=\"alert alertbox alert-success\">At some points I am mentioning things I consider to be important decisions or take-away messages for scholarly readers. E.g. whether or not to insert certain artefacts into the very transcription of your text, what the methodological ramifications of a certain approach or parameter are, what the implications of an example solution are, or what a possible interpretation of a certain result might be. I am highlighting these things in a block like this one here or at least in <font color=\"green\">**green bold font**</font>.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alertbox alert-danger\">**NOTE:** As I am continually improving the notebook on the side of the source text, wordlists and other parameters, it is sometimes hard to keep the prose description in synch. So while the actual descriptions still apply, the numbers that are mentioned in the prose (as where we have e.g. a \"table with 20 rows and 1.672 columns\") might no longer reflect the latest state of the sources, auxiliary files and parameters and you should take these with a grain of salt. Best double check them by reading the actual code ;-)\n",
    "\n",
    "I apologize for the inconsistency.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike in the previous case, where we had word files that we could export as plaintext, in this case Manuela has prepared a sample chapter with four editions transcribed *in parallel* in an office spreadsheet. So we first of all make sure that we have good **UTF-8** comma-separated-value files, e.g. by uploading a **csv** export of our office program of choice to [a CSV Linting service](https://csvlint.io/). (As a side remark, in my case, exporting with LibreOffice provided me with options to select UTF-8 encoding and choose the field delimiter and resulted in a valid csv file. MS Excel did neither of those.) Below, we expect the file below at the following position:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T16:26:53.368723Z",
     "start_time": "2018-02-05T16:26:53.365742Z"
    }
   },
   "outputs": [],
   "source": [
    "sourcePath = 'Azpilcueta/cap6_align_-_2018-01.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can go ahead and open the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T16:39:47.437114Z",
     "start_time": "2018-02-05T16:39:47.434106Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "sourceFile = open(sourcePath, newline='', encoding='utf-8')\n",
    "sourceTable = csv.reader(sourceFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T15:33:14.158534Z",
     "start_time": "2018-02-05T15:33:14.154549Z"
    }
   },
   "source": [
    "And we read each line into new elements of four respective arrays (since we're dealing with one sample chapter, we try to handle it all in memory first and see if we run into problems):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Note here and in the following that in most cases, when the program is counting, it does so beginning with zero. Which means that if we end up with 20 segments, they are going to be called segment 0, segment 1, ..., segment 19. There is not going to be a segment bearing the number twenty, although we do have twenty segments. The first one has the number zero and the twentieth one has the number nineteen. Even for more experienced coders, this sometimes leads to mistakes, called \"off-by-one errors\".)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T16:39:48.942568Z",
     "start_time": "2018-02-05T16:39:48.929534Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 rows read.\n",
      "1552 por\n",
      "¶ Capitolo VI. Das circunstancias.\n",
      "\n",
      "\n",
      "1.     [1] Pera fundamento disto diremos : lho primeiro, que a circumstancia do peccado, segundo a mente de S. Tho P & outros, he hum accidente daquilo, que he peccado. Dissemos (he accidente) porque nenhũa circũstãcia da obra, he a substancia della. Dissemos (da quilo que he peccado) & nã do peccado : porque muytas vezes a obra em si não he peccado, & pola circũ se faz peccado : & como então ella he aquilo, em que consiste ho peccado, não he tãto accidente do peccado, quanto da quilo que he peccado : segundo que ho declaramos em outra parteq (q. in d. c. Consideret n. 3), seguindo a Alex. de Ales r (r in. 4 pt. q. 77 ar.z. co.la.z).\n",
      "2.     ¶ [2] Ho II. Que a circunstancia se parte em sete species, que se conte nem aquelle verso : Quis, quid, ubi, quibus auxiliis, cur, quomodo, quando : Referido por S. Tho.s (in d. q. 7. ar 3) . Quem, Que, Onde, Com que ajudas, Porque, Em que maneira, Quando. O qual verso temos por melhor, que ho de Paudanot (t. in. 4. d. 16. q. 3. art. 1.u), como ho dissemos em outra parteu (u in princi. d. e. Consideret n. 4) : porque nelle se acrecenta Quotiens, quantas vezes : que denota ho numero : o qual não he circunstancia, se não multiplicação de peccado, como aly ho dissemosx (x. in. d. n. 4). [p. 33, 41 pdf] \n"
     ]
    }
   ],
   "source": [
    "    # Initialize a two-dimensional array ...\n",
    "    Editions = [[]]\n",
    "\n",
    "    # ...with four rows\n",
    "    for i in range(0,3):\n",
    "        a = []\n",
    "        Editions.append(a)\n",
    "\n",
    "    # Now populate it from our sourceTable\n",
    "    for row in sourceTable:\n",
    "        for i, field in enumerate(row):\n",
    "            Editions[i].append(field)\n",
    "\n",
    "    print(str(sourceTable.line_num) + \" rows read.\")\n",
    "\n",
    "    # As an example, see the first seven sections of the second edition:\n",
    "    for field in range(0,6):\n",
    "        print(Editions[1][field])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  1. [TF/IDF](#tfidf)\n",
    "  \n",
    "  2. [Segment source text](#SegmentSourceText) \n",
    "  3. [Read segments into Variable/List](#ReadSegmentsIntoVariable)\n",
    "  4. [Tokenising](#Tokenising)\n",
    "  5. [Stemming/Lemmatising](#StemmingLemmatising)\n",
    "  6. [Eliminate stopwords](#EliminateStopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF/IDF <a name=\"tfidf\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous (i.e. Solórzano) analyses, things like tokenization, lemmatization and stop-word lists filtering are explained step by step. Here, we rely on what we have found there and feed it all into functions that are ready-made and available in suitable libraries..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we build our lemmatization resource and \"function\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T16:26:54.088665Z",
     "start_time": "2018-02-05T16:26:54.075604Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1706 wordforms known to the system.\n"
     ]
    }
   ],
   "source": [
    "lemma    = {}    # we build a so-called dictionary for the lookups\n",
    "tempdict = []\n",
    "\n",
    "wordfile_path = 'Azpilcueta/wordforms-lat.txt'\n",
    "wordfile = open(wordfile_path, encoding='utf-8')\n",
    "\n",
    "# open the wordfile (defined above) for reading\n",
    "wordfile = open(wordfile_path, encoding='utf-8')\n",
    "\n",
    "for line in wordfile.readlines():\n",
    "    tempdict.append(tuple(line.split('>'))) # we split each line by \">\" and append a tuple to a\n",
    "                                            # temporary list.\n",
    "\n",
    "lemma = {k.strip(): v.strip() for k, v in tempdict} # for every tuple in the list,\n",
    "                                                    # we strip whitespace and make a key-value\n",
    "                                                    # pair, appending it to our \"lemma\" dictionary\n",
    "wordfile.close\n",
    "print(str(len(lemma)) + ' wordforms known to the system.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, a quick test: Let's see with which \"lemma\"/basic word the particular wordform \"ciuicior\" is associated, or, in other words, what *value* our lemma variable returns when we query for the *key* \"ciuicior\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T16:26:54.200937Z",
     "start_time": "2018-02-05T16:26:54.196957Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fides'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma['fidem']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we are going to need the stopwords list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T16:26:54.304239Z",
     "start_time": "2018-02-05T16:26:54.300249Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "388 stopwords known to the system, e.g.: ['a', 'ab', 'ac', 'ad', 'adhic', 'adhuc', 'ae', 'ait', 'ali', 'alii', 'aliis', 'alio', 'aliqua', 'aliqui', 'aliquid', 'aliquis', 'aliquo', 'am', 'an', 'ante', 'apud', 'ar', 'at', 'atque', 'au', 'aut', 'autem', 'bus', 'c', 'ca', 'cap', 'ceptum', 'co', 'con', 'cons', 'cui', 'cum', 'cur', 'cùm', 'd', 'da', 'de', 'deinde', 'detur', 'di', 'diu', 'do', 'dum', 'e', 'ea', 'eadem', 'ec', 'eccle', 'ego', 'ei', 'eis', 'eius', 'el', 'em', 'en', 'enim', 'eo', 'eos', 'er', 'erat', 'ergo', 'erit', 'es', 'esse', 'essent', 'esset', 'est', 'et', 'etenim', 'eti']\n"
     ]
    }
   ],
   "source": [
    "stopwords_path = 'Azpilcueta/stopwords-lat.txt'\n",
    "stopwords = open(stopwords_path, encoding='utf-8').read().splitlines()\n",
    "\n",
    "print(str(len(stopwords)) + ' stopwords known to the system, e.g.: ' + str(stopwords[95:170]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see how our corpus of four thousand \"tokens\" actually contains only one and a half thousand different words (plus stopwords, but these are at maximum 384). And, in contrast to simpler numbers that have been filtered out by our stopwords filter, I have left years like \"1610\" in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T16:30:00.031252Z",
     "start_time": "2018-02-05T16:30:00.008191Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# So first we build a tokenising and lemmatising function to work as an input filter\n",
    "# to the CountVectorizer function\n",
    "def ourLemmatiser(str_input):\n",
    "    wordforms = re.split('\\W+', str_input)\n",
    "    return [lemma[wordform].lower().strip() if wordform in lemma else wordform.lower().strip() for wordform in wordforms ]\n",
    "\n",
    "# !!!!\n",
    "# TODO: The above pipes all the tokens through the latin lemmatizer. We should lemmatize Spanish and Portuguese differently!\n",
    "# !!!!\n",
    "\n",
    "# Initialize the library's function\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stopwords, use_idf=True, tokenizer=ourLemmatiser, norm='l2')\n",
    "\n",
    "# Finally, we feed our corpus to the function to build a new \"tfidf_matrix\" object\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(Editions[0])\n",
    "\n",
    "# Print some results\n",
    "tfidf_matrix_frame = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names())\n",
    "\n",
    "print(len)(tfidf_matrix_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Clouds <a name=\"WordClouds\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a library that takes word frequencies like above, calculates corresponding relative sizes of words and creates nice wordcloud images for our sections (again, taking the fourth segment as an example) like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T16:41:40.577055Z",
     "start_time": "2018-02-05T16:41:40.545974Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-99-ee98c843801b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# We make tuples of (lemma, tf/idf score) for one of our segments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# But we have to convert our tf/idf weights to pseudo-frequencies (i.e. integer numbers)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# We make tuples of (lemma, tf/idf score) for one of our segments\n",
    "# But we have to convert our tf/idf weights to pseudo-frequencies (i.e. integer numbers)\n",
    "frq = [ int(round(x * 100000, 0)) for x in mx_array[3]]\n",
    "freq = dict(zip(fn, frq))\n",
    "\n",
    "wc = WordCloud(background_color=None, mode=\"RGBA\", max_font_size=40, relative_scaling=1).fit_words(freq)\n",
    "\n",
    "# Now show/plot the wordcloud\n",
    "plt.figure()\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to have a nicer overview over the many segments than is possible in this notebook, let's create a new html file listing some of the characteristics that we have found so far..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T16:41:34.901698Z",
     "start_time": "2018-02-05T16:41:34.866575Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-98-2073f2fd43f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m100000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmx_array\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mdicts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWordCloud\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbackground_color\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"RGBA\"\u001b[0m\u001b[1;33m,\u001b[0m                        \u001b[0mmax_font_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_font_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m                        \u001b[0mmax_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrelative_scaling\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdicts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[1;31m# We write the wordcloud image to a file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputDir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/wc_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.png'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "outputDir = \"Azpilcueta\"\n",
    "htmlfile = open(outputDir + '/Overview.html', encoding='utf-8', mode='w')\n",
    "\n",
    "# Write the html header and the opening of a layout table\n",
    "htmlfile.write(\"\"\"<!DOCTYPE html>\n",
    "<html>\n",
    "    <head>\n",
    "        <title>Section Characteristics</title>\n",
    "        <meta charset=\"utf-8\"/>\n",
    "    </head>\n",
    "    <body>\n",
    "        <table>\n",
    "\"\"\")\n",
    "\n",
    "a = [[]]\n",
    "a.clear()\n",
    "dicts = []\n",
    "w = []\n",
    "\n",
    "# For each segment, create a wordcloud and write it along with label and\n",
    "# other information into a new row of the html table\n",
    "for i in range(0, len(mx_array)):\n",
    "    # this is like above in the single-segment example...\n",
    "    a.append([ int(round(x * 100000, 0)) for x in mx_array[i]])\n",
    "    dicts.append(dict(zip(fn, a[i])))\n",
    "    w.append(WordCloud(background_color=None, mode=\"RGBA\", \\\n",
    "                       max_font_size=40, min_font_size=10, \\\n",
    "                       max_words=60, relative_scaling=0.8).fit_words(dicts[i]))\n",
    "    # We write the wordcloud image to a file\n",
    "    w[i].to_file(outputDir + '/wc_' + str(i) + '.png')\n",
    "    # Finally we write the column row\n",
    "    htmlfile.write(\"\"\"\n",
    "            <tr>\n",
    "                <td>\n",
    "                    <head>Section {a}: <b>{b}</b></head><br/>\n",
    "                    <img src=\"./wc_{a}.png\"/><br/>\n",
    "                    <small><i>length: {c} words</i></small>\n",
    "                </td>\n",
    "            </tr>\n",
    "            <tr><td>&nbsp;</td></tr>\n",
    "\"\"\".format(a = str(i), b = label[i], c = len(tokenised[i])))\n",
    "\n",
    "# And then we write the end of the html file.\n",
    "htmlfile.write(\"\"\"\n",
    "        </table>\n",
    "    </body>\n",
    "</html>\n",
    "\"\"\")\n",
    "htmlfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should have created a nice html file which we can open [here](./Solorzano/Overview.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-11T07:42:37.598721Z",
     "start_time": "2017-07-11T09:42:37.587926+02:00"
    }
   },
   "source": [
    "## Similarity <a name=\"DocumentSimilarity\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, once we have a representation of our text as a vector - which we can imagine as an arrow that goes a certain distance in one direction, another distance in another direction and so on - we can compare the different arrows. Do they go the same distance in a particular direction? And maybe almost the same in another direction? This would mean that one of the terms of our vocabulary has the same weight in both texts. Comparing the weight of our many, many dimensions, we can develop a measure for the similarity of the texts.\n",
    "\n",
    "(Probably, similarity in words that are occurring all over the place in the corpus should not count so much, and in fact it is attenuated by our arrows being made up of tf/idf weights.)\n",
    "\n",
    "Comparing arrows means calculating with angles and technically, what we are computing is the \"cosine similarity\" of texts. Again, there is a library ready for us to use (but you can find some documentation [here](http://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/), [here](http://scikit-learn.org/stable/modules/metrics.html#cosine-similarity) and [here](https://en.wikipedia.org/wiki/Cosine_similarity).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-10T11:14:55.123090Z",
     "start_time": "2017-11-10T11:14:55.101088Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairwise similarities:\n",
      "          0         1         2         3         4         5         6   \\\n",
      "0   0.000000  0.222897  0.120293  0.147745  0.130148  0.081761  0.094125   \n",
      "1   0.222897  0.000000  0.092630  0.072586  0.082198  0.029001  0.030976   \n",
      "2   0.120293  0.092630  0.000000  0.057610  0.120441  0.043699  0.041051   \n",
      "3   0.147745  0.072586  0.057610  0.000000  0.131746  0.097417  0.056908   \n",
      "4   0.130148  0.082198  0.120441  0.131746  0.000000  0.221282  0.181299   \n",
      "5   0.081761  0.029001  0.043699  0.097417  0.221282  0.000000  0.141404   \n",
      "6   0.094125  0.030976  0.041051  0.056908  0.181299  0.141404  0.000000   \n",
      "7   0.035931  0.067855  0.035364  0.082660  0.132764  0.162764  0.081296   \n",
      "8   0.100506  0.166803  0.060454  0.083347  0.078576  0.039125  0.044615   \n",
      "9   0.044722  0.086246  0.019550  0.102677  0.076310  0.039573  0.020726   \n",
      "10  0.025080  0.067164  0.076931  0.052992  0.110786  0.050311  0.022992   \n",
      "11  0.000000  0.005779  0.000000  0.026106  0.041592  0.008422  0.010421   \n",
      "12  0.027620  0.055453  0.014834  0.011949  0.044076  0.027170  0.005829   \n",
      "13  0.011649  0.043319  0.030025  0.020825  0.056447  0.062899  0.000000   \n",
      "14  0.054892  0.066205  0.060191  0.082729  0.118508  0.078591  0.037649   \n",
      "15  0.044378  0.036802  0.039557  0.070395  0.056771  0.038931  0.012981   \n",
      "16  0.021745  0.090299  0.014982  0.040984  0.067496  0.038400  0.019335   \n",
      "17  0.070181  0.080908  0.079733  0.139602  0.119730  0.087217  0.079050   \n",
      "18  0.068122  0.050428  0.062793  0.058399  0.112498  0.039119  0.033904   \n",
      "19  0.046805  0.057342  0.030257  0.068873  0.073610  0.062976  0.054613   \n",
      "\n",
      "          7         8         9         10        11        12        13  \\\n",
      "0   0.035931  0.100506  0.044722  0.025080  0.000000  0.027620  0.011649   \n",
      "1   0.067855  0.166803  0.086246  0.067164  0.005779  0.055453  0.043319   \n",
      "2   0.035364  0.060454  0.019550  0.076931  0.000000  0.014834  0.030025   \n",
      "3   0.082660  0.083347  0.102677  0.052992  0.026106  0.011949  0.020825   \n",
      "4   0.132764  0.078576  0.076310  0.110786  0.041592  0.044076  0.056447   \n",
      "5   0.162764  0.039125  0.039573  0.050311  0.008422  0.027170  0.062899   \n",
      "6   0.081296  0.044615  0.020726  0.022992  0.010421  0.005829  0.000000   \n",
      "7   0.000000  0.057690  0.033266  0.056735  0.004864  0.028876  0.035758   \n",
      "8   0.057690  0.000000  0.107561  0.028066  0.067699  0.048026  0.044510   \n",
      "9   0.033266  0.107561  0.000000  0.062075  0.110202  0.021130  0.061828   \n",
      "10  0.056735  0.028066  0.062075  0.000000  0.091940  0.122667  0.126188   \n",
      "11  0.004864  0.067699  0.110202  0.091940  0.000000  0.045572  0.059874   \n",
      "12  0.028876  0.048026  0.021130  0.122667  0.045572  0.000000  0.058163   \n",
      "13  0.035758  0.044510  0.061828  0.126188  0.059874  0.058163  0.000000   \n",
      "14  0.093891  0.099462  0.085722  0.100223  0.029431  0.057516  0.076954   \n",
      "15  0.035825  0.062046  0.015942  0.065009  0.021502  0.036766  0.016406   \n",
      "16  0.042488  0.056924  0.016298  0.084189  0.031190  0.044171  0.044116   \n",
      "17  0.091554  0.057106  0.049359  0.086471  0.052649  0.042823  0.078300   \n",
      "18  0.059060  0.023174  0.089074  0.078308  0.018603  0.049908  0.060606   \n",
      "19  0.021254  0.054920  0.019087  0.058867  0.023415  0.033189  0.006876   \n",
      "\n",
      "          14        15        16        17        18        19  \n",
      "0   0.054892  0.044378  0.021745  0.070181  0.068122  0.046805  \n",
      "1   0.066205  0.036802  0.090299  0.080908  0.050428  0.057342  \n",
      "2   0.060191  0.039557  0.014982  0.079733  0.062793  0.030257  \n",
      "3   0.082729  0.070395  0.040984  0.139602  0.058399  0.068873  \n",
      "4   0.118508  0.056771  0.067496  0.119730  0.112498  0.073610  \n",
      "5   0.078591  0.038931  0.038400  0.087217  0.039119  0.062976  \n",
      "6   0.037649  0.012981  0.019335  0.079050  0.033904  0.054613  \n",
      "7   0.093891  0.035825  0.042488  0.091554  0.059060  0.021254  \n",
      "8   0.099462  0.062046  0.056924  0.057106  0.023174  0.054920  \n",
      "9   0.085722  0.015942  0.016298  0.049359  0.089074  0.019087  \n",
      "10  0.100223  0.065009  0.084189  0.086471  0.078308  0.058867  \n",
      "11  0.029431  0.021502  0.031190  0.052649  0.018603  0.023415  \n",
      "12  0.057516  0.036766  0.044171  0.042823  0.049908  0.033189  \n",
      "13  0.076954  0.016406  0.044116  0.078300  0.060606  0.006876  \n",
      "14  0.000000  0.080179  0.100321  0.120819  0.112492  0.040502  \n",
      "15  0.080179  0.000000  0.052674  0.039606  0.034220  0.024331  \n",
      "16  0.100321  0.052674  0.000000  0.067907  0.023408  0.058716  \n",
      "17  0.120819  0.039606  0.067907  0.000000  0.071507  0.060739  \n",
      "18  0.112492  0.034220  0.023408  0.071507  0.000000  0.047117  \n",
      "19  0.040502  0.024331  0.058716  0.060739  0.047117  0.000000  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarities = pd.DataFrame(cosine_similarity(tfidf_matrix))\n",
    "similarities[round(similarities, 0) == 1] = 0 # Suppress a document's similarity to itself\n",
    "print(\"Pairwise similarities:\")\n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-10T11:14:55.196098Z",
     "start_time": "2017-11-10T11:14:55.124091Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The two most similar segments in the corpus are\n",
      "segments 0 and 1 .\n",
      "They have a similarity score of\n",
      "0.222896735543\n"
     ]
    }
   ],
   "source": [
    "print(\"The two most similar segments in the corpus are\")\n",
    "print(\"segments\", \\\n",
    "      similarities[similarities == similarities.values.max()].idxmax(axis=0).idxmax(axis=1), \\\n",
    "      \"and\", \\\n",
    "      similarities[similarities == similarities.values.max()].idxmax(axis=0)[ similarities[similarities == similarities.values.max()].idxmax(axis=0).idxmax(axis=1) ].astype(int), \\\n",
    "      \".\")\n",
    "print(\"They have a similarity score of\")\n",
    "print(similarities.values.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alertbox alert-success\">Of course, in every set of documents, we will always find two that are similar in the sense of them being more similar to each other than to the other ones. Whether or not this actually *means* anything in terms of content is still up to scholarly interpretation. But at least it means that a scholar can look at the two documents and when she determines that they are not so similar after all, then perhaps there is something interesting to say about similar vocabulary used for different puproses. Or the other way round: When the scholar knows that two passages are similar, but they have a low \"similarity score\", shouldn't that say something about the texts's rhetorics?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering <a name=\"DocumentClustering\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering is a method to find ways of grouping data into subsets, so that these do have some cohesion. Sentences that are more similar to a particular \"paradigm\" sentence than to another one are grouped with the first one, others are grouped with their respective \"paradigm\" sentence. Of course, one of the challenges is finding sentences that work well as such paradigm sentences. So we have two (or even three) stages: Find paradigms, group data accordingly. (And learn how many groups there are.)<img src=\"http://practicalcryptography.com/media/miscellaneous/files/k_mean_send.gif\"/>\n",
    "\n",
    "I hope to be able to add a discussion of this subject soon. For now, here are nice tutorials for the process:\n",
    "  - [http://brandonrose.org/clustering](http://brandonrose.org/clustering)\n",
    "  - [https://datasciencelab.wordpress.com/2013/12/12/clustering-with-k-means-in-python/](https://datasciencelab.wordpress.com/2013/12/12/clustering-with-k-means-in-python/)\n",
    "  - [https://de.dariah.eu/tatom/working_with_text.html](https://de.dariah.eu/tatom/working_with_text.html)\n",
    "  - [http://jonathansoma.com/lede/foundations/classes/text%20processing/tf-idf/](http://jonathansoma.com/lede/foundations/classes/text%20processing/tf-idf/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - Find good measure (word vectors, authorities cited, style, ...)\n",
    "  - Find starting centroids\n",
    "  - Find good K value\n",
    "  - K-Means clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with several languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us prepare a second text, this time in Spanish, and see how they compare..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-10T11:14:57.262304Z",
     "start_time": "2017-11-10T11:14:56.146193Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 files written.\n",
      "18 files read.\n",
      "18 labels found.\n",
      "614725 spanish wordforms known to the system.\n",
      "743 spanish stopwords known to the system.\n",
      " \n",
      "Significant words in the spanish text:\n",
      " \n",
      " Most significant words in the 1. segment:\n",
      "        lemma  tf/idf value\n",
      "0    capitvlo      0.399528\n",
      "1  totalmente      0.399528\n",
      "2     español      0.349703\n",
      "3        casa      0.286932\n",
      "4    tributar      0.286932\n",
      "5  particular      0.264527\n",
      "6      llamar      0.264527\n",
      "7        cosa      0.245585\n",
      "8    prohibir      0.245585\n",
      "9    personal      0.201756\n",
      " \n",
      " Most significant words in the 2. segment:\n",
      "        lemma  tf/idf value\n",
      "0       indio      0.176840\n",
      "1    servicio      0.158496\n",
      "2  famulicios      0.138930\n",
      "3     público      0.138930\n",
      "4  domesticos      0.138930\n",
      "5       carga      0.138930\n",
      "6    reservar      0.138930\n",
      "7       color      0.138930\n",
      "8    reperida      0.138930\n",
      "9      cobrar      0.138930\n",
      " \n",
      " Most significant words in the 3. segment:\n",
      "          lemma  tf/idf value\n",
      "0        hombre      0.382174\n",
      "1        forzar      0.334514\n",
      "2    conpadecen      0.191087\n",
      "3  emperadores3      0.191087\n",
      "4   contradecir      0.191087\n",
      "5   aristoteles      0.191087\n",
      "6      facultad      0.191087\n",
      "7   impedimetos      0.191087\n",
      "8      ocuparse      0.191087\n",
      "9        servil      0.191087\n",
      " \n",
      " Most significant words in the 4. segment:\n",
      "          lemma  tf/idf value\n",
      "0         veder      0.272392\n",
      "1      conducir      0.272392\n",
      "2      alquilar      0.272392\n",
      "3    extimables      0.272392\n",
      "4       prohibe      0.272392\n",
      "5         llano      0.272392\n",
      "6      precioso      0.272392\n",
      "7  regularmente      0.272392\n",
      "8        dignar      0.238423\n",
      "9        forzar      0.238423\n",
      " \n",
      " Most significant words in the 5. segment:\n",
      "         lemma  tf/idf value\n",
      "0       tratar      0.230925\n",
      "1       grande      0.230925\n",
      "2        razón      0.230925\n",
      "3    provincia      0.174680\n",
      "4        pagar      0.162171\n",
      "5     servicio      0.150490\n",
      "6    debiessen      0.131913\n",
      "7     discurso      0.131913\n",
      "8   permitirse      0.131913\n",
      "9  franciscano      0.131913\n",
      " \n",
      " Most significant words in the 6. segment:\n",
      "         lemma  tf/idf value\n",
      "0        indio      0.200085\n",
      "1    provisión      0.188630\n",
      "2         1549      0.188630\n",
      "3  encomendado      0.188630\n",
      "4        tasar      0.187338\n",
      "5        pagar      0.173923\n",
      "6         real      0.165106\n",
      "7    audiencia      0.165106\n",
      "8          año      0.162302\n",
      "9     voluntad      0.148416\n",
      " \n",
      " Most significant words in the 7. segment:\n",
      "        lemma  tf/idf value\n",
      "0     proveer      0.282385\n",
      "1      entrar      0.231697\n",
      "2    servicio      0.184026\n",
      "3    personal      0.162917\n",
      "4    convenir      0.161309\n",
      "5  hizieredes      0.161309\n",
      "6        vaco      0.161309\n",
      "7      holgar      0.161309\n",
      "8      juzgar      0.161309\n",
      "9       vacar      0.161309\n",
      " \n",
      " Most significant words in the 8. segment:\n",
      "       lemma  tf/idf value\n",
      "0       a el      0.235682\n",
      "1  envejecer      0.218778\n",
      "2     quitar      0.191495\n",
      "3    proveer      0.191495\n",
      "4  costumbre      0.191495\n",
      "5  audiencia      0.191495\n",
      "6     reinar      0.172137\n",
      "7     virrey      0.157121\n",
      "8      de+el      0.156100\n",
      "9    referir      0.144853\n",
      " \n",
      " Most significant words in the 9. segment:\n",
      "      lemma  tf/idf value\n",
      "0     mesmo      0.248627\n",
      "1  personal      0.212763\n",
      "2    perder      0.210663\n",
      "3   ordenar      0.210663\n",
      "4     casar      0.184391\n",
      "5     indio      0.178764\n",
      "6  voluntad      0.165751\n",
      "7  servicio      0.160220\n",
      "8    querer      0.151293\n",
      "9   tributo      0.151293\n",
      " \n",
      " Most significant words in the 10. segment:\n",
      "           lemma  tf/idf value\n",
      "0         cedula      0.281725\n",
      "1           a el      0.203724\n",
      "2     particular      0.187817\n",
      "3     presidente      0.141834\n",
      "4   encargandome      0.141834\n",
      "5        omisión      0.141834\n",
      "6          oidor      0.141834\n",
      "7       aranjuez      0.141834\n",
      "8  expressamente      0.141834\n",
      "9           limo      0.141834\n",
      " \n",
      " Most significant words in the 11. segment:\n",
      "        lemma  tf/idf value\n",
      "0      gravar      0.296242\n",
      "1       docto      0.169225\n",
      "2     materia      0.169225\n",
      "3   ajustarse      0.169225\n",
      "4      formar      0.169225\n",
      "5   diciembre      0.169225\n",
      "6        1610      0.169225\n",
      "7  reformasse      0.169225\n",
      "8       junta      0.169225\n",
      "9     haberse      0.169225\n",
      " \n",
      " Most significant words in the 12. segment:\n",
      "             lemma  tf/idf value\n",
      "0            señor      0.283368\n",
      "1          término      0.283368\n",
      "2         disponer      0.248030\n",
      "3        silvestro      0.141684\n",
      "4          ultimar      0.141684\n",
      "5          vasallo      0.141684\n",
      "6          abrazar      0.141684\n",
      "7          navarro      0.141684\n",
      "8         exacción      0.141684\n",
      "9  quebrantamiento      0.141684\n",
      " \n",
      " Most significant words in the 13. segment:\n",
      "           lemma  tf/idf value\n",
      "0         colono      0.236015\n",
      "1       celebrar      0.236015\n",
      "2         hablar      0.236015\n",
      "3      condición      0.236015\n",
      "4  adscripticios      0.236015\n",
      "5      propósito      0.236015\n",
      "6  violentamente      0.236015\n",
      "7         enseña      0.236015\n",
      "8       antiguar      0.236015\n",
      "9        volumen      0.236015\n",
      " \n",
      " Most significant words in the 14. segment:\n",
      "       lemma  tf/idf value\n",
      "0      manar      0.250435\n",
      "1   defender      0.250435\n",
      "2  excluirse      0.250435\n",
      "3     efecto      0.250435\n",
      "4    recibir      0.250435\n",
      "5  continuar      0.250435\n",
      "6   posesión      0.250435\n",
      "7   justicia      0.250435\n",
      "8    ciencia      0.250435\n",
      "9     fraude      0.250435\n",
      " \n",
      " Most significant words in the 15. segment:\n",
      "          lemma  tf/idf value\n",
      "0  prescripción      0.428202\n",
      "1        seguir      0.214101\n",
      "2         citar      0.214101\n",
      "3         lucas      0.214101\n",
      "4            fe      0.214101\n",
      "5      alegarse      0.214101\n",
      "6         valer      0.214101\n",
      "7         anuas      0.214101\n",
      "8       constar      0.214101\n",
      "9       poderse      0.214101\n",
      " \n",
      " Most significant words in the 16. segment:\n",
      "        lemma  tf/idf value\n",
      "0      gravar      0.318044\n",
      "1   inocencio      0.181679\n",
      "2    glorioso      0.181679\n",
      "3  frecuentar      0.181679\n",
      "4      africa      0.181679\n",
      "5    labrador      0.181679\n",
      "6  excessivos      0.181679\n",
      "7       mirar      0.181679\n",
      "8   estrechar      0.181679\n",
      "9    prefecto      0.181679\n",
      " \n",
      " Most significant words in the 17. segment:\n",
      "       lemma  tf/idf value\n",
      "0    señalar      0.431038\n",
      "1   tributar      0.309562\n",
      "2       cosa      0.176636\n",
      "3    contado      0.143679\n",
      "4    domingo      0.143679\n",
      "5  comodidad      0.143679\n",
      "6     demora      0.143679\n",
      "7     alegar      0.143679\n",
      "8  convencer      0.143679\n",
      "9    titular      0.143679\n",
      " \n",
      " Most significant words in the 18. segment:\n",
      "      lemma  tf/idf value\n",
      "0      apud      0.406932\n",
      "1     pagin      0.226074\n",
      "2     latir      0.180859\n",
      "3    acosta      0.177877\n",
      "4      agia      0.142301\n",
      "5     tomar      0.135644\n",
      "6       ego      0.135644\n",
      "7      dict      0.135644\n",
      "8    librar      0.097416\n",
      "9  capítulo      0.097416\n"
     ]
    }
   ],
   "source": [
    "bigspanishfile = 'Solorzano/Sections_II.2_PI.txt'\n",
    "spInput = open(bigspanishfile, encoding='utf-8').readlines()\n",
    "\n",
    "spAt    = -1\n",
    "spDest  = None\n",
    "\n",
    "for line in spInput:\n",
    "    if line[0:3] == '€€€':\n",
    "        if spDest:\n",
    "            spDest.close()\n",
    "        spAt += 1\n",
    "        spDest = open(outputBase + '.' + str(spAt) +\n",
    "                    '.spanish.txt', encoding='utf-8', mode='w')\n",
    "    else:\n",
    "        spDest.write(line.strip())\n",
    "\n",
    "spAt += 1\n",
    "spDest.close()\n",
    "print(str(spAt) + ' files written.')\n",
    "\n",
    "spSuffix = '.spanish.txt'\n",
    "spCorpus = []\n",
    "for i in range(0, spAt):\n",
    "    try:\n",
    "        with open(path + '/' + filename + str(i) + spSuffix, encoding='utf-8') as f:\n",
    "            spCorpus.append(f.read())\n",
    "            f.close()\n",
    "    except IOError as exc:\n",
    "        if exc.errno != errno.EISDIR:  # Do not fail if a directory is found, just ignore it.\n",
    "            raise                      # Propagate other kinds of IOError.\n",
    "\n",
    "print(str(len(spCorpus)) + ' files read.')\n",
    "\n",
    "# Labels\n",
    "spLabel = []\n",
    "i = 0\n",
    "for spLine in spInput:\n",
    "    if spLine[0:3] == '€€€':\n",
    "        spLabel.append(spLine[6:].strip())\n",
    "        i =+ 1\n",
    "print(str(len(spLabel)) + ' labels found.')\n",
    "\n",
    "# Tokens\n",
    "spTokenised = []\n",
    "for spSegment in spCorpus:\n",
    "    spTokenised.append(list(filter(None, (spWord.lower()\n",
    "                                        for spWord in re.split('\\W+', spSegment)))))\n",
    "\n",
    "# Lemmata\n",
    "spLemma    = {}\n",
    "spTempdict = []\n",
    "spWordfile_path = 'Solorzano/wordforms-es.txt'\n",
    "spWordfile = open(spWordfile_path, encoding='utf-8')\n",
    "\n",
    "for spLine in spWordfile.readlines():\n",
    "    spTempdict.append(tuple(spLine.split('>')))\n",
    "\n",
    "spLemma = {k.strip(): v.strip() for k, v in spTempdict}\n",
    "spWordfile.close\n",
    "print(str(len(spLemma)) + ' spanish wordforms known to the system.')\n",
    "\n",
    "# Stopwords\n",
    "spStopwords_path = 'Solorzano/stopwords-es.txt'\n",
    "spStopwords = open(spStopwords_path, encoding='utf-8').read().splitlines()\n",
    "print(str(len(spStopwords)) + ' spanish stopwords known to the system.')\n",
    "\n",
    "print(' ')\n",
    "print('Significant words in the spanish text:')\n",
    "\n",
    "# tokenising and lemmatising function\n",
    "def spOurLemmatiser(str_input):\n",
    "    spWordforms = re.split('\\W+', str_input)\n",
    "    return [spLemma[spWordform].lower() if spWordform in spLemma else spWordform.lower() for spWordform in spWordforms ]\n",
    "\n",
    "spTfidf_vectorizer = TfidfVectorizer(stop_words=spStopwords, use_idf=True, tokenizer=spOurLemmatiser, norm='l2')\n",
    "spTfidf_matrix = spTfidf_vectorizer.fit_transform(spCorpus)\n",
    "\n",
    "spMx_array = spTfidf_matrix.toarray()\n",
    "spFn = spTfidf_vectorizer.get_feature_names()\n",
    "\n",
    "pos = 1\n",
    "for l in spMx_array:\n",
    "    print(' ')\n",
    "    print(' Most significant words in the ' + str(pos) + '. segment:')\n",
    "    print(pd.DataFrame.rename(pd.DataFrame.from_dict([(spFn[x], l[x]) for x in (l*-1).argsort()][:10]), columns={0:'lemma',1:'tf/idf value'}))\n",
    "    pos += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alertbox alert-success\">Our spanish wordfiles ([lemmata list](Solorzano/wordforms-es.txt) and [stopwords list](Solorzano/stopwords-es.txt)) are quite large and generous - they spare us some work of resolving quite a lot of abbreviations. However, since they are actually originating from a completely different project, it is very unlikely, that this goes without mistakes. Also some lemmata (like \"de+el\" in the eighth segment) are not really such. So we need to clean our wordlist and adapt it to the current text material urgently!</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now imagine how we would bring the two documents together in a vector space. We would generate dimensions for all the words of our spanish vocabulary and would end up with a common space of roughly twice as many dimensions as before - and the latin work would be only in the first half of the dimensions and the spanish work only in the second half. The respective other half would be populated with only zeroes. So in effect, we would not really have a *common* space or something on the basis of which we could compare the two works. :-("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What might be an interesting perspective, however - since in this case, the second text is a translation of the first one - is a parallel, synoptic overview of both texts. So, let's at least add the second text to our html overview with the wordclouds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-10T11:14:59.064485Z",
     "start_time": "2017-11-10T11:14:57.457324Z"
    }
   },
   "outputs": [],
   "source": [
    "htmlfile2 = open(outputDir + '/Synopsis.html', encoding='utf-8', mode='w')\n",
    "\n",
    "htmlfile2.write(\"\"\"<!DOCTYPE html>\n",
    "<html>\n",
    "    <head>\n",
    "        <title>Section Characteristics, parallel view</title>\n",
    "        <meta charset=\"utf-8\"/>\n",
    "    </head>\n",
    "    <body>\n",
    "        <table>\n",
    "\"\"\")\n",
    "spA = [[]]\n",
    "spA.clear()\n",
    "spDicts = []\n",
    "spW = []\n",
    "for i in range(0, max(len(mx_array), len(spMx_array))):\n",
    "    if (i > len(mx_array) - 1):\n",
    "        htmlfile2.write(\"\"\"\n",
    "            <tr>\n",
    "                <td>\n",
    "                    <head>Section {a}: n/a</head>\n",
    "                </td>\"\"\".format(a = str(i)))\n",
    "    else:\n",
    "        htmlfile2.write(\"\"\"\n",
    "            <tr>\n",
    "                <td>\n",
    "                    <head>Section {a}: <b>{b}</b></head><br/>\n",
    "                    <img src=\"./wc_{a}.png\"/><br/>\n",
    "                    <small><i>length: {c} words</i></small>\n",
    "                </td>\"\"\".format(a = str(i), b = label[i], c = len(tokenised[i])))\n",
    "    if (i > len(spMx_array) - 1):\n",
    "        htmlfile2.write(\"\"\"\n",
    "                <td>\n",
    "                    <head>Section {a}: n/a</head>\n",
    "                </td>\n",
    "            </tr><tr><td>&nbsp;</td></tr>\"\"\".format(a = str(i)))\n",
    "    else:\n",
    "        spA.append([ int(round(x * 100000, 0)) for x in spMx_array[i]])\n",
    "        spDicts.append(dict(zip(spFn, spA[i])))\n",
    "        spW.append(WordCloud(background_color=None, mode=\"RGBA\", \\\n",
    "                           max_font_size=40, min_font_size=10, \\\n",
    "                           max_words=60, relative_scaling=0.8).fit_words(spDicts[i]))\n",
    "        spW[i].to_file(outputDir + '/wc_' + str(i) + '_sp.png')\n",
    "        htmlfile2.write(\"\"\"\n",
    "                <td>\n",
    "                    <head>Section {d}: <b>{e}</b></head><br/>\n",
    "                    <img src=\"./wc_{d}_sp.png\"/><br/>\n",
    "                    <small><i>length: {f} words</i></small>\n",
    "                </td>\n",
    "            </tr>\n",
    "            <tr><td>&nbsp;</td></tr>\"\"\".format(d = str(i), e = spLabel[i], f = len(spTokenised[i])))\n",
    "    \n",
    "htmlfile2.write(\"\"\"\n",
    "        </table>\n",
    "    </body>\n",
    "</html>\n",
    "\"\"\")\n",
    "htmlfile2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the resulting file can be opened [here](Solorzano/Synopsis.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-30T12:30:15.482766Z",
     "start_time": "2017-08-30T14:30:15.474338+02:00"
    }
   },
   "source": [
    "## Translations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe there is an approach to inter-lingual comparison after all. Here is the [API documentation](https://github.com/commonsense/conceptnet5/wiki/API) of [conceptnet.io](http://conceptnet.io), which we can use to lookup synonyms, related terms and translations. Like with such a URI:\n",
    "\n",
    "[http://api.conceptnet.io/related/c/la/rex?filter=/c/es](http://api.conceptnet.io/related/c/la/rex?filter=/c/es)\n",
    "\n",
    "We can get an identifier for a word and many possible translations for this word. So, we could - this remains to be tested in practice - look up our ten (or so) most frequent words in one language and collect all possible translations in the second language. Then we could compare these with what we actually find in the second work. How much overlap there is going to be and how univocal it is going to be remains to be seen, however..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, with a single segment, we could do something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-10T11:33:14.195313Z",
     "start_time": "2017-11-10T11:33:11.728113Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing words from segments 6 (latin) and 8 (spanish)...\n",
      " \n",
      "Most significant words in the latin text:\n",
      "['semi', 'haya', 'por', 'casso', 'pario', 'volo', 'paro', 'servicios', 'servicio', 'personal', 'indios', 'tribuo']\n",
      " \n",
      " \n",
      "For each of the latin words, here are possible translations:\n",
      "semi:\n",
      "mitad, semi, medio, parcialmente, media, parte, mediano, cora, intermedio, parcial, tercio, semifinal, mediana, cuasi, cuarta\n",
      "haya:\n",
      "haya, hamás, ele, jeque, alteza, cordobés, mahoma, córdoba, tanzania, princesa, árabe, israel, tablón, malentendido, palestina\n",
      "por:\n",
      "veces, ésos, ele, aquéllos, aquéllas, ésas, éste, doña, aquél, por, hai, éstas, ia, ése, favor\n",
      "casso:\n",
      "caer, caída, recaer, caerse, caído, comenzar, empezar, empiece, empiezo, comienzo, empezado, inicio, iniciar, iniciarse, vacilar\n",
      "volo:\n",
      "vuelo, volando, volar, avión, copiloto, chicago, paloma, palomar, milán, volador, mosca, pájaro, piloto, aves, aviación\n",
      "paro:\n",
      "huelga, paro, nepal, desempleo, perú, pelotudo, desempleado, paraguay, laburo, desocupación, delhi, lama, boludo, uruguay, bolivia\n",
      "personal:\n",
      "individual, personalmente, particular, personal, personales, individuo, privado, individualidad, personalidad, propio, espiritual, su, personalizado, subjetivo, suyo\n",
      " \n",
      " \n",
      "Most significant words in the spanish text:\n",
      "['mesmo', 'personal', 'perder', 'ordenar', 'casar', 'indio', 'voluntad', 'servicio', 'querer', 'tributo', 'encomendar', 'cosa']\n",
      " \n",
      " \n",
      "Overlaps:\n",
      "semi\n",
      "haya\n",
      "por\n",
      "\n",
      "\n",
      "paro\n",
      "personal\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "segment_no = 6\n",
    "spSegment_no = 8\n",
    "\n",
    "print(\"Comparing words from segments \" + str(segment_no) + \" (latin) and \" + str(spSegment_no) + \" (spanish)...\")\n",
    "\n",
    "print(\" \")\n",
    "# Build List of most significant words for a segment\n",
    "top10a = []\n",
    "top10a = ([fn[x] for x in (mx_array[segment_no]*-1).argsort()][:12])\n",
    "print(\"Most significant words in the latin text:\")\n",
    "print(top10a)\n",
    "\n",
    "print(\" \")\n",
    "# Build lists of possible translations (the 15 most closely related ones)\n",
    "top10a_possible_translations = defaultdict(list)\n",
    "for word in top10a:\n",
    "    concepts_uri = \"http://api.conceptnet.io/related/c/la/\" + word + \"?filter=/c/es\"\n",
    "    response = urllib.request.urlopen(concepts_uri)\n",
    "    concepts = json.loads(response.read().decode(response.info().get_param('charset') or 'utf-8'))\n",
    "    for rel in concepts[\"related\"][0:15]:\n",
    "        top10a_possible_translations[word].append(rel.get(\"@id\").split('/')[-1])\n",
    "\n",
    "print(\" \")\n",
    "print(\"For each of the latin words, here are possible translations:\")\n",
    "for word in top10a_possible_translations:\n",
    "    print(word + \":\")\n",
    "    print(', '.join(trans for trans in top10a_possible_translations[word]))\n",
    "\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "# Build list of 10 most significant words in the second language\n",
    "top10b = []\n",
    "top10b = ([spFn[x] for x in (spMx_array[spSegment_no]*-1).argsort()][:12])\n",
    "print(\"Most significant words in the spanish text:\")\n",
    "print(top10b)\n",
    "\n",
    "# calculate number of overlapping terms\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\"Overlaps:\")\n",
    "for word in top10a_possible_translations:\n",
    "    print(', '.join(trans for trans in top10a_possible_translations[word] if (trans in top10b or trans == word)))\n",
    "\n",
    "# do a nifty ranking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph-based NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - [Unsupervised keywords extraction using graphs](https://graphaware.com/neo4j/2017/10/03/efficient-unsupervised-topic-extraction-nlp-neo4j.html)\n",
    "  - [Reverse Engineering Book Stories with Neo4j and GraphAware NLP](https://graphaware.com/neo4j/2017/07/24/reverse-engineering-book-stories-nlp.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - http://jonathansoma.com/lede/foundations/classes/text%20processing/tf-idf/\n",
    "  - http://blog.christianperone.com/2011/09/machine-learning-text-feature-extraction-tf-idf-part-i/\n",
    "  - http://blog.christianperone.com/2011/10/machine-learning-text-feature-extraction-tf-idf-part-ii/\n",
    "  - https://de.dariah.eu/tatom/index.html\n",
    "  - https://stanford.edu/~rjweiss/public_html/IRiSS2013/text2/notebooks/tfidf.html\n",
    "  - http://takwatanabe.me/data_science/pyspark/cs110_lab3b.html\n",
    "  - https://github.com/mccurdyc/tf-idf/blob/master/README.md\n",
    "  - https://people.duke.edu/~ccc14/sta-663/TextProcessingExtras.html\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "nav_menu": {
    "height": "245px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
