{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Text-Processing\" data-toc-modified-id=\"Text-Processing-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Text Processing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Introduction</a></span></li></ul></li><li><span><a href=\"#Preparations\" data-toc-modified-id=\"Preparations-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Preparations</a></span></li><li><span><a href=\"#TF/IDF-\" data-toc-modified-id=\"TF/IDF--3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>TF/IDF <a name=\"tfidf\"></a></a></span></li><li><span><a href=\"#Translations?\" data-toc-modified-id=\"Translations?-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Translations?</a></span></li><li><span><a href=\"#Similarity-\" data-toc-modified-id=\"Similarity--5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Similarity <a name=\"DocumentSimilarity\"></a></a></span></li><li><span><a href=\"#Word-Clouds-\" data-toc-modified-id=\"Word-Clouds--6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Word Clouds <a name=\"WordClouds\"></a></a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file is the continuation of preceding work. Previously, I have worked my way through a couple of text-analysing approaches - such as tf/idf frequencies, n-grams and the like - in the context of a project concerned with Juan de Solórzano Pereira's *Politica Indiana*. This can be seen [here](TextProcessing_Solorzano.ipynb).\n",
    "\n",
    "In the former context, I got somewhat stuck when I was trying to automatically align corresponding passages of two editions of the same work ... where the one edition would be a **translation** of the other and thus we would have two different languages.\n",
    "\n",
    "The present file takes this up, tries to refine an approach taken there and to find alternative ways of analysing a text across several languages. This time, the work concerned is Martín de Azpilcueta's *Manual de confesores*, a work of the 16th century that has seen very many editions and translations, quite a few of them even by the work's original author and it is the subject of the research project [\"Martín de Azpilcueta’s Manual for Confessors and the Phenomenon of Epitomisation\"](http://www.rg.mpg.de/research/martin-de-azpilcuetas-manual-for-confessors) by Manuela Bragagnolo. \n",
    "\n",
    "(There are a few DH-ey things about the project that are not directly of concern here, like a synoptic display of several editions or the presentation of the divergence of many actual translations of a given term. Such aspects are being treated with other software, like [HyperMachiavel](http://hyperprince.ens-lyon.fr/hypermachiavel) or [Lera](http://lera.uzi.uni-halle.de/).)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous case, the programming language used in the following examples is called \"python\" and the tool used to get prose discussion and code samples together is called [\"jupyter\"](http://jupyter.org/). (A common way of installing both the language and the jupyter software, especially in windows, is by installing a python \"distribution\" like [Anaconda](https://www.anaconda.com/what-is-anaconda/).) In jupyter, you have a \"notebook\" that you can populate with text (if you want to use it, jupyter understands [markdown](http://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Working%20With%20Markdown%20Cells.html) code formatting), or code and a program that pipes a nice rendering of the notebook to a web browser as you are reading right now. In many places in such a notebook, the output that the code samples produce is printed right below the code itself. Sometimes this can be quite a lot of output and depending on your viewing environment you might have to scroll quite some way to get to the continuation of the discussion.\n",
    "\n",
    "You can save your notebook online (the current one is [here at github](https://github.com/awagner-mainz/notebooks/blob/master/gallery/TextProcessing_Azpilcueta.ipynb)) and there is an online service, nbviewer, able to render any notebook that it can access online. So chances are you are reading this present notebook at the web address [https://nbviewer.jupyter.org/github/awagner-mainz/notebooks/blob/master/gallery/TextProcessing_Azpilcueta.ipynb](https://nbviewer.jupyter.org/github/awagner-mainz/notebooks/blob/master/gallery/TextProcessing_Azpilcueta.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A final word about the elements of this notebook:\n",
    "\n",
    "<div class=\"alert alertbox alert-success\">At some points I am mentioning things I consider to be important decisions or take-away messages for scholarly readers. E.g. whether or not to insert certain artefacts into the very transcription of your text, what the methodological ramifications of a certain approach or parameter are, what the implications of an example solution are, or what a possible interpretation of a certain result might be. I am highlighting these things in a block like this one here or at least in <font color=\"green\">**green bold font**</font>.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alertbox alert-danger\">**NOTE:** As I am continually improving the notebook on the side of the source text, wordlists and other parameters, it is sometimes hard to keep the prose description in synch. So while the actual descriptions still apply, the numbers that are mentioned in the prose (as where we have e.g. a \"table with 20 rows and 1.672 columns\") might no longer reflect the latest state of the sources, auxiliary files and parameters and you should take these with a grain of salt. Best double check them by reading the actual code ;-)\n",
    "\n",
    "I apologize for the inconsistency.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike in the previous case, where we had word files that we could export as plaintext, in this case Manuela has prepared a sample chapter with four editions transcribed *in parallel* in an office spreadsheet. So we first of all make sure that we have good **UTF-8** comma-separated-value files, e.g. by uploading a **csv** export of our office program of choice to [a CSV Linting service](https://csvlint.io/). (As a side remark, in my case, exporting with LibreOffice provided me with options to select UTF-8 encoding and choose the field delimiter and resulted in a valid csv file. MS Excel did neither of those.) Below, we expect the file below at the following position:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-06T14:18:31.741215Z",
     "start_time": "2018-02-06T14:18:31.739210Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sourcePath = 'Azpilcueta/cap6_align_-_2018-01.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can go ahead and open the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-06T14:18:31.867550Z",
     "start_time": "2018-02-06T14:18:31.743220Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "sourceFile = open(sourcePath, newline='', encoding='utf-8')\n",
    "sourceTable = csv.reader(sourceFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T15:33:14.158534Z",
     "start_time": "2018-02-05T15:33:14.154549Z"
    }
   },
   "source": [
    "And we read each line into new elements of four respective arrays (since we're dealing with one sample chapter, we try to handle it all in memory first and see if we run into problems):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Note here and in the following that in most cases, when the program is counting, it does so beginning with zero. Which means that if we end up with 20 segments, they are going to be called segment 0, segment 1, ..., segment 19. There is not going to be a segment bearing the number twenty, although we do have twenty segments. The first one has the number zero and the twentieth one has the number nineteen. Even for more experienced coders, this sometimes leads to mistakes, called \"off-by-one errors\".)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-06T14:18:32.021961Z",
     "start_time": "2018-02-06T14:18:31.868555Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 rows read.\n",
      "1552 por\n",
      "¶ Capitolo VI. Das circunstancias.\n",
      "\n",
      "\n",
      "1.     [1] Pera fundamento disto diremos : lho primeiro, que a circumstancia do peccado, segundo a mente de S. Tho P & outros, he hum accidente daquilo, que he peccado. Dissemos (he accidente) porque nenhũa circũstãcia da obra, he a substancia della. Dissemos (da quilo que he peccado) & nã do peccado : porque muytas vezes a obra em si não he peccado, & pola circũ se faz peccado : & como então ella he aquilo, em que consiste ho peccado, não he tãto accidente do peccado, quanto da quilo que he peccado : segundo que ho declaramos em outra parteq (q. in d. c. Consideret n. 3), seguindo a Alex. de Ales r (r in. 4 pt. q. 77 ar.z. co.la.z).\n",
      "2.     ¶ [2] Ho II. Que a circunstancia se parte em sete species, que se conte nem aquelle verso : Quis, quid, ubi, quibus auxiliis, cur, quomodo, quando : Referido por S. Tho.s (in d. q. 7. ar 3) . Quem, Que, Onde, Com que ajudas, Porque, Em que maneira, Quando. O qual verso temos por melhor, que ho de Paudanot (t. in. 4. d. 16. q. 3. art. 1.u), como ho dissemos em outra parteu (u in princi. d. e. Consideret n. 4) : porque nelle se acrecenta Quotiens, quantas vezes : que denota ho numero : o qual não he circunstancia, se não multiplicação de peccado, como aly ho dissemosx (x. in. d. n. 4). [p. 33, 41 pdf] \n",
      "3.     ¶ [3] Ho III que destas circũstancias : todas, & soos aquellas, se hão de confessar de necessidade, que fazen que as obras cujas são, sejão peccados mortae : ou as que sam mortaes de hũa especie, que ho sejão da outra : ou o que he mortal mor hum respeito, ho seja tãben por outro : ora muden as obras de hũa especie em outra, ora não, segundo a comũ opiniam, que copiosamente em outra parte x (x. f. in d. Consideret a n. 5) tractamos : & soos e todas aquellas circunstancias são desta qualidade (segũdo S. Thoy (y. in d. d. 10 q. 6 ar. z. q. 3)) que alen da malicia da mesma obra, repugnã especialmente aa rezão segundo Scoto : as quaes se defendem por diversos, & especiaes mandamentos. Dissemos (especiaes) porque não abasta, que sejã taes : que hũ delle; se incluya no outro : quaes são, a ley que defende todo mal, & a que defende homicidio, como ho provamos en outra partez (z : in d. c. consideret n. 74). O qual todo, polas seguintes illaçones se decrara.\n"
     ]
    }
   ],
   "source": [
    "    # Initialize a two-dimensional array ...\n",
    "    Editions = [[]]\n",
    "\n",
    "    # ...with four rows\n",
    "    for i in range(4):\n",
    "        a = []\n",
    "        Editions.append(a)\n",
    "\n",
    "    # Now populate it from our sourceTable\n",
    "    for row in sourceTable:\n",
    "        for i, field in enumerate(row):\n",
    "            Editions[i].append(field)\n",
    "\n",
    "    print(str(sourceTable.line_num) + \" rows read.\")\n",
    "\n",
    "    # As an example, see the first seven sections of the second edition:\n",
    "    for field in range(7):\n",
    "        print(Editions[1][field])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF/IDF <a name=\"tfidf\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous (i.e. Solórzano) analyses, things like tokenization, lemmatization and stop-word lists filtering are explained step by step. Here, we rely on what we have found there and feed it all into functions that are ready-made and available in suitable libraries..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we build our lemmatization resource and \"function\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-06T14:18:34.339138Z",
     "start_time": "2018-02-06T14:18:32.023968Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2131211 wordforms known to the system.\n"
     ]
    }
   ],
   "source": [
    "lemma    = {}    # we build a so-called dictionary for the lookups\n",
    "tempdict = []\n",
    "\n",
    "wordfile_path = 'Azpilcueta/wordforms-lat-full.txt'\n",
    "wordfile = open(wordfile_path, encoding='utf-8')\n",
    "\n",
    "# open the wordfile (defined above) for reading\n",
    "wordfile = open(wordfile_path, encoding='utf-8')\n",
    "\n",
    "for line in wordfile.readlines():\n",
    "    tempdict.append(tuple(line.split('>'))) # we split each line by \">\" and append a tuple to a\n",
    "                                            # temporary list.\n",
    "\n",
    "lemma = {k.strip(): v.strip() for k, v in tempdict} # for every tuple in the list,\n",
    "                                                    # we strip whitespace and make a key-value\n",
    "                                                    # pair, appending it to our \"lemma\" dictionary\n",
    "wordfile.close\n",
    "print(str(len(lemma)) + ' wordforms known to the system.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, a quick test: Let's see with which \"lemma\"/basic word the particular wordform \"fidem\" is associated, or, in other words, what *value* our lemma variable returns when we query for the *key* \"fidem\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-06T14:18:34.348145Z",
     "start_time": "2018-02-06T14:18:34.340125Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fides'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma['fidem']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we are going to need the stopwords list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-06T14:18:34.432601Z",
     "start_time": "2018-02-06T14:18:34.350151Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "389 stopwords known to the system, e.g.: ['a', 'ab', 'ac', 'ad', 'adhic', 'adhuc', 'ae', 'ait', 'ali', 'alii', 'aliis', 'alio', 'aliqua', 'aliqui', 'aliquid', 'aliquis', 'aliquo', 'am', 'an', 'ante', 'apud', 'ar', 'at', 'atque', 'au', 'aut', 'autem', 'bus', 'c', 'ca', 'cap', 'ceptum', 'co', 'con', 'cons', 'cui', 'cum', 'cur', 'cùm', 'd', 'da', 'de', 'deinde', 'detur', 'di', 'diu', 'do', 'dum', 'e', 'ea', 'eadem', 'ec', 'eccle', 'ego', 'ei', 'eis', 'eius', 'el', 'em', 'en', 'enim', 'eo', 'eos', 'er', 'erat', 'ergo', 'erit', 'es', 'esse', 'essent', 'esset', 'est', 'et', 'etenim', 'eti']\n"
     ]
    }
   ],
   "source": [
    "stopwords_path = 'Azpilcueta/stopwords-lat.txt'\n",
    "stopwords = open(stopwords_path, encoding='utf-8').read().splitlines()\n",
    "\n",
    "print(str(len(stopwords)) + ' stopwords known to the system, e.g.: ' + str(stopwords[95:170]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(In contrast to simpler numbers that have been filtered out by our stopwords filter, I have left numbers representing years like \"1610\" in place.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we should find some very characteristic words for each segment for each edition. (Let's say we are looking for the \"Top 20\".) We should build a vocabulary for each edition individually and only afterwards work towards a common vocabulary of several \"Top n\" sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-06T14:18:46.583015Z",
     "start_time": "2018-02-06T14:18:34.434607Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "numTopTerms = 20\n",
    "\n",
    "# So first we build a tokenising and lemmatising function to work as an input filter\n",
    "# to the CountVectorizer function\n",
    "def ourLemmatiser(str_input):\n",
    "    wordforms = re.split('\\W+', str_input)\n",
    "    return [lemma[wordform].lower().strip() if wordform in lemma else wordform.lower().strip() for wordform in wordforms ]\n",
    "\n",
    "# !!!!\n",
    "# TODO: The above pipes all the tokens through the latin lemmatizer.\n",
    "# We should lemmatize Spanish and Portuguese differently!\n",
    "# !!!!\n",
    "\n",
    "topTerms = []\n",
    "for i in range(4):\n",
    "\n",
    "    topTermsEd = []\n",
    "    # Initialize the library's function, specifying our\n",
    "    # tokenizing function from above and our stopwords list.\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words=stopwords, use_idf=True, tokenizer=ourLemmatiser, norm='l2')\n",
    "\n",
    "    # Finally, we feed our corpus to the function to build a new \"tfidf_matrix\" object\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(Editions[i])\n",
    "\n",
    "    # convert your matrix to an array to loop over it\n",
    "    mx_array = tfidf_matrix.toarray()\n",
    "\n",
    "    # get your feature names\n",
    "    fn = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "    # now loop through all segments and get the respective top n words.\n",
    "    pos = 0\n",
    "    for j in mx_array:\n",
    "        # We have empty segments, i.e. none of the words in our vocabulary has any tf/idf score > 0\n",
    "        if (j.max() == 0):\n",
    "            topTermsEd.append([(\"\", 0)])\n",
    "        # otherwise append (present) lemmatised words until numTopTerms or the number of words (-stopwords) is reached\n",
    "        else:\n",
    "            topTermsEd.append(\n",
    "                [(fn[x], j[x]) for x in ((j*-1).argsort()) if j[x] > 0] \\\n",
    "                [:min(numTopTerms, len(\n",
    "                    [word for word in re.split('\\W+', Editions[i][pos]) if ourLemmatiser(word) not in stopwords]\n",
    "                ))])\n",
    "        pos += 1\n",
    "    topTerms.append(topTermsEd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-30T12:30:15.482766Z",
     "start_time": "2017-08-30T14:30:15.474338+02:00"
    }
   },
   "source": [
    "# Translations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe there is an approach to inter-lingual comparison after all. After a first unsuccessful try with [conceptnet.io](http://conceptnet.io), I next want to try [Babelnet](http://babelnet.org) in order to lookup synonyms, related terms and translations. I still have to study the [API](http://babelnet.org/guide)...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, let's take this single segment 19:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-06T14:18:46.610979Z",
     "start_time": "2018-02-06T14:18:46.585887Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing words from segments 19 ...\n",
      " \n",
      "Here is the segment in the four editions:\n",
      " \n",
      "Ed. 0:\n",
      "------\n",
      "¶A circunstancia do dia deputado a jejuum, ou oraçam : nam he de necessidade confessata : salvo quando fizesse peccado com proposito de ho quebrantar como acima do dia da festa. Segundo Navarro, ubi supra.\n",
      " \n",
      "Ed. 1:\n",
      "------\n",
      " ¶ A IX que a circunstancia do dia de jejuũ, ou de oração, não se ha de confessar necessariamente, se não quando se pecca com proposito de ho quebrãtar : porque nã faz algũa das ditas tres cousas, segũdo em outra parte ho provamoss (s : f. in d. c. Consideret n. 32 vers. sic. Ad primum) \n",
      " \n",
      "Ed. 2:\n",
      "------\n",
      "17 ¶  El X que la circunstancia del dia de ayuno, o de oracion, no se ha de confessar necessariamente, fino quando se peca con proposito delo quebrantar, por ello por que no haze alguna delas dichas tres cosas, segun lo provamos alibim (m : in d. c. Consideret nu. 32 ver. Ad primum) \n",
      " \n",
      "Ed. 3:\n",
      "------\n",
      "17Decimo. Quod circunstantia diei ieiunio vel orationi consecrati, licet videatur aliquantulum augere peccatum, non est de necessitate confitenda, nisi peccatum perpetretur cum proposito violandi  per illud huiusmodi diem; Quoniam non efficit mortale, quod alias non [p. 69r, 150 pdf] esset tale, nec mutat in speciem mortalis, nec facit ut novo respectu sit tale quorum aliquod requiritur ad hoc un circunstantiae fonfessio sit necessaria, ut supra dictum est nu. 3. & probabimus in princip. dicti. c. Consideret, nu. 32 verb. Ad primum.\n",
      " \n",
      " \n",
      " \n",
      "Most significant words in the segment:\n",
      " \n",
      "Ed. 0:\n",
      "------\n",
      "[('dia', 0.42455257858436557), ('oraçam', 0.2561802690414211), ('jejuum', 0.2561802690414211), ('fizesse', 0.2561802690414211), ('confessata', 0.2561802690414211), ('deputado', 0.2561802690414211), ('acima', 0.2561802690414211), ('salvo', 0.23049808725569565), ('festum', 0.23049808725569565), ('quebrantar', 0.23049808725569565), ('supra', 0.19814236473353714), ('propono', 0.19814236473353714), ('circunstancia', 0.18659410750645733), ('navarro', 0.16091192572073185), ('segundo', 0.15423838498429882), ('necessidade', 0.14820143306213274), ('peccado', 0.13762021834211571), ('quando', 0.13762021834211571), ('ou', 0.12446832979370608), ('he', 0.12062836298264033)]\n",
      " \n",
      "Ed. 1:\n",
      "------\n",
      "[('semi', 0.2795470450623101), ('oração', 0.25472084058951883), ('provamoss', 0.25472084058951883), ('vers', 0.25472084058951883), ('ix', 0.25472084058951883), ('jejuũ', 0.25472084058951883), ('segũdo', 0.22918496713169567), ('quebrãtar', 0.22918496713169567), ('necessariamente', 0.19701357130342106), ('dia', 0.19701357130342106), ('não', 0.19130487601330534), ('pecco', 0.18553110312102511), ('dito', 0.18553110312102511), ('algũa', 0.17582281460300003), ('propono', 0.17582281460300003), ('tres', 0.16741311256817765), ('nã', 0.16741311256817765), ('perodi', 0.15999522966320193), ('cousas', 0.15999522966320193), ('pario', 0.14735714716598278)]\n",
      " \n",
      "Ed. 2:\n",
      "------\n",
      "[('alibim', 0.25386516280297927), ('fino', 0.25386516280297927), ('ayuno', 0.22841507141005182), ('quebrantar', 0.22841507141005182), ('ver', 0.22841507141005182), ('conficio', 0.22841507141005182), ('oracion', 0.22841507141005182), ('delo', 0.22841507141005182), ('delas', 0.21035794420084336), ('por', 0.18573864612357718), ('necessariamente', 0.18490785280791594), ('semi', 0.17643547982620192), ('provamos', 0.17523217711737277), ('alguna', 0.17523217711737277), ('peca', 0.17523217711737277), ('dichas', 0.17523217711737277), ('propono', 0.17523217711737277), ('dia', 0.17523217711737277), ('ello', 0.16685072559870748), ('segun', 0.16685072559870748)]\n",
      " \n",
      "Ed. 3:\n",
      "------\n",
      "[('dies', 0.28024949092614448), ('69r', 0.2030037628071113), ('aliquantulum', 0.2030037628071113), ('requiro', 0.2030037628071113), ('17decimo', 0.2030037628071113), ('consecro', 0.2030037628071113), ('fonfessio', 0.2030037628071113), ('mortalis', 0.19726374238722788), ('oratio', 0.18265254856603502), ('ieiunium', 0.18265254856603502), ('novo', 0.18265254856603502), ('violo', 0.18265254856603502), ('necessitas', 0.18265254856603502), ('nu', 0.17486357956927889), ('circunstantiae', 0.16821313226927886), ('princip', 0.16821313226927886), ('video', 0.16821313226927886), ('dico', 0.16083364123353905), ('necessarius', 0.15701305086030437), ('probo', 0.15701305086030437)]\n",
      " \n"
     ]
    }
   ],
   "source": [
    "startEd = 3\n",
    "segment_no = 19 \n",
    "\n",
    "print(\"Comparing words from segments \" + str(segment_no) + \" ...\")\n",
    "\n",
    "print(\" \")\n",
    "print(\"Here is the segment in the four editions:\")\n",
    "print(\" \")\n",
    "for i in range(4):\n",
    "    print(\"Ed. \" + str(i) + \":\")\n",
    "    print(\"------\")\n",
    "    print(Editions[i][segment_no])\n",
    "    print(\" \")\n",
    "\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "\n",
    "# Build List of most significant words for a segment\n",
    "\n",
    "print(\"Most significant words in the segment:\")\n",
    "print(\" \")\n",
    "for i in range(4):\n",
    "    print(\"Ed. \" + str(i) + \":\")\n",
    "    print(\"------\")\n",
    "    print(topTerms[i][segment_no])\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we look up the \"concepts\" associated to those words in babelnet. Then we look up the concepts associated with the words of the present segment from another edition/language, and see if the concepts are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-06T14:18:55.913513Z",
     "start_time": "2018-02-06T14:18:46.612958Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "For each of the latin words, here are possible synsets:\n",
      " \n",
      "dies: bn:00025419n, bn:00025341n, bn:00000086n, bn:00025422n, bn:00025417n\n",
      " \n",
      "oratio: bn:00027523n, bn:16473014n\n",
      " \n",
      "ieiunium: bn:00033737n\n",
      " \n",
      "novo: bn:00105752a, bn:00107274a, bn:00107275a, bn:00109521a, bn:00107267a, bn:00103327a, bn:00103328a, bn:02989482n, bn:06978195n\n",
      " \n",
      "necessitas: bn:03277980n, bn:00046093n\n",
      " \n",
      "nu: bn:00058225n, bn:00058342n, bn:00351758n, bn:00058316n, bn:10185248n, bn:08302435n, bn:03284263n, bn:00097745a, bn:03295401n, bn:06965733n, bn:00056748n, bn:00078931n, bn:14479066n, bn:00056688n\n",
      " \n",
      "video: bn:00079978n, bn:00062276n, bn:00079972n, bn:08263856n, bn:00093430v, bn:00091096v, bn:00076373n, bn:00085496v, bn:00093435v, bn:08263853n, bn:00085652v, bn:00082727v, bn:00092640v\n",
      " \n",
      "dico: bn:00082800v, bn:00093292v, bn:00093287v\n",
      " \n",
      "probo: bn:00086567v, bn:03617190n\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "For each of the Spanish words, here are possible synsets:\n",
      " \n",
      "fino: bn:00105687a, bn:00100876a, bn:00102975a, bn:00098683a, bn:00034612n, bn:00099735a, bn:17174869n, bn:00110742a, bn:00101122a, bn:00099634a, bn:00101501a, bn:00096409a, bn:16569518n, bn:00102104a, bn:00102974a, bn:00111743a, bn:00102972a, bn:00111843a, bn:00096725a, bn:00102993a\n",
      " \n",
      "ayuno: bn:00033737n\n",
      " \n",
      "quebrantar: bn:00083911v, bn:00093586v, bn:13772797v\n",
      " \n",
      "ver: bn:05674200n, bn:00092443v, bn:00086664v, bn:00818735n, bn:00083356v, bn:00085339v, bn:00091096v, bn:00093431v, bn:00093432v, bn:00086708v, bn:00085653v, bn:00082813v, bn:00093438v, bn:00095665v, bn:00085334v, bn:00093436v, bn:00090661v, bn:00088674v, bn:03207003n, bn:00088705v, bn:00090463v, bn:00082908v, bn:00088428v, bn:00070252n, bn:00084662v, bn:00009669n, bn:00085647v, bn:00093430v, bn:00087701v, bn:00085496v, bn:00093435v, bn:00087940v, bn:00082701v, bn:00093433v, bn:00090448v, bn:00082812v, bn:00085652v, bn:00073636n, bn:00091119v, bn:00086933v, bn:00093726v, bn:02168192n, bn:00088869v, bn:00088205v, bn:00088746v\n",
      " \n",
      "oracion: bn:00059529n, bn:00070528n, bn:08296656n\n",
      " \n",
      "delo: bn:02567443n\n",
      " \n",
      "por: bn:00114629r, bn:00087733v, bn:16037816n, bn:13603718a, bn:00115110r\n",
      " \n",
      "necessariamente: bn:00115917r, bn:00116330r, bn:00116331r\n",
      " \n",
      "semi: bn:10825508n, bn:00070409n\n",
      " \n",
      "peca: bn:03556581n, bn:00036309n\n",
      " \n",
      "dia: bn:00025419n, bn:06181956n, bn:00014708n, bn:00077268n, bn:00000086n, bn:03701000n, bn:00081592n, bn:00117066r, bn:00025422n, bn:03180434n, bn:00025423n, bn:15617169n, bn:03704045n, bn:00025336n, bn:00025459n, bn:00025417n, bn:03799214n, bn:00025418n, bn:00456074n, bn:00025420n, bn:00025893n, bn:00082037n\n",
      " \n",
      "ello: bn:03415425n, bn:04601707n, bn:02076169n, bn:16953746n, bn:03538631n\n",
      " \n",
      "segun: bn:13959262n\n",
      " \n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "babelAPIKey = '18546fd3-8999-43db-ac31-dc113506f825'\n",
    "babelGetSynsetIdsURL = \"https://babelnet.io/v4/getSynsetIds?\" + \\\n",
    "                       \"langs=LA&langs=ES&langs=PT\" + \\\n",
    "                       \"&filterLangs=LA&filterLangs=ES&filterLangs=PT\" + \\\n",
    "                       \"&key=\" + babelAPIKey + \\\n",
    "                       \"&word=\"\n",
    "\n",
    "# Build lists of possible concepts\n",
    "top_possible_conceptIDs = defaultdict(list)\n",
    "for (word, val) in topTerms[startEd][segment_no]:\n",
    "    concepts_uri = babelGetSynsetIdsURL + word\n",
    "    response = urllib.request.urlopen(concepts_uri)\n",
    "    conceptIDs = json.loads(response.read().decode(response.info().get_param('charset') or 'utf-8'))\n",
    "    for rel in conceptIDs:\n",
    "        top_possible_conceptIDs[word].append(rel.get(\"id\"))\n",
    "\n",
    "print(\" \")\n",
    "print(\"For each of the latin words, here are possible synsets:\")\n",
    "print(\" \")\n",
    "for word in top_possible_conceptIDs:\n",
    "    print(word + \":\" + \" \" + ', '.join(c for c in top_possible_conceptIDs[word]))\n",
    "    print(\" \")\n",
    "\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "# Build list of 10 most significant words in the second language\n",
    "top_possible_conceptIDs_2 = defaultdict(list)\n",
    "for (word, val) in topTerms[2][segment_no]:\n",
    "    concepts_uri = babelGetSynsetIdsURL + word\n",
    "    response = urllib.request.urlopen(concepts_uri)\n",
    "    conceptIDs = json.loads(response.read().decode(response.info().get_param('charset') or 'utf-8'))\n",
    "    for rel in conceptIDs:\n",
    "        top_possible_conceptIDs_2[word].append(rel.get(\"id\"))\n",
    "\n",
    "print(\" \")\n",
    "print(\"For each of the Spanish words, here are possible synsets:\")\n",
    "print(\" \")\n",
    "for word in top_possible_conceptIDs_2:\n",
    "    print(word + \":\" + \" \" + ', '.join(c for c in top_possible_conceptIDs_2[word]))\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-06T14:18:58.199197Z",
     "start_time": "2018-02-06T14:18:55.914529Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlaps:\n",
      "bn:00025422n: día (es)\n",
      "bn:00093430v: ver (es)\n",
      "bn:00000086n: día (es)\n",
      "bn:00085496v: percibir (es)\n",
      "bn:00025419n: día (es)\n",
      "bn:00093435v: ver (es)\n",
      "bn:00025417n: día (es)\n",
      "bn:00033737n: ayuno (es)\n",
      "bn:00091096v: notar (es)\n",
      "bn:00085652v: considerar (es)\n"
     ]
    }
   ],
   "source": [
    "# calculate number of overlapping terms\n",
    "values_a = set([item for sublist in top_possible_conceptIDs.values() for item in sublist])\n",
    "values_b = set([item for sublist in top_possible_conceptIDs_2.values() for item in sublist])\n",
    "overlaps = values_a & values_b\n",
    "print(\"Overlaps:\")\n",
    "\n",
    "babelGetSynsetInfoURL = \"https://babelnet.io/v4/getSynset?key=\" + babelAPIKey + \\\n",
    "                        \"&filterLangs=LA&filterLangs=ES&filterLangs=PT\" + \\\n",
    "                        \"&id=\"\n",
    "\n",
    "for c in overlaps:\n",
    "    info_uri = babelGetSynsetInfoURL + c\n",
    "    response = urllib.request.urlopen(info_uri)\n",
    "    words = json.loads(response.read().decode(response.info().get_param('charset') or 'utf-8'))\n",
    "    \n",
    "    senses = words['senses']\n",
    "    for result in senses[:1]:\n",
    "        lemma = result.get('lemma')\n",
    "        language = result.get('language')\n",
    "        print(c + \": \" + lemma + \" (\" + language.lower() + \")\")\n",
    "\n",
    "# do a nifty ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-11T07:42:37.598721Z",
     "start_time": "2017-07-11T09:42:37.587926+02:00"
    }
   },
   "source": [
    "# Similarity <a name=\"DocumentSimilarity\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems we could now create another matrix replacing lemmata with concepts and retaining the tf/idf values (so as to keep a weight coefficient to the concepts). Then we should be able to calculate similarity measures across the same concepts...\n",
    "\n",
    "The approach to choose would probably be the \"cosine similarity\" of concept vector spaces. Again, there is a library ready for us to use (but you can find some documentation [here](http://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/), [here](http://scikit-learn.org/stable/modules/metrics.html#cosine-similarity) and [here](https://en.wikipedia.org/wiki/Cosine_similarity).)\n",
    "\n",
    "**However, this is where I have to take a break now. I will return to here soon...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-06T14:18:58.454301Z",
     "start_time": "2018-02-06T14:18:58.200131Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairwise similarities:\n",
      "     0         1    2         3         4         5         6         7   \\\n",
      "0   0.0  0.000000  0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "1   0.0  0.000000  0.0  0.021730  0.051073  0.009358  0.051261  0.013045   \n",
      "2   0.0  0.000000  0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "3   0.0  0.021730  0.0  0.000000  0.128698  0.201813  0.205456  0.066350   \n",
      "4   0.0  0.051073  0.0  0.128698  0.000000  0.148800  0.176078  0.045951   \n",
      "5   0.0  0.009358  0.0  0.201813  0.148800  0.000000  0.100226  0.039999   \n",
      "6   0.0  0.051261  0.0  0.205456  0.176078  0.100226  0.000000  0.139324   \n",
      "7   0.0  0.013045  0.0  0.066350  0.045951  0.039999  0.139324  0.000000   \n",
      "8   0.0  0.013191  0.0  0.086402  0.041347  0.083278  0.043033  0.049024   \n",
      "9   0.0  0.000000  0.0  0.155631  0.064009  0.079912  0.165588  0.063541   \n",
      "10  0.0  0.000000  0.0  0.123848  0.070881  0.070336  0.197763  0.081927   \n",
      "11  0.0  0.030417  0.0  0.098265  0.090397  0.039227  0.187536  0.116556   \n",
      "12  0.0  0.000000  0.0  0.151205  0.024834  0.036400  0.152133  0.000000   \n",
      "13  0.0  0.000000  0.0  0.104908  0.018484  0.011687  0.142064  0.101306   \n",
      "14  0.0  0.044368  0.0  0.169226  0.158355  0.076969  0.153209  0.079009   \n",
      "15  0.0  0.060683  0.0  0.217255  0.136815  0.103885  0.359562  0.101345   \n",
      "16  0.0  0.000000  0.0  0.035570  0.010827  0.047700  0.013926  0.028413   \n",
      "17  0.0  0.000000  0.0  0.043121  0.050876  0.057991  0.070963  0.000000   \n",
      "18  0.0  0.011369  0.0  0.204398  0.141631  0.103165  0.147657  0.097670   \n",
      "19  0.0  0.068881  0.0  0.291043  0.118267  0.106216  0.247292  0.078674   \n",
      "20  0.0  0.028680  0.0  0.181806  0.158103  0.055562  0.225556  0.060079   \n",
      "21  0.0  0.008600  0.0  0.137637  0.092614  0.073125  0.065372  0.018312   \n",
      "22  0.0  0.004625  0.0  0.171417  0.115501  0.073288  0.230436  0.120143   \n",
      "23  0.0  0.010574  0.0  0.181729  0.077895  0.023005  0.080518  0.082326   \n",
      "24  0.0  0.026889  0.0  0.117802  0.190428  0.106502  0.095976  0.015334   \n",
      "25  0.0  0.015733  0.0  0.079456  0.080416  0.152222  0.138008  0.018197   \n",
      "26  0.0  0.000000  0.0  0.032237  0.057140  0.054870  0.074963  0.015297   \n",
      "27  0.0  0.020386  0.0  0.086238  0.113863  0.071122  0.093097  0.011625   \n",
      "28  0.0  0.022178  0.0  0.062443  0.137511  0.047652  0.106960  0.023145   \n",
      "29  0.0  0.030442  0.0  0.078778  0.131235  0.048595  0.149009  0.048525   \n",
      "30  0.0  0.015426  0.0  0.063560  0.088001  0.051146  0.118584  0.039169   \n",
      "31  0.0  0.020554  0.0  0.114147  0.094184  0.055100  0.090597  0.033131   \n",
      "32  0.0  0.019519  0.0  0.149998  0.152454  0.092845  0.201042  0.057362   \n",
      "33  0.0  0.006058  0.0  0.158423  0.102772  0.064282  0.112504  0.036514   \n",
      "34  0.0  0.055137  0.0  0.233052  0.221249  0.049567  0.280305  0.108418   \n",
      "35  0.0  0.000000  0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "36  0.0  0.000000  0.0  0.000000  0.000000  0.000000  0.025687  0.050995   \n",
      "37  0.0  0.027714  0.0  0.217779  0.151922  0.056087  0.082443  0.088841   \n",
      "38  0.0  0.028305  0.0  0.279375  0.075961  0.136870  0.049512  0.016142   \n",
      "39  0.0  0.000000  0.0  0.229442  0.036174  0.099502  0.045404  0.017523   \n",
      "40  0.0  0.000000  0.0  0.156777  0.009858  0.038614  0.015219  0.000000   \n",
      "\n",
      "          8         9     ...           31        32        33        34   35  \\\n",
      "0   0.000000  0.000000    ...     0.000000  0.000000  0.000000  0.000000  0.0   \n",
      "1   0.013191  0.000000    ...     0.020554  0.019519  0.006058  0.055137  0.0   \n",
      "2   0.000000  0.000000    ...     0.000000  0.000000  0.000000  0.000000  0.0   \n",
      "3   0.086402  0.155631    ...     0.114147  0.149998  0.158423  0.233052  0.0   \n",
      "4   0.041347  0.064009    ...     0.094184  0.152454  0.102772  0.221249  0.0   \n",
      "5   0.083278  0.079912    ...     0.055100  0.092845  0.064282  0.049567  0.0   \n",
      "6   0.043033  0.165588    ...     0.090597  0.201042  0.112504  0.280305  0.0   \n",
      "7   0.049024  0.063541    ...     0.033131  0.057362  0.036514  0.108418  0.0   \n",
      "8   0.000000  0.007427    ...     0.046187  0.041326  0.044584  0.109771  0.0   \n",
      "9   0.007427  0.000000    ...     0.044701  0.085289  0.086872  0.054796  0.0   \n",
      "10  0.000000  0.350512    ...     0.043717  0.080787  0.050293  0.022624  0.0   \n",
      "11  0.063855  0.146718    ...     0.024900  0.101029  0.134317  0.119015  0.0   \n",
      "12  0.047109  0.138064    ...     0.049746  0.034742  0.021809  0.036824  0.0   \n",
      "13  0.025158  0.054181    ...     0.035205  0.040650  0.063281  0.096453  0.0   \n",
      "14  0.065453  0.056545    ...     0.084353  0.169153  0.133299  0.178935  0.0   \n",
      "15  0.056006  0.103817    ...     0.108354  0.212839  0.182166  0.245835  0.0   \n",
      "16  0.039915  0.022153    ...     0.000000  0.023334  0.045445  0.010603  0.0   \n",
      "17  0.027325  0.083655    ...     0.030565  0.102626  0.031562  0.049895  0.0   \n",
      "18  0.021630  0.084899    ...     0.042359  0.211675  0.059652  0.123035  0.0   \n",
      "19  0.072191  0.095166    ...     0.110543  0.154370  0.079252  0.143231  0.0   \n",
      "20  0.028870  0.153084    ...     0.055201  0.139640  0.049120  0.159665  0.0   \n",
      "21  0.040272  0.134853    ...     0.058090  0.108850  0.138802  0.102653  0.0   \n",
      "22  0.038230  0.119003    ...     0.135868  0.172904  0.126082  0.151506  0.0   \n",
      "23  0.014432  0.058070    ...     0.032093  0.129441  0.034123  0.132603  0.0   \n",
      "24  0.044119  0.020379    ...     0.081073  0.107085  0.063765  0.096926  0.0   \n",
      "25  0.050107  0.048883    ...     0.080837  0.096284  0.095001  0.103459  0.0   \n",
      "26  0.023867  0.031624    ...     0.105032  0.083174  0.138857  0.067212  0.0   \n",
      "27  0.029863  0.032721    ...     0.054293  0.143156  0.026905  0.134225  0.0   \n",
      "28  0.036732  0.039517    ...     0.069141  0.075839  0.042507  0.108950  0.0   \n",
      "29  0.049232  0.049448    ...     0.098229  0.096394  0.107710  0.145330  0.0   \n",
      "30  0.044741  0.039488    ...     0.050560  0.091355  0.123103  0.179394  0.0   \n",
      "31  0.046187  0.044701    ...     0.000000  0.055964  0.119985  0.106185  0.0   \n",
      "32  0.041326  0.085289    ...     0.055964  0.000000  0.073144  0.173382  0.0   \n",
      "33  0.044584  0.086872    ...     0.119985  0.073144  0.000000  0.199344  0.0   \n",
      "34  0.109771  0.054796    ...     0.106185  0.173382  0.199344  0.000000  0.0   \n",
      "35  0.000000  0.000000    ...     0.000000  0.000000  0.000000  0.000000  0.0   \n",
      "36  0.000000  0.000000    ...     0.044219  0.097290  0.000000  0.105544  0.0   \n",
      "37  0.015981  0.011258    ...     0.038315  0.053977  0.097382  0.107507  0.0   \n",
      "38  0.016322  0.011499    ...     0.039132  0.041046  0.037088  0.093226  0.0   \n",
      "39  0.017718  0.009132    ...     0.020082  0.023904  0.080467  0.106709  0.0   \n",
      "40  0.000000  0.000000    ...     0.010579  0.040326  0.009355  0.014189  0.0   \n",
      "\n",
      "          36        37        38        39        40  \n",
      "0   0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "1   0.000000  0.027714  0.028305  0.000000  0.000000  \n",
      "2   0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "3   0.000000  0.217779  0.279375  0.229442  0.156777  \n",
      "4   0.000000  0.151922  0.075961  0.036174  0.009858  \n",
      "5   0.000000  0.056087  0.136870  0.099502  0.038614  \n",
      "6   0.025687  0.082443  0.049512  0.045404  0.015219  \n",
      "7   0.050995  0.088841  0.016142  0.017523  0.000000  \n",
      "8   0.000000  0.015981  0.016322  0.017718  0.000000  \n",
      "9   0.000000  0.011258  0.011499  0.009132  0.000000  \n",
      "10  0.000000  0.044623  0.000000  0.000000  0.000000  \n",
      "11  0.021153  0.029482  0.030111  0.040401  0.000000  \n",
      "12  0.000000  0.000000  0.000000  0.056151  0.000000  \n",
      "13  0.000000  0.057426  0.017676  0.030497  0.000000  \n",
      "14  0.020761  0.078438  0.035342  0.046924  0.033144  \n",
      "15  0.047529  0.063900  0.068499  0.062616  0.019991  \n",
      "16  0.000000  0.016954  0.017316  0.013752  0.000000  \n",
      "17  0.000000  0.015499  0.015829  0.012571  0.000000  \n",
      "18  0.000000  0.044980  0.025434  0.016662  0.038138  \n",
      "19  0.000000  0.065937  0.120768  0.047010  0.032126  \n",
      "20  0.000000  0.070196  0.045830  0.017845  0.018516  \n",
      "21  0.000000  0.133545  0.047857  0.052659  0.000000  \n",
      "22  0.184012  0.036330  0.045391  0.031278  0.031028  \n",
      "23  0.154021  0.092714  0.034226  0.030994  0.043635  \n",
      "24  0.028250  0.045737  0.109821  0.033377  0.010380  \n",
      "25  0.033059  0.030171  0.211970  0.134948  0.000000  \n",
      "26  0.056067  0.009762  0.009970  0.017203  0.000000  \n",
      "27  0.057114  0.034295  0.133717  0.086161  0.017548  \n",
      "28  0.000000  0.060609  0.084682  0.027367  0.006849  \n",
      "29  0.014875  0.069125  0.070600  0.046785  0.000000  \n",
      "30  0.047490  0.029582  0.046614  0.070616  0.000000  \n",
      "31  0.044219  0.038315  0.039132  0.020082  0.010579  \n",
      "32  0.097290  0.053977  0.041046  0.023904  0.040326  \n",
      "33  0.000000  0.097382  0.037088  0.080467  0.009355  \n",
      "34  0.105544  0.107507  0.093226  0.106709  0.014189  \n",
      "35  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "36  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "37  0.000000  0.000000  0.371503  0.075753  0.042792  \n",
      "38  0.000000  0.371503  0.000000  0.339323  0.043705  \n",
      "39  0.000000  0.075753  0.339323  0.000000  0.034709  \n",
      "40  0.000000  0.042792  0.043705  0.034709  0.000000  \n",
      "\n",
      "[41 rows x 41 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarities = pd.DataFrame(cosine_similarity(tfidf_matrix))\n",
    "similarities[round(similarities, 0) == 1] = 0 # Suppress a document's similarity to itself\n",
    "print(\"Pairwise similarities:\")\n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-06T14:18:58.542536Z",
     "start_time": "2018-02-06T14:18:58.455305Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The two most similar segments in the corpus are\n",
      "segments 37 and 38 .\n",
      "They have a similarity score of\n",
      "0.371503404623\n"
     ]
    }
   ],
   "source": [
    "print(\"The two most similar segments in the corpus are\")\n",
    "print(\"segments\", \\\n",
    "      similarities[similarities == similarities.values.max()].idxmax(axis=0).idxmax(axis=1), \\\n",
    "      \"and\", \\\n",
    "      similarities[similarities == similarities.values.max()].idxmax(axis=0)[ similarities[similarities == similarities.values.max()].idxmax(axis=0).idxmax(axis=1) ].astype(int), \\\n",
    "      \".\")\n",
    "print(\"They have a similarity score of\")\n",
    "print(similarities.values.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alertbox alert-success\">Of course, in every set of documents, we will always find two that are similar in the sense of them being more similar to each other than to the other ones. Whether or not this actually *means* anything in terms of content is still up to scholarly interpretation. But at least it means that a scholar can look at the two documents and when she determines that they are not so similar after all, then perhaps there is something interesting to say about similar vocabulary used for different puproses. Or the other way round: When the scholar knows that two passages are similar, but they have a low \"similarity score\", shouldn't that say something about the texts's rhetorics?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Clouds <a name=\"WordClouds\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a library that takes word frequencies like above, calculates corresponding relative sizes of words and creates nice wordcloud images for our sections (again, taking the fourth segment as an example) like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-06T14:19:00.407048Z",
     "start_time": "2018-02-06T14:18:58.545544Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-91283a5980e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# We make tuples of (lemma, tf/idf score) for one of our segments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# But we have to convert our tf/idf weights to pseudo-frequencies (i.e. integer numbers)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# We make tuples of (lemma, tf/idf score) for one of our segments\n",
    "# But we have to convert our tf/idf weights to pseudo-frequencies (i.e. integer numbers)\n",
    "frq = [ int(round(x * 100000, 0)) for x in Editions[1][3]]\n",
    "freq = dict(zip(fn, frq))\n",
    "\n",
    "wc = WordCloud(background_color=None, mode=\"RGBA\", max_font_size=40, relative_scaling=1).fit_words(freq)\n",
    "\n",
    "# Now show/plot the wordcloud\n",
    "plt.figure()\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to have a nicer overview over the many segments than is possible in this notebook, let's create a new html file listing some of the characteristics that we have found so far..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-06T14:19:00.407048Z",
     "start_time": "2018-02-06T14:18:32.804Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputDir = \"Azpilcueta\"\n",
    "htmlfile = open(outputDir + '/Overview.html', encoding='utf-8', mode='w')\n",
    "\n",
    "# Write the html header and the opening of a layout table\n",
    "htmlfile.write(\"\"\"<!DOCTYPE html>\n",
    "<html>\n",
    "    <head>\n",
    "        <title>Section Characteristics</title>\n",
    "        <meta charset=\"utf-8\"/>\n",
    "    </head>\n",
    "    <body>\n",
    "        <table>\n",
    "\"\"\")\n",
    "\n",
    "a = [[]]\n",
    "a.clear()\n",
    "dicts = []\n",
    "w = []\n",
    "\n",
    "# For each segment, create a wordcloud and write it along with label and\n",
    "# other information into a new row of the html table\n",
    "for i in range(len(mx_array)):\n",
    "    # this is like above in the single-segment example...\n",
    "    a.append([ int(round(x * 100000, 0)) for x in mx_array[i]])\n",
    "    dicts.append(dict(zip(fn, a[i])))\n",
    "    w.append(WordCloud(background_color=None, mode=\"RGBA\", \\\n",
    "                       max_font_size=40, min_font_size=10, \\\n",
    "                       max_words=60, relative_scaling=0.8).fit_words(dicts[i]))\n",
    "    # We write the wordcloud image to a file\n",
    "    w[i].to_file(outputDir + '/wc_' + str(i) + '.png')\n",
    "    # Finally we write the column row\n",
    "    htmlfile.write(\"\"\"\n",
    "            <tr>\n",
    "                <td>\n",
    "                    <head>Section {a}: <b>{b}</b></head><br/>\n",
    "                    <img src=\"./wc_{a}.png\"/><br/>\n",
    "                    <small><i>length: {c} words</i></small>\n",
    "                </td>\n",
    "            </tr>\n",
    "            <tr><td>&nbsp;</td></tr>\n",
    "\"\"\".format(a = str(i), b = label[i], c = len(tokenised[i])))\n",
    "\n",
    "# And then we write the end of the html file.\n",
    "htmlfile.write(\"\"\"\n",
    "        </table>\n",
    "    </body>\n",
    "</html>\n",
    "\"\"\")\n",
    "htmlfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should have created a nice html file which we can open [here](./Solorzano/Overview.html)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {
    "height": "245px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
