{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Text-Processing\" data-toc-modified-id=\"Text-Processing-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Text Processing</a></div><div class=\"lev2 toc-item\"><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Introduction</a></div><div class=\"lev1 toc-item\"><a href=\"#Preparations\" data-toc-modified-id=\"Preparations-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Preparations</a></div><div class=\"lev2 toc-item\"><a href=\"#TF/IDF-\" data-toc-modified-id=\"TF/IDF--21\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>TF/IDF </a></div><div class=\"lev2 toc-item\"><a href=\"#Translations?\" data-toc-modified-id=\"Translations?-22\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Translations?</a></div><div class=\"lev2 toc-item\"><a href=\"#Similarity-\" data-toc-modified-id=\"Similarity--23\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Similarity </a></div><div class=\"lev2 toc-item\"><a href=\"#Word-Clouds-\" data-toc-modified-id=\"Word-Clouds--24\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Word Clouds </a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file is the continuation of preceding work. Previously, I have worked my way through a couple of text-analysing approaches - such as tf/idf frequencies, n-grams and the like - in the context of a project concerned with Juan de Solórzano Pereira's *Politica Indiana*. This can be seen [here](TextProcessing_Solorzano.ipynb).\n",
    "\n",
    "In the former context, I got somewhat stuck when I was trying to automatically align corresponding passages of two editions of the same work ... where the one edition would be a **translation** of the other and thus we would have two different languages.\n",
    "\n",
    "The present file takes this up, tries to refine an approach taken there and to find alternative ways of analysing a text across several languages. This time, the work concerned is Martín de Azpilcueta's *Manual de confesores*, a work of the 16th century that has seen very many editions and translations, quite a few of them even by the work's original author and it is the subject of the research project [\"Martín de Azpilcueta’s Manual for Confessors and the Phenomenon of Epitomisation\"](http://www.rg.mpg.de/research/martin-de-azpilcuetas-manual-for-confessors) by Manuela Bragagnolo. \n",
    "\n",
    "(There are a few DH-ey things about the project that are not directly of concern here, like a synoptic display of several editions or the presentation of the divergence of many actual translations of a given term. Such aspects are being treated with other software, like [HyperMachiavel](http://hyperprince.ens-lyon.fr/hypermachiavel) or [Lera](http://lera.uzi.uni-halle.de/).)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous case, the programming language used in the following examples is called \"python\" and the tool used to get prose discussion and code samples together is called [\"jupyter\"](http://jupyter.org/). (A common way of installing both the language and the jupyter software, especially in windows, is by installing a python \"distribution\" like [Anaconda](https://www.anaconda.com/what-is-anaconda/).) In jupyter, you have a \"notebook\" that you can populate with text (if you want to use it, jupyter understands [markdown](http://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Working%20With%20Markdown%20Cells.html) code formatting), or code and a program that pipes a nice rendering of the notebook to a web browser as you are reading right now. In many places in such a notebook, the output that the code samples produce is printed right below the code itself. Sometimes this can be quite a lot of output and depending on your viewing environment you might have to scroll quite some way to get to the continuation of the discussion.\n",
    "\n",
    "You can save your notebook online (the current one is [here at github](https://github.com/awagner-mainz/notebooks/blob/master/gallery/TextProcessing_Azpilcueta.ipynb)) and there is an online service, nbviewer, able to render any notebook that it can access online. So chances are you are reading this present notebook at the web address [https://nbviewer.jupyter.org/github/awagner-mainz/notebooks/blob/master/gallery/TextProcessing_Azpilcueta.ipynb](https://nbviewer.jupyter.org/github/awagner-mainz/notebooks/blob/master/gallery/TextProcessing_Azpilcueta.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A final word about the elements of this notebook:\n",
    "\n",
    "<div class=\"alert alertbox alert-success\">At some points I am mentioning things I consider to be important decisions or take-away messages for scholarly readers. E.g. whether or not to insert certain artefacts into the very transcription of your text, what the methodological ramifications of a certain approach or parameter are, what the implications of an example solution are, or what a possible interpretation of a certain result might be. I am highlighting these things in a block like this one here or at least in <font color=\"green\">**green bold font**</font>.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alertbox alert-danger\">**NOTE:** As I am continually improving the notebook on the side of the source text, wordlists and other parameters, it is sometimes hard to keep the prose description in synch. So while the actual descriptions still apply, the numbers that are mentioned in the prose (as where we have e.g. a \"table with 20 rows and 1.672 columns\") might no longer reflect the latest state of the sources, auxiliary files and parameters and you should take these with a grain of salt. Best double check them by reading the actual code ;-)\n",
    "\n",
    "I apologize for the inconsistency.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike in the previous case, where we had word files that we could export as plaintext, in this case Manuela has prepared a sample chapter with four editions transcribed *in parallel* in an office spreadsheet. So we first of all make sure that we have good **UTF-8** comma-separated-value files, e.g. by uploading a **csv** export of our office program of choice to [a CSV Linting service](https://csvlint.io/). (As a side remark, in my case, exporting with LibreOffice provided me with options to select UTF-8 encoding and choose the field delimiter and resulted in a valid csv file. MS Excel did neither of those.) Below, we expect the file below at the following position:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T23:21:46.165533Z",
     "start_time": "2018-02-05T23:21:46.156580Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sourcePath = 'Azpilcueta/cap6_align_-_2018-01.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can go ahead and open the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T23:21:46.381127Z",
     "start_time": "2018-02-05T23:21:46.369460Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "sourceFile = open(sourcePath, newline='', encoding='utf-8')\n",
    "sourceTable = csv.reader(sourceFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T15:33:14.158534Z",
     "start_time": "2018-02-05T15:33:14.154549Z"
    }
   },
   "source": [
    "And we read each line into new elements of four respective arrays (since we're dealing with one sample chapter, we try to handle it all in memory first and see if we run into problems):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Note here and in the following that in most cases, when the program is counting, it does so beginning with zero. Which means that if we end up with 20 segments, they are going to be called segment 0, segment 1, ..., segment 19. There is not going to be a segment bearing the number twenty, although we do have twenty segments. The first one has the number zero and the twentieth one has the number nineteen. Even for more experienced coders, this sometimes leads to mistakes, called \"off-by-one errors\".)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T23:21:46.789886Z",
     "start_time": "2018-02-05T23:21:46.733411Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 rows read.\n",
      "1552 por\n",
      "¶ Capitolo VI. Das circunstancias.\n",
      "\n",
      "\n",
      "1.     [1] Pera fundamento disto diremos : lho primeiro, que a circumstancia do peccado, segundo a mente de S. Tho P & outros, he hum accidente daquilo, que he peccado. Dissemos (he accidente) porque nenhũa circũstãcia da obra, he a substancia della. Dissemos (da quilo que he peccado) & nã do peccado : porque muytas vezes a obra em si não he peccado, & pola circũ se faz peccado : & como então ella he aquilo, em que consiste ho peccado, não he tãto accidente do peccado, quanto da quilo que he peccado : segundo que ho declaramos em outra parteq (q. in d. c. Consideret n. 3), seguindo a Alex. de Ales r (r in. 4 pt. q. 77 ar.z. co.la.z).\n",
      "2.     ¶ [2] Ho II. Que a circunstancia se parte em sete species, que se conte nem aquelle verso : Quis, quid, ubi, quibus auxiliis, cur, quomodo, quando : Referido por S. Tho.s (in d. q. 7. ar 3) . Quem, Que, Onde, Com que ajudas, Porque, Em que maneira, Quando. O qual verso temos por melhor, que ho de Paudanot (t. in. 4. d. 16. q. 3. art. 1.u), como ho dissemos em outra parteu (u in princi. d. e. Consideret n. 4) : porque nelle se acrecenta Quotiens, quantas vezes : que denota ho numero : o qual não he circunstancia, se não multiplicação de peccado, como aly ho dissemosx (x. in. d. n. 4). [p. 33, 41 pdf] \n"
     ]
    }
   ],
   "source": [
    "    # Initialize a two-dimensional array ...\n",
    "    Editions = [[]]\n",
    "\n",
    "    # ...with four rows\n",
    "    for i in range(0,3):\n",
    "        a = []\n",
    "        Editions.append(a)\n",
    "\n",
    "    # Now populate it from our sourceTable\n",
    "    for row in sourceTable:\n",
    "        for i, field in enumerate(row):\n",
    "            Editions[i].append(field)\n",
    "\n",
    "    print(str(sourceTable.line_num) + \" rows read.\")\n",
    "\n",
    "    # As an example, see the first seven sections of the second edition:\n",
    "    for field in range(0,6):\n",
    "        print(Editions[1][field])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  1. [TF/IDF](#tfidf)\n",
    "  \n",
    "  2. [Segment source text](#SegmentSourceText) \n",
    "  3. [Read segments into Variable/List](#ReadSegmentsIntoVariable)\n",
    "  4. [Tokenising](#Tokenising)\n",
    "  5. [Stemming/Lemmatising](#StemmingLemmatising)\n",
    "  6. [Eliminate stopwords](#EliminateStopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF/IDF <a name=\"tfidf\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous (i.e. Solórzano) analyses, things like tokenization, lemmatization and stop-word lists filtering are explained step by step. Here, we rely on what we have found there and feed it all into functions that are ready-made and available in suitable libraries..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we build our lemmatization resource and \"function\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T23:21:47.553135Z",
     "start_time": "2018-02-05T23:21:47.502084Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1706 wordforms known to the system.\n"
     ]
    }
   ],
   "source": [
    "lemma    = {}    # we build a so-called dictionary for the lookups\n",
    "tempdict = []\n",
    "\n",
    "wordfile_path = 'Azpilcueta/wordforms-lat.txt'\n",
    "wordfile = open(wordfile_path, encoding='utf-8')\n",
    "\n",
    "# open the wordfile (defined above) for reading\n",
    "wordfile = open(wordfile_path, encoding='utf-8')\n",
    "\n",
    "for line in wordfile.readlines():\n",
    "    tempdict.append(tuple(line.split('>'))) # we split each line by \">\" and append a tuple to a\n",
    "                                            # temporary list.\n",
    "\n",
    "lemma = {k.strip(): v.strip() for k, v in tempdict} # for every tuple in the list,\n",
    "                                                    # we strip whitespace and make a key-value\n",
    "                                                    # pair, appending it to our \"lemma\" dictionary\n",
    "wordfile.close\n",
    "print(str(len(lemma)) + ' wordforms known to the system.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, a quick test: Let's see with which \"lemma\"/basic word the particular wordform \"fidem\" is associated, or, in other words, what *value* our lemma variable returns when we query for the *key* \"fidem\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-06T08:11:07.287188Z",
     "start_time": "2018-02-06T08:11:07.278020Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fides'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma['fidem']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we are going to need the stopwords list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T23:21:47.959525Z",
     "start_time": "2018-02-05T23:21:47.934882Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "388 stopwords known to the system, e.g.: ['a', 'ab', 'ac', 'ad', 'adhic', 'adhuc', 'ae', 'ait', 'ali', 'alii', 'aliis', 'alio', 'aliqua', 'aliqui', 'aliquid', 'aliquis', 'aliquo', 'am', 'an', 'ante', 'apud', 'ar', 'at', 'atque', 'au', 'aut', 'autem', 'bus', 'c', 'ca', 'cap', 'ceptum', 'co', 'con', 'cons', 'cui', 'cum', 'cur', 'cùm', 'd', 'da', 'de', 'deinde', 'detur', 'di', 'diu', 'do', 'dum', 'e', 'ea', 'eadem', 'ec', 'eccle', 'ego', 'ei', 'eis', 'eius', 'el', 'em', 'en', 'enim', 'eo', 'eos', 'er', 'erat', 'ergo', 'erit', 'es', 'esse', 'essent', 'esset', 'est', 'et', 'etenim', 'eti']\n"
     ]
    }
   ],
   "source": [
    "stopwords_path = 'Azpilcueta/stopwords-lat.txt'\n",
    "stopwords = open(stopwords_path, encoding='utf-8').read().splitlines()\n",
    "\n",
    "print(str(len(stopwords)) + ' stopwords known to the system, e.g.: ' + str(stopwords[95:170]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(In contrast to simpler numbers that have been filtered out by our stopwords filter, I have left numbers representing years like \"1610\" in place.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we should find some very characteristic words for each segment for each edition. (Let's say we are looking for the \"Top 20\".) We should build a vocabulary for each edition individually and only afterwards work towards a common vocabulary of several \"Top n\" sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-06T09:18:49.022370Z",
     "start_time": "2018-02-06T09:18:48.113265Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('1549', 0), ('oua', 443)],\n",
       " [('capitulo', 86),\n",
       "  ('sexto', 600),\n",
       "  ('das', 179),\n",
       "  ('circunstancias', 105),\n",
       "  ('paribus', 455),\n",
       "  ('ou', 442),\n",
       "  ('oua', 443)],\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " [('lugar', 365),\n",
       "  ('cousa', 168),\n",
       "  ('igreja', 323),\n",
       "  ('dizer', 221),\n",
       "  ('as', 61),\n",
       "  ('sagrada', 569),\n",
       "  ('sagrado', 570),\n",
       "  ('furtum', 296),\n",
       "  ('furtou', 295),\n",
       "  ('sam', 572),\n",
       "  ('quando', 533),\n",
       "  ('ē', 671),\n",
       "  ('maneyras', 377),\n",
       "  ('tambẽ', 631),\n",
       "  ('homicidio', 309),\n",
       "  ('mudan', 396),\n",
       "  ('nastaria', 403),\n",
       "  ('necessarias', 406),\n",
       "  ('boãventura', 76),\n",
       "  ('ouuesse', 449)],\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " [('tres', 647),\n",
       "  ('casada', 89),\n",
       "  ('adulterio', 23),\n",
       "  ('sacrilegio', 567),\n",
       "  ('cometeo', 119),\n",
       "  ('he', 305),\n",
       "  ('religiosus', 547),\n",
       "  ('furtum', 296),\n",
       "  ('porque', 500),\n",
       "  ('declarar', 181),\n",
       "  ('sam', 572),\n",
       "  ('com', 113),\n",
       "  ('peccado', 468),\n",
       "  ('cada', 80),\n",
       "  ('dizendo', 220),\n",
       "  ('convem', 161),\n",
       "  ('hũ', 319),\n",
       "  ('prohibiçoes', 516),\n",
       "  ('distintos', 217),\n",
       "  ('restas', 550)],\n",
       " [('furtar', 294),\n",
       "  ('fim', 273),\n",
       "  ('porque', 500),\n",
       "  ('proposito', 518),\n",
       "  ('outro', 447),\n",
       "  ('he', 305),\n",
       "  ('particular', 457),\n",
       "  ('or', 437),\n",
       "  ('mandamento', 373),\n",
       "  ('depende', 192),\n",
       "  ('fornique', 286),\n",
       "  ('sacrament', 565),\n",
       "  ('lesta', 356),\n",
       "  ('luxuriar', 367),\n",
       "  ('defeso', 187),\n",
       "  ('hep', 306),\n",
       "  ('mudasse', 397),\n",
       "  ('final', 274),\n",
       "  ('conf', 128),\n",
       "  ('funda', 291)],\n",
       " [('mortal', 391),\n",
       "  ('semi', 591),\n",
       "  ('dizer', 221),\n",
       "  ('sam', 572),\n",
       "  ('venial', 653),\n",
       "  ('mas', 381),\n",
       "  ('mortalmente', 392),\n",
       "  ('he', 305),\n",
       "  ('estas', 249),\n",
       "  ('ao', 51),\n",
       "  ('necessidade', 409),\n",
       "  ('porque', 500),\n",
       "  ('peccado', 468),\n",
       "  ('mentira', 386),\n",
       "  ('provoque', 523),\n",
       "  ('apras', 53),\n",
       "  ('corpo', 166),\n",
       "  ('deos', 191),\n",
       "  ('soubesse', 613),\n",
       "  ('palavra', 450)],\n",
       " [('confessor', 142),\n",
       "  ('porque', 500),\n",
       "  ('semi', 591),\n",
       "  ('alcum', 31),\n",
       "  ('outras', 446),\n",
       "  ('penitente', 480),\n",
       "  ('ou', 442),\n",
       "  ('as', 61),\n",
       "  ('ao', 51),\n",
       "  ('segundo', 584),\n",
       "  ('hua', 311),\n",
       "  ('pbre', 463),\n",
       "  ('socorer', 607),\n",
       "  ('escandalize', 241),\n",
       "  ('hã', 318),\n",
       "  ('quaersma', 528),\n",
       "  ('pasassem', 458),\n",
       "  ('provocou', 522),\n",
       "  ('aliuiam', 43),\n",
       "  ('feria', 264)],\n",
       " [('poco', 491),\n",
       "  ('como', 121),\n",
       "  ('muyto', 399),\n",
       "  ('mesma', 387),\n",
       "  ('causa', 93),\n",
       "  ('seja', 586),\n",
       "  ('he', 305),\n",
       "  ('sam', 572),\n",
       "  ('specie', 616),\n",
       "  ('peccado', 468),\n",
       "  ('differir', 209),\n",
       "  ('prova', 520),\n",
       "  ('comuun', 123),\n",
       "  ('couda', 167),\n",
       "  ('contratar', 159),\n",
       "  ('procedẽ', 515),\n",
       "  ('probavel', 514),\n",
       "  ('fazem', 258),\n",
       "  ('xvii', 668),\n",
       "  ('alhea', 42)],\n",
       " [('he', 305),\n",
       "  ('semi', 591),\n",
       "  ('mais', 371),\n",
       "  ('sacerdote', 561),\n",
       "  ('casus', 92),\n",
       "  ('ihe', 324),\n",
       "  ('furtar', 294),\n",
       "  ('foy', 289),\n",
       "  ('porque', 500),\n",
       "  ('papa', 452),\n",
       "  ('cen', 96),\n",
       "  ('ordenam', 439),\n",
       "  ('clerigo', 111),\n",
       "  ('bispo', 69),\n",
       "  ('bios', 68),\n",
       "  ('impoer', 327),\n",
       "  ('comũa', 124),\n",
       "  ('palude', 451),\n",
       "  ('notavelmente', 418),\n",
       "  ('restituyçã', 551)],\n",
       " [('semi', 591),\n",
       "  ('nã', 422),\n",
       "  ('porque', 500),\n",
       "  ('quando', 533),\n",
       "  ('nega', 412),\n",
       "  ('silvestr', 602),\n",
       "  ('receber', 546),\n",
       "  ('agravã', 28),\n",
       "  ('sobredita', 605),\n",
       "  ('receba', 545),\n",
       "  ('collegir', 112),\n",
       "  ('duos', 228),\n",
       "  ('falar', 255),\n",
       "  ('conveniente', 162),\n",
       "  ('suade', 621),\n",
       "  ('suma', 624),\n",
       "  ('antisiodoro', 49),\n",
       "  ('graça', 301),\n",
       "  ('quer', 540),\n",
       "  ('sepone', 595)],\n",
       " [('dia', 207),\n",
       "  ('festa', 268),\n",
       "  ('obra', 423),\n",
       "  ('necessarius', 408),\n",
       "  ('peccado', 468),\n",
       "  ('circunstancia', 104),\n",
       "  ('mortal', 391),\n",
       "  ('he', 305),\n",
       "  ('segundo', 584),\n",
       "  ('porque', 500),\n",
       "  ('quando', 533),\n",
       "  ('fiz', 276),\n",
       "  ('sententiarum', 594),\n",
       "  ('razoavel', 543),\n",
       "  ('parece', 453),\n",
       "  ('fex', 269),\n",
       "  ('servuil', 599),\n",
       "  ('entendido', 234),\n",
       "  ('dous', 225),\n",
       "  ('quebrou', 538)],\n",
       " [('dia', 207),\n",
       "  ('oraçam', 438),\n",
       "  ('jejuum', 347),\n",
       "  ('fizesse', 277),\n",
       "  ('confessata', 137),\n",
       "  ('deputado', 193),\n",
       "  ('acima', 15),\n",
       "  ('salvo', 571),\n",
       "  ('festa', 268),\n",
       "  ('quebrantar', 537),\n",
       "  ('supra', 626),\n",
       "  ('proposito', 518),\n",
       "  ('circunstancia', 104),\n",
       "  ('com', 113),\n",
       "  ('navarro', 405),\n",
       "  ('segundo', 584),\n",
       "  ('necessidade', 409),\n",
       "  ('quando', 533),\n",
       "  ('peccado', 468),\n",
       "  ('ou', 442)],\n",
       " [('lugar', 365),\n",
       "  ('sagrado', 570),\n",
       "  ('ou', 442),\n",
       "  ('circunstancia', 104),\n",
       "  ('necessario', 407),\n",
       "  ('immunidade', 326),\n",
       "  ('semi', 591),\n",
       "  ('he', 305),\n",
       "  ('igreja', 323),\n",
       "  ('quando', 533),\n",
       "  ('peccado', 468),\n",
       "  ('cousas', 169),\n",
       "  ('as', 61),\n",
       "  ('obra', 423),\n",
       "  ('estas', 249),\n",
       "  ('com', 113),\n",
       "  ('navarro', 405),\n",
       "  ('mas', 381),\n",
       "  ('acolhem', 17),\n",
       "  ('come', 114)],\n",
       " '',\n",
       " [('he', 305),\n",
       "  ('mays', 383),\n",
       "  ('mais', 371),\n",
       "  ('porque', 500),\n",
       "  ('pecca', 467),\n",
       "  ('peccado', 468),\n",
       "  ('entonces', 237),\n",
       "  ('mas', 381),\n",
       "  ('stado', 619),\n",
       "  ('manifestala', 378),\n",
       "  ('convexido', 164),\n",
       "  ('mao', 380),\n",
       "  ('muda', 394),\n",
       "  ('ignorante', 322),\n",
       "  ('blasfemia', 71),\n",
       "  ('propria', 519),\n",
       "  ('menos', 385),\n",
       "  ('algũas', 40),\n",
       "  ('paribus', 455),\n",
       "  ('fornicetur', 284)],\n",
       " [('contra', 157),\n",
       "  ('consciencia', 148),\n",
       "  ('feyta', 270),\n",
       "  ('conscientia', 149),\n",
       "  ('ley', 358),\n",
       "  ('fomente', 278),\n",
       "  ('senam', 593),\n",
       "  ('erronea', 240),\n",
       "  ('nenhũa', 414),\n",
       "  ('necessarius', 408),\n",
       "  ('fez', 272),\n",
       "  ('circunstantia', 107),\n",
       "  ('foy', 289),\n",
       "  ('peccar', 471),\n",
       "  ('supra', 626),\n",
       "  ('era', 239),\n",
       "  ('entonces', 237),\n",
       "  ('obra', 423),\n",
       "  ('navarro', 405),\n",
       "  ('quando', 533)],\n",
       " [('peccado', 468),\n",
       "  ('veçes', 661),\n",
       "  ('numero', 421),\n",
       "  ('muytas', 398),\n",
       "  ('circunstantia', 107),\n",
       "  ('he', 305),\n",
       "  ('tanto', 633),\n",
       "  ('porque', 500),\n",
       "  ('declare', 182),\n",
       "  ('pasta', 462),\n",
       "  ('pequey', 483),\n",
       "  ('delles', 188),\n",
       "  ('demaneyra', 189),\n",
       "  ('verifica', 656),\n",
       "  ('diçam', 222),\n",
       "  ('dez', 206),\n",
       "  ('frequentaçam', 290),\n",
       "  ('acrecentamento', 19),\n",
       "  ('constitue', 152),\n",
       "  ('cento', 97)],\n",
       " [('memoria', 384),\n",
       "  ('numero', 421),\n",
       "  ('vezes', 660),\n",
       "  ('semi', 591),\n",
       "  ('assi', 63),\n",
       "  ('cousa', 168),\n",
       "  ('mortalmente', 392),\n",
       "  ('ham', 304),\n",
       "  ('sabe', 558),\n",
       "  ('como', 121),\n",
       "  ('ao', 51),\n",
       "  ('lata', 353),\n",
       "  ('cuidou', 173),\n",
       "  ('conto', 156),\n",
       "  ('conta', 154),\n",
       "  ('confesio', 129),\n",
       "  ('lançar', 352),\n",
       "  ('certo', 99),\n",
       "  ('alguũ', 38),\n",
       "  ('calasse', 85)],\n",
       " [('basta', 66),\n",
       "  ('diga', 210),\n",
       "  ('perseverou', 489),\n",
       "  ('angelo', 46),\n",
       "  ('archidiacono', 59),\n",
       "  ('podesse', 493),\n",
       "  ('vezes', 660),\n",
       "  ('assi', 63),\n",
       "  ('mas', 381),\n",
       "  ('quando', 533),\n",
       "  ('publica', 525),\n",
       "  ('tantos', 634),\n",
       "  ('converdita', 163),\n",
       "  ('amancebado', 45),\n",
       "  ('tempo', 636),\n",
       "  ('anno', 47),\n",
       "  ('annos', 48),\n",
       "  ('huũ', 317),\n",
       "  ('inparius', 335),\n",
       "  ('specificar', 617)],\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " [('cayda', 95),\n",
       "  ('presença', 511),\n",
       "  ('peccado', 468),\n",
       "  ('he', 305),\n",
       "  ('quando', 533),\n",
       "  ('tal', 628),\n",
       "  ('mas', 381),\n",
       "  ('pecca', 467),\n",
       "  ('com', 113),\n",
       "  ('casos', 91),\n",
       "  ('dos', 224),\n",
       "  ('outro', 447),\n",
       "  ('comete', 117),\n",
       "  ('scandalum', 579),\n",
       "  ('tomaran', 645),\n",
       "  ('disse', 214),\n",
       "  ('ocasiam', 426),\n",
       "  ('confessarse', 135),\n",
       "  ('occasiã', 429),\n",
       "  ('presenza', 510)],\n",
       " [('fez', 272),\n",
       "  ('avia', 65),\n",
       "  ('nã', 422),\n",
       "  ('circunstância', 109),\n",
       "  ('confessou', 144),\n",
       "  ('mas', 381),\n",
       "  ('diga', 210),\n",
       "  ('basta', 66),\n",
       "  ('vaãgloria', 652),\n",
       "  ('esmola', 243),\n",
       "  ('jurado', 349),\n",
       "  ('juramento', 350),\n",
       "  ('três', 648),\n",
       "  ('maa', 369),\n",
       "  ('confesse', 138),\n",
       "  ('necessário', 411),\n",
       "  ('vez', 659),\n",
       "  ('obra', 423),\n",
       "  ('semi', 591),\n",
       "  ('ou', 442)],\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '']"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "numTopTerms = 20\n",
    "\n",
    "# So first we build a tokenising and lemmatising function to work as an input filter\n",
    "# to the CountVectorizer function\n",
    "def ourLemmatiser(str_input):\n",
    "    wordforms = re.split('\\W+', str_input)\n",
    "    return [lemma[wordform].lower().strip() if wordform in lemma else wordform.lower().strip() for wordform in wordforms ]\n",
    "\n",
    "# !!!!\n",
    "# TODO: The above pipes all the tokens through the latin lemmatizer.\n",
    "# We should lemmatize Spanish and Portuguese differently!\n",
    "# !!!!\n",
    "\n",
    "topTerms = []\n",
    "for i in range(0,4):\n",
    "\n",
    "    topTermsEd = []\n",
    "    # Initialize the library's function, specifying our\n",
    "    # tokenizing function from above and our stopwords list.\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words=stopwords, use_idf=True, tokenizer=ourLemmatiser, norm='l2')\n",
    "\n",
    "    # Finally, we feed our corpus to the function to build a new \"tfidf_matrix\" object\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(Editions[i])\n",
    "\n",
    "    # convert your matrix to an array to loop over it\n",
    "    mx_array = tfidf_matrix.toarray()\n",
    "\n",
    "    # get your feature names\n",
    "    fn = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "    # now loop through all segments and get the respective top n words.\n",
    "    pos = 0\n",
    "    for j in mx_array:\n",
    "    \n",
    "#        print(j[:10][:10])\n",
    "        # Build List of most significant words for a segment\n",
    "#        topTermsEd.append([fn[x] for x in (mx_array[pos]*-1).argsort()][:numTopTerms])\n",
    "        if (j.max() == 0):\n",
    "            topTermsEd.append(\"\")\n",
    "        else:\n",
    "            topTermsEd.append([(fn[x], x) for x in ((j*-1).argsort())]\\\n",
    "                [:min(numTopTerms, len([ourLemmatiser(word) for word in re.split('\\W+', Editions[i][pos]) if ourLemmatiser(word) not in stopwords]))])\n",
    "#        print(\"Most significant words in Edition \" + str(i) + \", segment \" + str(pos) + \": \")\n",
    "#        print(topTermsEd[pos])\n",
    "        pos += 1\n",
    "        \n",
    "    topTerms.append(topTermsEd)\n",
    "#    print(\"Ed. \" + str(i) + \": \")\n",
    "topTerms[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-30T12:30:15.482766Z",
     "start_time": "2017-08-30T14:30:15.474338+02:00"
    }
   },
   "source": [
    "## Translations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe there is an approach to inter-lingual comparison after all. After a first unsuccessful try with [conceptnet.io](http://conceptnet.io), I next want to try [Babelnet](http://babelnet.org) in order to lookup synonyms, related terms and translations. I still have to study the [API](http://babelnet.org/guide)...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, with a single segment, we could do something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-06T09:19:12.968104Z",
     "start_time": "2018-02-06T09:19:12.708900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing words from segments 19 ...\n",
      " \n",
      "Here is the segment in the four editions:\n",
      "Ed. 0:\n",
      "¶A circunstancia do dia deputado a jejuum, ou oraçam : nam he de necessidade confessata : salvo quando fizesse peccado com proposito de ho quebrantar como acima do dia da festa. Segundo Navarro, ubi supra.\n",
      "Ed. 1:\n",
      " ¶ A IX que a circunstancia do dia de jejuũ, ou de oração, não se ha de confessar necessariamente, se não quando se pecca com proposito de ho quebrãtar : porque nã faz algũa das ditas tres cousas, segũdo em outra parte ho provamoss (s : f. in d. c. Consideret n. 32 vers. sic. Ad primum) \n",
      "Ed. 2:\n",
      "17 ¶  El X que la circunstancia del dia de ayuno, o de oracion, no se ha de confessar necessariamente, fino quando se peca con proposito delo quebrantar, por ello por que no haze alguna delas dichas tres cosas, segun lo provamos alibim (m : in d. c. Consideret nu. 32 ver. Ad primum) \n",
      "Ed. 3:\n",
      "17Decimo. Quod circunstantia diei ieiunio vel orationi consecrati, licet videatur aliquantulum augere peccatum, non est de necessitate confitenda, nisi peccatum perpetretur cum proposito violandi  per illud huiusmodi diem; Quoniam non efficit mortale, quod alias non [p. 69r, 150 pdf] esset tale, nec mutat in speciem mortalis, nec facit ut novo respectu sit tale quorum aliquod requiritur ad hoc un circunstantiae fonfessio sit necessaria, ut supra dictum est nu. 3. & probabimus in princip. dicti. c. Consideret, nu. 32 verb. Ad primum.\n",
      " \n",
      "Most significant words in the segment:\n",
      "Ed. 1:\n",
      "[('semi', 802), ('jejuũ', 476), ('vers', 920), ('oração', 586), ('ix', 472), ('provamoss', 705), ('segũdo', 794), ('quebrãtar', 725), ('dia', 278), ('necessariamente', 553), ('pecca', 627), ('ditas', 296), ('proposito', 700), ('não', 571), ('algũa', 52), ('tres', 897), ('nã', 570), ('cousas', 229), ('das', 243), ('pario', 611)]\n",
      "Ed. 2:\n",
      "[('x', 1091), ('fino', 459), ('alibim', 68), ('delo', 317), ('quebrantar', 859), ('ver', 1064), ('oracion', 706), ('ayuno', 142), ('delas', 311), ('proposito', 820), ('necessariamente', 667), ('semi', 948), ('peca', 745), ('alguna', 55), ('dia', 347), ('dichas', 350), ('provamos', 828), ('ello', 403), ('segun', 943), ('nu', 680)]\n",
      "Ed. 3:\n",
      "[('tale', 1352), ('consecrati', 297), ('perpetretur', 1002), ('augere', 157), ('ieiunio', 631), ('orationi', 935), ('probabimus', 1087), ('fonfessio', 553), ('requiro', 1200), ('69r', 31), ('17decimo', 15), ('aliquantulum', 103), ('novo', 886), ('violandi', 1456), ('necessitas', 866), ('nu', 888), ('diei', 399), ('dies', 400), ('video', 1453), ('proposito', 1102)]\n",
      "Ed. 4:\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-138-25071b076d5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Ed. \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\":\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopTerms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msegment_no\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "segment_no = 19\n",
    "\n",
    "print(\"Comparing words from segments \" + str(segment_no) + \" ...\")\n",
    "\n",
    "print(\" \")\n",
    "print(\"Here is the segment in the four editions:\")\n",
    "for i in range(0,4):\n",
    "    print(\"Ed. \" + str(i) + \":\")\n",
    "    print(Editions[i][segment_no])\n",
    "\n",
    "print(\" \")\n",
    "# Build List of most significant words for a segment\n",
    "\n",
    "print(\"Most significant words in the segment:\")\n",
    "for i in range(1,5):\n",
    "    print(\"Ed. \" + str(i) + \":\")\n",
    "    print(topTerms[i][segment_no])\n",
    "\n",
    "print(\" \")\n",
    "# Build lists of possible translations (the 15 most closely related ones)\n",
    "top10a_possible_translations = defaultdict(list)\n",
    "for word in top10a:\n",
    "    concepts_uri = \"http://api.conceptnet.io/related/c/la/\" + word + \"?filter=/c/es\"\n",
    "    response = urllib.request.urlopen(concepts_uri)\n",
    "    concepts = json.loads(response.read().decode(response.info().get_param('charset') or 'utf-8'))\n",
    "    for rel in concepts[\"related\"][0:15]:\n",
    "        top10a_possible_translations[word].append(rel.get(\"@id\").split('/')[-1])\n",
    "\n",
    "print(\" \")\n",
    "print(\"For each of the latin words, here are possible translations:\")\n",
    "for word in top10a_possible_translations:\n",
    "    print(word + \":\")\n",
    "    print(', '.join(trans for trans in top10a_possible_translations[word]))\n",
    "\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "# Build list of 10 most significant words in the second language\n",
    "top10b = []\n",
    "top10b = ([spFn[x] for x in (spMx_array[spSegment_no]*-1).argsort()][:12])\n",
    "print(\"Most significant words in the spanish text:\")\n",
    "print(top10b)\n",
    "\n",
    "# calculate number of overlapping terms\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\"Overlaps:\")\n",
    "for word in top10a_possible_translations:\n",
    "    print(', '.join(trans for trans in top10a_possible_translations[word] if (trans in top10b or trans == word)))\n",
    "\n",
    "# do a nifty ranking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-11T07:42:37.598721Z",
     "start_time": "2017-07-11T09:42:37.587926+02:00"
    }
   },
   "source": [
    "## Similarity <a name=\"DocumentSimilarity\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, once we have a representation of our text as a vector - which we can imagine as an arrow that goes a certain distance in one direction, another distance in another direction and so on - we can compare the different arrows. Do they go the same distance in a particular direction? And maybe almost the same in another direction? This would mean that one of the terms of our vocabulary has the same weight in both texts. Comparing the weight of our many, many dimensions, we can develop a measure for the similarity of the texts.\n",
    "\n",
    "(Probably, similarity in words that are occurring all over the place in the corpus should not count so much, and in fact it is attenuated by our arrows being made up of tf/idf weights.)\n",
    "\n",
    "Comparing arrows means calculating with angles and technically, what we are computing is the \"cosine similarity\" of texts. Again, there is a library ready for us to use (but you can find some documentation [here](http://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/), [here](http://scikit-learn.org/stable/modules/metrics.html#cosine-similarity) and [here](https://en.wikipedia.org/wiki/Cosine_similarity).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T23:23:27.224335Z",
     "start_time": "2018-02-05T23:23:27.017552Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairwise similarities:\n",
      "     0         1    2    3    4    5         6    7    8    9  ...    31  \\\n",
      "0   0.0  0.000000  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.0 ...   0.0   \n",
      "1   0.0  0.000000  0.0  0.0  0.0  0.0  0.038523  0.0  0.0  0.0 ...   0.0   \n",
      "2   0.0  0.000000  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.0 ...   0.0   \n",
      "3   0.0  0.000000  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.0 ...   0.0   \n",
      "4   0.0  0.000000  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.0 ...   0.0   \n",
      "5   0.0  0.000000  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.0 ...   0.0   \n",
      "6   0.0  0.038523  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.0 ...   0.0   \n",
      "7   0.0  0.000000  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.0 ...   0.0   \n",
      "8   0.0  0.000000  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.0 ...   0.0   \n",
      "9   0.0  0.000000  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.0 ...   0.0   \n",
      "10  0.0  0.000000  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.0 ...   0.0   \n",
      "11  0.0  0.000000  0.0  0.0  0.0  0.0  0.196924  0.0  0.0  0.0 ...   0.0   \n",
      "12  0.0  0.000000  0.0  0.0  0.0  0.0  0.085710  0.0  0.0  0.0 ...   0.0   \n",
      "13  0.0  0.000000  0.0  0.0  0.0  0.0  0.238937  0.0  0.0  0.0 ...   0.0   \n",
      "14  0.0  0.034567  0.0  0.0  0.0  0.0  0.145037  0.0  0.0  0.0 ...   0.0   \n",
      "15  0.0  0.043680  0.0  0.0  0.0  0.0  0.127832  0.0  0.0  0.0 ...   0.0   \n",
      "16  0.0  0.000000  0.0  0.0  0.0  0.0  0.108137  0.0  0.0  0.0 ...   0.0   \n",
      "17  0.0  0.046656  0.0  0.0  0.0  0.0  0.118749  0.0  0.0  0.0 ...   0.0   \n",
      "18  0.0  0.051104  0.0  0.0  0.0  0.0  0.059103  0.0  0.0  0.0 ...   0.0   \n",
      "19  0.0  0.000000  0.0  0.0  0.0  0.0  0.067991  0.0  0.0  0.0 ...   0.0   \n",
      "20  0.0  0.000000  0.0  0.0  0.0  0.0  0.394159  0.0  0.0  0.0 ...   0.0   \n",
      "21  0.0  0.000000  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.0 ...   0.0   \n",
      "22  0.0  0.000000  0.0  0.0  0.0  0.0  0.090631  0.0  0.0  0.0 ...   0.0   \n",
      "23  0.0  0.000000  0.0  0.0  0.0  0.0  0.044704  0.0  0.0  0.0 ...   0.0   \n",
      "24  0.0  0.000000  0.0  0.0  0.0  0.0  0.076221  0.0  0.0  0.0 ...   0.0   \n",
      "25  0.0  0.037599  0.0  0.0  0.0  0.0  0.133569  0.0  0.0  0.0 ...   0.0   \n",
      "26  0.0  0.038039  0.0  0.0  0.0  0.0  0.086768  0.0  0.0  0.0 ...   0.0   \n",
      "27  0.0  0.000000  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.0 ...   0.0   \n",
      "28  0.0  0.000000  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.0 ...   0.0   \n",
      "29  0.0  0.000000  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.0 ...   0.0   \n",
      "30  0.0  0.000000  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.0 ...   0.0   \n",
      "31  0.0  0.000000  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.0 ...   0.0   \n",
      "32  0.0  0.000000  0.0  0.0  0.0  0.0  0.166708  0.0  0.0  0.0 ...   0.0   \n",
      "33  0.0  0.000000  0.0  0.0  0.0  0.0  0.092031  0.0  0.0  0.0 ...   0.0   \n",
      "34  0.0  0.000000  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.0 ...   0.0   \n",
      "35  0.0  0.000000  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.0 ...   0.0   \n",
      "36  0.0  0.000000  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.0 ...   0.0   \n",
      "37  0.0  0.000000  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.0 ...   0.0   \n",
      "38  0.0  0.000000  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.0 ...   0.0   \n",
      "39  0.0  0.000000  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.0 ...   0.0   \n",
      "40  0.0  0.000000  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.0 ...   0.0   \n",
      "\n",
      "          32        33   34   35   36   37   38   39   40  \n",
      "0   0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1   0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2   0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3   0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4   0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "5   0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "6   0.166708  0.092031  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "7   0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "8   0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "9   0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "10  0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "11  0.172538  0.090993  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "12  0.190024  0.130108  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "13  0.252831  0.125200  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "14  0.173420  0.110849  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "15  0.197760  0.067197  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "16  0.164747  0.139258  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "17  0.172675  0.107433  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "18  0.234770  0.130440  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "19  0.173493  0.043454  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "20  0.224946  0.135974  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "21  0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "22  0.255747  0.151672  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "23  0.144045  0.126098  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "24  0.208317  0.104588  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "25  0.156473  0.100471  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "26  0.154855  0.199225  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "27  0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "28  0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "29  0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "30  0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "31  0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "32  0.000000  0.192803  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "33  0.192803  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "34  0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "35  0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "36  0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "37  0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "38  0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "39  0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "40  0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[41 rows x 41 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarities = pd.DataFrame(cosine_similarity(tfidf_matrix))\n",
    "similarities[round(similarities, 0) == 1] = 0 # Suppress a document's similarity to itself\n",
    "print(\"Pairwise similarities:\")\n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T23:23:39.058626Z",
     "start_time": "2018-02-05T23:23:39.008667Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The two most similar segments in the corpus are\n",
      "segments 18 and 19 .\n",
      "They have a similarity score of\n",
      "0.4102107979124507\n"
     ]
    }
   ],
   "source": [
    "print(\"The two most similar segments in the corpus are\")\n",
    "print(\"segments\", \\\n",
    "      similarities[similarities == similarities.values.max()].idxmax(axis=0).idxmax(axis=1), \\\n",
    "      \"and\", \\\n",
    "      similarities[similarities == similarities.values.max()].idxmax(axis=0)[ similarities[similarities == similarities.values.max()].idxmax(axis=0).idxmax(axis=1) ].astype(int), \\\n",
    "      \".\")\n",
    "print(\"They have a similarity score of\")\n",
    "print(similarities.values.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alertbox alert-success\">Of course, in every set of documents, we will always find two that are similar in the sense of them being more similar to each other than to the other ones. Whether or not this actually *means* anything in terms of content is still up to scholarly interpretation. But at least it means that a scholar can look at the two documents and when she determines that they are not so similar after all, then perhaps there is something interesting to say about similar vocabulary used for different puproses. Or the other way round: When the scholar knows that two passages are similar, but they have a low \"similarity score\", shouldn't that say something about the texts's rhetorics?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Clouds <a name=\"WordClouds\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a library that takes word frequencies like above, calculates corresponding relative sizes of words and creates nice wordcloud images for our sections (again, taking the fourth segment as an example) like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T23:22:48.941039Z",
     "start_time": "2018-02-05T23:22:48.887734Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-50500268115e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# But we have to convert our tf/idf weights to pseudo-frequencies (i.e. integer numbers)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mfrq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mEditions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mfreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mwc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackground_color\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"RGBA\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_font_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelative_scaling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fn' is not defined"
     ]
    }
   ],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# We make tuples of (lemma, tf/idf score) for one of our segments\n",
    "# But we have to convert our tf/idf weights to pseudo-frequencies (i.e. integer numbers)\n",
    "frq = [ int(round(x * 100000, 0)) for x in Editions[1][3]]\n",
    "freq = dict(zip(fn, frq))\n",
    "\n",
    "wc = WordCloud(background_color=None, mode=\"RGBA\", max_font_size=40, relative_scaling=1).fit_words(freq)\n",
    "\n",
    "# Now show/plot the wordcloud\n",
    "plt.figure()\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to have a nicer overview over the many segments than is possible in this notebook, let's create a new html file listing some of the characteristics that we have found so far..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-05T16:41:34.901698Z",
     "start_time": "2018-02-05T16:41:34.866575Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-98-2073f2fd43f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m100000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmx_array\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mdicts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWordCloud\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbackground_color\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"RGBA\"\u001b[0m\u001b[1;33m,\u001b[0m                        \u001b[0mmax_font_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_font_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m                        \u001b[0mmax_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrelative_scaling\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdicts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[1;31m# We write the wordcloud image to a file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputDir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/wc_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.png'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "outputDir = \"Azpilcueta\"\n",
    "htmlfile = open(outputDir + '/Overview.html', encoding='utf-8', mode='w')\n",
    "\n",
    "# Write the html header and the opening of a layout table\n",
    "htmlfile.write(\"\"\"<!DOCTYPE html>\n",
    "<html>\n",
    "    <head>\n",
    "        <title>Section Characteristics</title>\n",
    "        <meta charset=\"utf-8\"/>\n",
    "    </head>\n",
    "    <body>\n",
    "        <table>\n",
    "\"\"\")\n",
    "\n",
    "a = [[]]\n",
    "a.clear()\n",
    "dicts = []\n",
    "w = []\n",
    "\n",
    "# For each segment, create a wordcloud and write it along with label and\n",
    "# other information into a new row of the html table\n",
    "for i in range(0, len(mx_array)):\n",
    "    # this is like above in the single-segment example...\n",
    "    a.append([ int(round(x * 100000, 0)) for x in mx_array[i]])\n",
    "    dicts.append(dict(zip(fn, a[i])))\n",
    "    w.append(WordCloud(background_color=None, mode=\"RGBA\", \\\n",
    "                       max_font_size=40, min_font_size=10, \\\n",
    "                       max_words=60, relative_scaling=0.8).fit_words(dicts[i]))\n",
    "    # We write the wordcloud image to a file\n",
    "    w[i].to_file(outputDir + '/wc_' + str(i) + '.png')\n",
    "    # Finally we write the column row\n",
    "    htmlfile.write(\"\"\"\n",
    "            <tr>\n",
    "                <td>\n",
    "                    <head>Section {a}: <b>{b}</b></head><br/>\n",
    "                    <img src=\"./wc_{a}.png\"/><br/>\n",
    "                    <small><i>length: {c} words</i></small>\n",
    "                </td>\n",
    "            </tr>\n",
    "            <tr><td>&nbsp;</td></tr>\n",
    "\"\"\".format(a = str(i), b = label[i], c = len(tokenised[i])))\n",
    "\n",
    "# And then we write the end of the html file.\n",
    "htmlfile.write(\"\"\"\n",
    "        </table>\n",
    "    </body>\n",
    "</html>\n",
    "\"\"\")\n",
    "htmlfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should have created a nice html file which we can open [here](./Solorzano/Overview.html)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "245px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "threshold": "2",
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
